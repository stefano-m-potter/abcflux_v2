{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "14e5be34-f9be-436a-8ec6-b55125715c48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Columns selected for feature engineering: ['EVI', 'NDVI', 'sur_refl_b01', 'sur_refl_b02', 'sur_refl_b03', 'sur_refl_b07', 'NDWI', 'aet', 'def', 'pdsi', 'pet', 'pr', 'ro', 'srad', 'swe', 'tmmn', 'tmmx', 'vap', 'vpd', 'vs', 'lai', 'fpar', 'Percent_NonTree_Vegetation', 'Percent_NonVegetated', 'Percent_Tree_Cover', 'nee', 'gpp', 'reco', 'ch4_flux_total', 'tmean_C']\n",
      "ðŸŽ‰ Successfully saved data with new features to: /explore/nobackup/people/spotter5/anna_v/v2/v2_model_training_data_with_lags.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load data\n",
    "# Ensure the path to your CSV is correct\n",
    "\n",
    "df = pd.read_csv(\"/explore/nobackup/people/spotter5/anna_v/v2/v2_model_training_final.csv\")\n",
    "\n",
    "df = df[df['flux_method'] == 'EC']\n",
    "\n",
    "\n",
    "# --- Data Preprocessing ---\n",
    "\n",
    "# Fill ALT with 2000 for NaNs between 1997â€“2021\n",
    "# This assumes 'ALT' column exists.\n",
    "# if 'ALT' in df.columns:\n",
    "#     time_interval_mask = (df['year'] >= 1997) & (df['year'] <= 2021)\n",
    "#     df.loc[time_interval_mask, 'ALT'] = df.loc[time_interval_mask, 'ALT'].fillna(2000)\n",
    "\n",
    "# Create tmean_C and date\n",
    "# Note: Assuming 'tmmx' and 'tmmn' are the correct column names instead of tmax_C/tmin_C\n",
    "df['tmean_C'] = df[['tmmx', 'tmmn']].mean(axis=1)\n",
    "df['date'] = pd.to_datetime(df[['year', 'month']].assign(day=1))\n",
    "\n",
    "# --- Dynamic Feature Selection ---\n",
    "\n",
    "# Define columns to EXCLUDE from lag/rolling feature generation\n",
    "# This includes identifiers, categorical data, dates, and user-specified exclusions\n",
    "base_exclusions = {\n",
    "    'site_name', 'site_reference', 'latitude', 'longitude', 'flux_method',\n",
    "    'country', 'land_cover_eco', 'land_cover_plot', 'bawld_class', 'year',\n",
    "    'month', 'SummaryQA', 'land_cover', 'co2_cont', 'date', 'ALT', 'soil', 'land_cover',\n",
    "    'siteID', 'Flux'\n",
    "}\n",
    "\n",
    "# Find all columns that end with the '_100cm' suffix\n",
    "suffix_exclusions = {col for col in df.columns if col.endswith('_100cm')}\n",
    "\n",
    "# Combine all sets of exclusions\n",
    "all_exclusions = base_exclusions.union(suffix_exclusions)\n",
    "\n",
    "# Dynamically create the list of predictors to expand\n",
    "predictors_to_expand = [col for col in df.columns if col not in all_exclusions]\n",
    "print(f\"âœ… Columns selected for feature engineering: {predictors_to_expand}\")\n",
    "\n",
    "# --- Feature Engineering ---\n",
    "\n",
    "# Make a copy to store engineered features\n",
    "df_engineered = df[['site_reference', 'date']].copy()\n",
    "\n",
    "# Sort by site and date for consistent time-series calculations\n",
    "df = df.sort_values(['site_reference', 'date'])\n",
    "\n",
    "# Generate lag, rolling mean, and rolling std features\n",
    "for var in predictors_to_expand:\n",
    "    for lag in [1, 2, 3]:\n",
    "        df_engineered[f'{var}_lag{lag}'] = (\n",
    "            df.groupby('site_reference')[var]\n",
    "            .shift(lag)\n",
    "        )\n",
    "    \n",
    "    # df_engineered[f'{var}_roll3_mean'] = (\n",
    "    #     df.groupby('site_reference')[var]\n",
    "    #     .shift(1)\n",
    "    #     .rolling(window=3, min_periods=1)\n",
    "    #     .mean()\n",
    "    #     .reset_index(level=0, drop=True) # Add reset_index to align correctly\n",
    "    # )\n",
    "    \n",
    "    # df_engineered[f'{var}_roll3_std'] = (\n",
    "    #     df.groupby('site_reference')[var]\n",
    "    #     .shift(1)\n",
    "    #     .rolling(window=3, min_periods=1)\n",
    "    #     .std()\n",
    "    #     .reset_index(level=0, drop=True) # Add reset_index to align correctly\n",
    "    # )\n",
    "\n",
    "    # df_engineered[f'{var}_roll3_std'] = (\n",
    "    #     df.groupby('site_reference')[var]\n",
    "    #     .shift(1)\n",
    "    #     .rolling(window=3, min_periods=1)\n",
    "    #     .std()\n",
    "    #     .reset_index(level=0, drop=True)\n",
    "    #     .fillna(0)  # <-- Add this line to fill NaNs with 0\n",
    "    # )\n",
    "\n",
    "# --- Finalizing DataFrame ---\n",
    "\n",
    "# Merge engineered features back into original dataframe\n",
    "df_full = pd.merge(df, df_engineered, on=['site_reference', 'date'], how='left')\n",
    "\n",
    "# Save to new CSV\n",
    "output_path = \"/explore/nobackup/people/spotter5/anna_v/v2/v2_model_training_data_with_lags.csv\"\n",
    "df_full.to_csv(output_path, index=False)\n",
    "print(f\"ðŸŽ‰ Successfully saved data with new features to: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e069506f-ccc8-4a3d-9952-928c2b5f8765",
   "metadata": {},
   "source": [
    "Use group mean instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5170a50f-449f-4b29-9aa8-ddeeb6e40d77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Columns selected for feature engineering: ['EVI', 'NDVI', 'sur_refl_b01', 'sur_refl_b02', 'sur_refl_b03', 'sur_refl_b07', 'NDWI', 'aet', 'def', 'pdsi', 'pet', 'pr', 'ro', 'srad', 'swe', 'tmmn', 'tmmx', 'vap', 'vpd', 'vs', 'lai', 'fpar', 'Percent_NonTree_Vegetation', 'Percent_NonVegetated', 'Percent_Tree_Cover', 'nee', 'gpp', 'reco', 'ch4_flux_total', 'Flux', 'tmean_C']\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Could not convert CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4 to numeric",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[0;32m~/.conda/envs/xgboost_gpu/lib/python3.9/site-packages/pandas/core/groupby/groupby.py:1791\u001b[0m, in \u001b[0;36mGroupBy._cython_agg_general.<locals>.array_func\u001b[0;34m(values)\u001b[0m\n\u001b[1;32m   1790\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1791\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrouper\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cython_operation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1792\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43maggregate\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1793\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1794\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhow\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1795\u001b[0m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mndim\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1796\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmin_count\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmin_count\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1797\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1798\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1799\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m:\n\u001b[1;32m   1800\u001b[0m     \u001b[38;5;66;03m# generally if we have numeric_only=False\u001b[39;00m\n\u001b[1;32m   1801\u001b[0m     \u001b[38;5;66;03m# and non-applicable functions\u001b[39;00m\n\u001b[1;32m   1802\u001b[0m     \u001b[38;5;66;03m# try to python agg\u001b[39;00m\n\u001b[1;32m   1803\u001b[0m     \u001b[38;5;66;03m# TODO: shouldn't min_count matter?\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/xgboost_gpu/lib/python3.9/site-packages/pandas/core/groupby/ops.py:1039\u001b[0m, in \u001b[0;36mBaseGrouper._cython_operation\u001b[0;34m(self, kind, values, how, axis, min_count, **kwargs)\u001b[0m\n\u001b[1;32m   1038\u001b[0m ngroups \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mngroups\n\u001b[0;32m-> 1039\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcy_op\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcython_operation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1040\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalues\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1041\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1042\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmin_count\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmin_count\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1043\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcomp_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1044\u001b[0m \u001b[43m    \u001b[49m\u001b[43mngroups\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mngroups\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1045\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1046\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/xgboost_gpu/lib/python3.9/site-packages/pandas/core/groupby/ops.py:708\u001b[0m, in \u001b[0;36mWrappedCythonOp.cython_operation\u001b[0;34m(self, values, axis, min_count, comp_ids, ngroups, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ea_wrap_cython_operation(\n\u001b[1;32m    701\u001b[0m         values,\n\u001b[1;32m    702\u001b[0m         min_count\u001b[38;5;241m=\u001b[39mmin_count,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    705\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    706\u001b[0m     )\n\u001b[0;32m--> 708\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cython_op_ndim_compat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    709\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    710\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmin_count\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmin_count\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    711\u001b[0m \u001b[43m    \u001b[49m\u001b[43mngroups\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mngroups\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    712\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcomp_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcomp_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    713\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    714\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    715\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/xgboost_gpu/lib/python3.9/site-packages/pandas/core/groupby/ops.py:512\u001b[0m, in \u001b[0;36mWrappedCythonOp._cython_op_ndim_compat\u001b[0;34m(self, values, min_count, ngroups, comp_ids, mask, result_mask, **kwargs)\u001b[0m\n\u001b[1;32m    511\u001b[0m     result_mask \u001b[38;5;241m=\u001b[39m result_mask[\u001b[38;5;28;01mNone\u001b[39;00m, :]\n\u001b[0;32m--> 512\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_cython_op\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    513\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalues2d\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    514\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmin_count\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmin_count\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    515\u001b[0m \u001b[43m    \u001b[49m\u001b[43mngroups\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mngroups\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    516\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcomp_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcomp_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    517\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    518\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresult_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresult_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    519\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    520\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    521\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m res\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/.conda/envs/xgboost_gpu/lib/python3.9/site-packages/pandas/core/groupby/ops.py:571\u001b[0m, in \u001b[0;36mWrappedCythonOp._call_cython_op\u001b[0;34m(self, values, min_count, ngroups, comp_ids, mask, result_mask, **kwargs)\u001b[0m\n\u001b[1;32m    570\u001b[0m out_shape \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_output_shape(ngroups, values)\n\u001b[0;32m--> 571\u001b[0m func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_cython_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkind\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhow\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalues\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_numeric\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    572\u001b[0m values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_cython_vals(values)\n",
      "File \u001b[0;32m~/.conda/envs/xgboost_gpu/lib/python3.9/site-packages/pandas/core/groupby/ops.py:192\u001b[0m, in \u001b[0;36mWrappedCythonOp._get_cython_function\u001b[0;34m(cls, kind, how, dtype, is_numeric)\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobject\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m f\u001b[38;5;241m.\u001b[39m__signatures__:\n\u001b[1;32m    191\u001b[0m     \u001b[38;5;66;03m# raise NotImplementedError here rather than TypeError later\u001b[39;00m\n\u001b[0;32m--> 192\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[1;32m    193\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunction is not implemented for this dtype: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    194\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[how->\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhow\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m,dtype->\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdtype_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    195\u001b[0m     )\n\u001b[1;32m    196\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m f\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: function is not implemented for this dtype: [how->mean,dtype->object]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m~/.conda/envs/xgboost_gpu/lib/python3.9/site-packages/pandas/core/nanops.py:1630\u001b[0m, in \u001b[0;36m_ensure_numeric\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m   1629\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1630\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1631\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[1;32m   1632\u001b[0m     \u001b[38;5;66;03m# e.g. \"1+1j\" or \"foo\"\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: could not convert string to float: 'CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m~/.conda/envs/xgboost_gpu/lib/python3.9/site-packages/pandas/core/nanops.py:1634\u001b[0m, in \u001b[0;36m_ensure_numeric\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m   1633\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1634\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcomplex\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1635\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m   1636\u001b[0m     \u001b[38;5;66;03m# e.g. \"foo\"\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: complex() arg is a malformed string",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 53\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# Generate lag, rolling mean, and rolling std features\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m var \u001b[38;5;129;01min\u001b[39;00m predictors_to_expand:\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;66;03m# Calculate the mean for the current variable, grouped by site\u001b[39;00m\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;66;03m# This will be used to fill NaNs for the initial lag periods.\u001b[39;00m\n\u001b[0;32m---> 53\u001b[0m     group_mean \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroupby\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msite_reference\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[43mvar\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmean\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;66;03m# Generate lag features and fill NaNs with the site-specific group mean\u001b[39;00m\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m lag \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m]:\n",
      "File \u001b[0;32m~/.conda/envs/xgboost_gpu/lib/python3.9/site-packages/pandas/core/groupby/generic.py:446\u001b[0m, in \u001b[0;36mSeriesGroupBy.transform\u001b[0;34m(self, func, engine, engine_kwargs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    443\u001b[0m \u001b[38;5;129m@Substitution\u001b[39m(klass\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSeries\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    444\u001b[0m \u001b[38;5;129m@Appender\u001b[39m(_transform_template)\n\u001b[1;32m    445\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtransform\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, \u001b[38;5;241m*\u001b[39margs, engine\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, engine_kwargs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 446\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    447\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mengine_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    448\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/xgboost_gpu/lib/python3.9/site-packages/pandas/core/groupby/groupby.py:1872\u001b[0m, in \u001b[0;36mGroupBy._transform\u001b[0;34m(self, func, engine, engine_kwargs, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1868\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m com\u001b[38;5;241m.\u001b[39mtemp_setattr(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobserved\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m   1869\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m com\u001b[38;5;241m.\u001b[39mtemp_setattr(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas_index\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m   1870\u001b[0m         \u001b[38;5;66;03m# GH#49834 - result needs groups in the index for\u001b[39;00m\n\u001b[1;32m   1871\u001b[0m         \u001b[38;5;66;03m# _wrap_transform_fast_result\u001b[39;00m\n\u001b[0;32m-> 1872\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1874\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wrap_transform_fast_result(result)\n",
      "File \u001b[0;32m~/.conda/envs/xgboost_gpu/lib/python3.9/site-packages/pandas/core/groupby/groupby.py:2183\u001b[0m, in \u001b[0;36mGroupBy.mean\u001b[0;34m(self, numeric_only, engine, engine_kwargs)\u001b[0m\n\u001b[1;32m   2181\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_numba_agg_general(sliding_mean, engine_kwargs)\n\u001b[1;32m   2182\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2183\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cython_agg_general\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2184\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmean\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2185\u001b[0m \u001b[43m        \u001b[49m\u001b[43malt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mSeries\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnumeric_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnumeric_only_bool\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2186\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnumeric_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnumeric_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2187\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2188\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgroupby\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.conda/envs/xgboost_gpu/lib/python3.9/site-packages/pandas/core/groupby/groupby.py:1810\u001b[0m, in \u001b[0;36mGroupBy._cython_agg_general\u001b[0;34m(self, how, alt, numeric_only, min_count, ignore_failures, **kwargs)\u001b[0m\n\u001b[1;32m   1806\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[1;32m   1808\u001b[0m \u001b[38;5;66;03m# TypeError -> we may have an exception in trying to aggregate\u001b[39;00m\n\u001b[1;32m   1809\u001b[0m \u001b[38;5;66;03m#  continue and exclude the block\u001b[39;00m\n\u001b[0;32m-> 1810\u001b[0m new_mgr \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrouped_reduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_failures\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_failures\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1812\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_ser \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(new_mgr) \u001b[38;5;241m<\u001b[39m orig_len:\n\u001b[1;32m   1813\u001b[0m     warn_dropping_nuisance_columns_deprecated(\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m), how, numeric_only)\n",
      "File \u001b[0;32m~/.conda/envs/xgboost_gpu/lib/python3.9/site-packages/pandas/core/internals/base.py:199\u001b[0m, in \u001b[0;36mSingleDataManager.grouped_reduce\u001b[0;34m(self, func, ignore_failures)\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;124;03mignore_failures : bool, default False\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;124;03m    Not used; for compatibility with ArrayManager/BlockManager.\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    198\u001b[0m arr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39marray\n\u001b[0;32m--> 199\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    200\u001b[0m index \u001b[38;5;241m=\u001b[39m default_index(\u001b[38;5;28mlen\u001b[39m(res))\n\u001b[1;32m    202\u001b[0m mgr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mfrom_array(res, index)\n",
      "File \u001b[0;32m~/.conda/envs/xgboost_gpu/lib/python3.9/site-packages/pandas/core/groupby/groupby.py:1804\u001b[0m, in \u001b[0;36mGroupBy._cython_agg_general.<locals>.array_func\u001b[0;34m(values)\u001b[0m\n\u001b[1;32m   1791\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrouper\u001b[38;5;241m.\u001b[39m_cython_operation(\n\u001b[1;32m   1792\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maggregate\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1793\u001b[0m         values,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1797\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   1798\u001b[0m     )\n\u001b[1;32m   1799\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m:\n\u001b[1;32m   1800\u001b[0m     \u001b[38;5;66;03m# generally if we have numeric_only=False\u001b[39;00m\n\u001b[1;32m   1801\u001b[0m     \u001b[38;5;66;03m# and non-applicable functions\u001b[39;00m\n\u001b[1;32m   1802\u001b[0m     \u001b[38;5;66;03m# try to python agg\u001b[39;00m\n\u001b[1;32m   1803\u001b[0m     \u001b[38;5;66;03m# TODO: shouldn't min_count matter?\u001b[39;00m\n\u001b[0;32m-> 1804\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_agg_py_fallback\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mndim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mndim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1806\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/.conda/envs/xgboost_gpu/lib/python3.9/site-packages/pandas/core/groupby/groupby.py:1745\u001b[0m, in \u001b[0;36mGroupBy._agg_py_fallback\u001b[0;34m(self, values, ndim, alt)\u001b[0m\n\u001b[1;32m   1740\u001b[0m     ser \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39miloc[:, \u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# We do not get here with UDFs, so we know that our dtype\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m#  should always be preserved by the implemented aggregations\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;66;03m# TODO: Is this exactly right; see WrappedCythonOp get_result_dtype?\u001b[39;00m\n\u001b[0;32m-> 1745\u001b[0m res_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrouper\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magg_series\u001b[49m\u001b[43m(\u001b[49m\u001b[43mser\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreserve_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(values, Categorical):\n\u001b[1;32m   1748\u001b[0m     \u001b[38;5;66;03m# Because we only get here with known dtype-preserving\u001b[39;00m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;66;03m#  reductions, we cast back to Categorical.\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m     \u001b[38;5;66;03m# TODO: if we ever get \"rank\" working, exclude it here.\u001b[39;00m\n\u001b[1;32m   1751\u001b[0m     res_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(values)\u001b[38;5;241m.\u001b[39m_from_sequence(res_values, dtype\u001b[38;5;241m=\u001b[39mvalues\u001b[38;5;241m.\u001b[39mdtype)\n",
      "File \u001b[0;32m~/.conda/envs/xgboost_gpu/lib/python3.9/site-packages/pandas/core/groupby/ops.py:1081\u001b[0m, in \u001b[0;36mBaseGrouper.agg_series\u001b[0;34m(self, obj, func, preserve_dtype)\u001b[0m\n\u001b[1;32m   1078\u001b[0m     preserve_dtype \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1080\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1081\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_aggregate_series_pure_python\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1083\u001b[0m npvalues \u001b[38;5;241m=\u001b[39m lib\u001b[38;5;241m.\u001b[39mmaybe_convert_objects(result, try_float\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m   1084\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m preserve_dtype:\n",
      "File \u001b[0;32m~/.conda/envs/xgboost_gpu/lib/python3.9/site-packages/pandas/core/groupby/ops.py:1104\u001b[0m, in \u001b[0;36mBaseGrouper._aggregate_series_pure_python\u001b[0;34m(self, obj, func)\u001b[0m\n\u001b[1;32m   1101\u001b[0m splitter \u001b[38;5;241m=\u001b[39m get_splitter(obj, ids, ngroups, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, group \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(splitter):\n\u001b[0;32m-> 1104\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1105\u001b[0m     res \u001b[38;5;241m=\u001b[39m libreduction\u001b[38;5;241m.\u001b[39mextract_result(res)\n\u001b[1;32m   1107\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m initialized:\n\u001b[1;32m   1108\u001b[0m         \u001b[38;5;66;03m# We only do this validation on the first iteration\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/xgboost_gpu/lib/python3.9/site-packages/pandas/core/groupby/groupby.py:2185\u001b[0m, in \u001b[0;36mGroupBy.mean.<locals>.<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m   2181\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_numba_agg_general(sliding_mean, engine_kwargs)\n\u001b[1;32m   2182\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2183\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cython_agg_general(\n\u001b[1;32m   2184\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m-> 2185\u001b[0m         alt\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[43mSeries\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnumeric_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnumeric_only_bool\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m   2186\u001b[0m         numeric_only\u001b[38;5;241m=\u001b[39mnumeric_only,\n\u001b[1;32m   2187\u001b[0m     )\n\u001b[1;32m   2188\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgroupby\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.conda/envs/xgboost_gpu/lib/python3.9/site-packages/pandas/core/generic.py:11847\u001b[0m, in \u001b[0;36mNDFrame._add_numeric_operations.<locals>.mean\u001b[0;34m(self, axis, skipna, level, numeric_only, **kwargs)\u001b[0m\n\u001b[1;32m  11829\u001b[0m \u001b[38;5;129m@doc\u001b[39m(\n\u001b[1;32m  11830\u001b[0m     _num_doc,\n\u001b[1;32m  11831\u001b[0m     desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReturn the mean of the values over the requested axis.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m  11845\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m  11846\u001b[0m ):\n\u001b[0;32m> 11847\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mNDFrame\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskipna\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumeric_only\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/xgboost_gpu/lib/python3.9/site-packages/pandas/core/generic.py:11401\u001b[0m, in \u001b[0;36mNDFrame.mean\u001b[0;34m(self, axis, skipna, level, numeric_only, **kwargs)\u001b[0m\n\u001b[1;32m  11393\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmean\u001b[39m(\n\u001b[1;32m  11394\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m  11395\u001b[0m     axis: Axis \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m lib\u001b[38;5;241m.\u001b[39mNoDefault \u001b[38;5;241m=\u001b[39m lib\u001b[38;5;241m.\u001b[39mno_default,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m  11399\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m  11400\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Series \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mfloat\u001b[39m:\n\u001b[0;32m> 11401\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stat_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m  11402\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmean\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnanops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnanmean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskipna\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumeric_only\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m  11403\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/xgboost_gpu/lib/python3.9/site-packages/pandas/core/generic.py:11353\u001b[0m, in \u001b[0;36mNDFrame._stat_function\u001b[0;34m(self, name, func, axis, skipna, level, numeric_only, **kwargs)\u001b[0m\n\u001b[1;32m  11343\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m  11344\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing the level keyword in DataFrame and Series aggregations is \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m  11345\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdeprecated and will be removed in a future version. Use groupby \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m  11348\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m  11349\u001b[0m     )\n\u001b[1;32m  11350\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_agg_by_level(\n\u001b[1;32m  11351\u001b[0m         name, axis\u001b[38;5;241m=\u001b[39maxis, level\u001b[38;5;241m=\u001b[39mlevel, skipna\u001b[38;5;241m=\u001b[39mskipna, numeric_only\u001b[38;5;241m=\u001b[39mnumeric_only\n\u001b[1;32m  11352\u001b[0m     )\n\u001b[0;32m> 11353\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reduce\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m  11354\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskipna\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskipna\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumeric_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnumeric_only\u001b[49m\n\u001b[1;32m  11355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/xgboost_gpu/lib/python3.9/site-packages/pandas/core/series.py:4816\u001b[0m, in \u001b[0;36mSeries._reduce\u001b[0;34m(self, op, name, axis, skipna, numeric_only, filter_type, **kwds)\u001b[0m\n\u001b[1;32m   4812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[1;32m   4813\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSeries.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not implement \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkwd_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   4814\u001b[0m     )\n\u001b[1;32m   4815\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m np\u001b[38;5;241m.\u001b[39merrstate(\u001b[38;5;28mall\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m-> 4816\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdelegate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskipna\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskipna\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/xgboost_gpu/lib/python3.9/site-packages/pandas/core/nanops.py:93\u001b[0m, in \u001b[0;36mdisallow.__call__.<locals>._f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     92\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m np\u001b[38;5;241m.\u001b[39merrstate(invalid\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m---> 93\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;66;03m# we want to transform an object array\u001b[39;00m\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;66;03m# ValueError message to the more typical TypeError\u001b[39;00m\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;66;03m# e.g. this is normally a disallowed function on\u001b[39;00m\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;66;03m# object arrays that contain strings\u001b[39;00m\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_object_dtype(args[\u001b[38;5;241m0\u001b[39m]):\n",
      "File \u001b[0;32m~/.conda/envs/xgboost_gpu/lib/python3.9/site-packages/pandas/core/nanops.py:155\u001b[0m, in \u001b[0;36mbottleneck_switch.__call__.<locals>.f\u001b[0;34m(values, axis, skipna, **kwds)\u001b[0m\n\u001b[1;32m    153\u001b[0m         result \u001b[38;5;241m=\u001b[39m alt(values, axis\u001b[38;5;241m=\u001b[39maxis, skipna\u001b[38;5;241m=\u001b[39mskipna, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 155\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43malt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskipna\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskipna\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/.conda/envs/xgboost_gpu/lib/python3.9/site-packages/pandas/core/nanops.py:418\u001b[0m, in \u001b[0;36m_datetimelike_compat.<locals>.new_func\u001b[0;34m(values, axis, skipna, mask, **kwargs)\u001b[0m\n\u001b[1;32m    415\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m datetimelike \u001b[38;5;129;01mand\u001b[39;00m mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    416\u001b[0m     mask \u001b[38;5;241m=\u001b[39m isna(values)\n\u001b[0;32m--> 418\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskipna\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskipna\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    420\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m datetimelike:\n\u001b[1;32m    421\u001b[0m     result \u001b[38;5;241m=\u001b[39m _wrap_results(result, orig_values\u001b[38;5;241m.\u001b[39mdtype, fill_value\u001b[38;5;241m=\u001b[39miNaT)\n",
      "File \u001b[0;32m~/.conda/envs/xgboost_gpu/lib/python3.9/site-packages/pandas/core/nanops.py:706\u001b[0m, in \u001b[0;36mnanmean\u001b[0;34m(values, axis, skipna, mask)\u001b[0m\n\u001b[1;32m    703\u001b[0m     dtype_count \u001b[38;5;241m=\u001b[39m dtype\n\u001b[1;32m    705\u001b[0m count \u001b[38;5;241m=\u001b[39m _get_counts(values\u001b[38;5;241m.\u001b[39mshape, mask, axis, dtype\u001b[38;5;241m=\u001b[39mdtype_count)\n\u001b[0;32m--> 706\u001b[0m the_sum \u001b[38;5;241m=\u001b[39m \u001b[43m_ensure_numeric\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype_sum\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    708\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m axis \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(the_sum, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mndim\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    709\u001b[0m     count \u001b[38;5;241m=\u001b[39m cast(np\u001b[38;5;241m.\u001b[39mndarray, count)\n",
      "File \u001b[0;32m~/.conda/envs/xgboost_gpu/lib/python3.9/site-packages/pandas/core/nanops.py:1637\u001b[0m, in \u001b[0;36m_ensure_numeric\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m   1634\u001b[0m             x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcomplex\u001b[39m(x)\n\u001b[1;32m   1635\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m   1636\u001b[0m             \u001b[38;5;66;03m# e.g. \"foo\"\u001b[39;00m\n\u001b[0;32m-> 1637\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not convert \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m to numeric\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   1638\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\u001b[0;31mTypeError\u001b[0m: Could not convert CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4CO2 and CH4 to numeric"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load data\n",
    "# Ensure the path to your CSV is correct\n",
    "df = pd.read_csv(\"/explore/nobackup/people/spotter5/anna_v/v2/v2_model_training_final.csv\")\n",
    "\n",
    "df = df[df['flux_method'] == 'EC']\n",
    "\n",
    "\n",
    "# --- Data Preprocessing ---\n",
    "\n",
    "# Create tmean_C and date\n",
    "df['tmean_C'] = df[['tmmx', 'tmmn']].mean(axis=1)\n",
    "df['date'] = pd.to_datetime(df[['year', 'month']].assign(day=1))\n",
    "\n",
    "# --- Dynamic Feature Selection ---\n",
    "\n",
    "# Define columns to EXCLUDE from lag/rolling feature generation\n",
    "base_exclusions = {\n",
    "    'site_name', 'site_reference', 'latitude', 'longitude', 'flux_method',\n",
    "    'country', 'land_cover_eco', 'land_cover_plot', 'bawld_class', 'year',\n",
    "    'month', 'SummaryQA', 'land_cover', 'co2_cont', 'date', 'ALT', 'soil', 'land_cover',\n",
    "    'siteID'\n",
    "}\n",
    "\n",
    "# Find all columns that end with the '_100cm' suffix\n",
    "suffix_exclusions = {col for col in df.columns if col.endswith('_100cm')}\n",
    "\n",
    "# Combine all sets of exclusions\n",
    "all_exclusions = base_exclusions.union(suffix_exclusions)\n",
    "\n",
    "# Dynamically create the list of predictors to expand\n",
    "predictors_to_expand = [col for col in df.columns if col not in all_exclusions]\n",
    "print(f\"âœ… Columns selected for feature engineering: {predictors_to_expand}\")\n",
    "\n",
    "# --- Feature Engineering ---\n",
    "\n",
    "# Make a copy to store engineered features\n",
    "df_engineered = df[['site_reference', 'date']].copy()\n",
    "\n",
    "# Sort by site and date for consistent time-series calculations\n",
    "df = df.sort_values(['site_reference', 'date'])\n",
    "\n",
    "# Generate lag, rolling mean, and rolling std features\n",
    "for var in predictors_to_expand:\n",
    "    # Calculate the mean for the current variable, grouped by site\n",
    "    # This will be used to fill NaNs for the initial lag periods.\n",
    "    group_mean = df.groupby('site_reference')[var].transform('mean')\n",
    "\n",
    "    # Generate lag features and fill NaNs with the site-specific group mean\n",
    "    for lag in [1, 2, 3]:\n",
    "        df_engineered[f'{var}_lag{lag}'] = (\n",
    "            df.groupby('site_reference')[var]\n",
    "            .shift(lag)\n",
    "            .fillna(group_mean)\n",
    "        )\n",
    "    \n",
    "    # Generate rolling features (these handle NaNs by default so no change needed)\n",
    "    # df_engineered[f'{var}_roll3_mean'] = (\n",
    "    #     df.groupby('site_reference')[var]\n",
    "    #     .shift(1)\n",
    "    #     .rolling(window=3, min_periods=1)\n",
    "    #     .mean()\n",
    "    #     .reset_index(level=0, drop=True)\n",
    "        \n",
    "    # )\n",
    "    \n",
    "    # df_engineered[f'{var}_roll3_std'] = (\n",
    "    #     df.groupby('site_reference')[var]\n",
    "    #     .shift(1)\n",
    "    #     .rolling(window=3, min_periods=1)\n",
    "    #     .std()\n",
    "    #     .reset_index(level=0, drop=True)\n",
    "    # )\n",
    "    \n",
    "    # df_engineered[f'{var}_roll3_std'] = (\n",
    "    #     df.groupby('site_reference')[var]\n",
    "    #     .shift(1)\n",
    "    #     .rolling(window=3, min_periods=1)\n",
    "    #     .std()\n",
    "    #     .reset_index(level=0, drop=True)\n",
    "    #     .fillna(0)  # <-- Add this line to fill NaNs with 0\n",
    "    # )\n",
    "\n",
    "# --- Finalizing DataFrame ---\n",
    "\n",
    "# Merge engineered features back into original dataframe\n",
    "df_full = pd.merge(df, df_engineered, on=['site_reference', 'date'], how='left')\n",
    "\n",
    "# Save to new CSV\n",
    "output_path = \"/explore/nobackup/people/spotter5/anna_v/v2/v2_model_training_data_with_lags_filled.csv\"\n",
    "df_full.to_csv(output_path, index=False)\n",
    "print(f\"ðŸŽ‰ Successfully saved data with new features to: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "19a78692-44f2-4e06-a252-cc64459dc0e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12165, 52)\n",
      "(12165, 127)\n"
     ]
    }
   ],
   "source": [
    "print(df.shape)\n",
    "print(df_full.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "deada692-6bd1-4113-a42e-bba5bda9db83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>site_reference</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>185071</th>\n",
       "      <td>Barrow-CMDL_US-Brw_tower</td>\n",
       "      <td>2013</td>\n",
       "      <td>9</td>\n",
       "      <td>2013-09-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185072</th>\n",
       "      <td>Barrow-CMDL_US-Brw_tower</td>\n",
       "      <td>2013</td>\n",
       "      <td>9</td>\n",
       "      <td>2013-09-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185073</th>\n",
       "      <td>Barrow-CMDL_US-Brw_tower</td>\n",
       "      <td>2013</td>\n",
       "      <td>9</td>\n",
       "      <td>2013-09-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185074</th>\n",
       "      <td>Barrow-CMDL_US-Brw_tower</td>\n",
       "      <td>2013</td>\n",
       "      <td>9</td>\n",
       "      <td>2013-09-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194296</th>\n",
       "      <td>Barrow-CMDL_US-Brw_tower</td>\n",
       "      <td>2014</td>\n",
       "      <td>7</td>\n",
       "      <td>2014-07-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>625479</th>\n",
       "      <td>Barrow-CMDL_US-Brw_tower</td>\n",
       "      <td>2022</td>\n",
       "      <td>11</td>\n",
       "      <td>2022-11-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>627403</th>\n",
       "      <td>Barrow-CMDL_US-Brw_tower</td>\n",
       "      <td>2022</td>\n",
       "      <td>12</td>\n",
       "      <td>2022-12-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>627404</th>\n",
       "      <td>Barrow-CMDL_US-Brw_tower</td>\n",
       "      <td>2022</td>\n",
       "      <td>12</td>\n",
       "      <td>2022-12-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>627405</th>\n",
       "      <td>Barrow-CMDL_US-Brw_tower</td>\n",
       "      <td>2022</td>\n",
       "      <td>12</td>\n",
       "      <td>2022-12-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>627406</th>\n",
       "      <td>Barrow-CMDL_US-Brw_tower</td>\n",
       "      <td>2022</td>\n",
       "      <td>12</td>\n",
       "      <td>2022-12-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>432 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  site_reference  year  month       date\n",
       "185071  Barrow-CMDL_US-Brw_tower  2013      9 2013-09-01\n",
       "185072  Barrow-CMDL_US-Brw_tower  2013      9 2013-09-01\n",
       "185073  Barrow-CMDL_US-Brw_tower  2013      9 2013-09-01\n",
       "185074  Barrow-CMDL_US-Brw_tower  2013      9 2013-09-01\n",
       "194296  Barrow-CMDL_US-Brw_tower  2014      7 2014-07-01\n",
       "...                          ...   ...    ...        ...\n",
       "625479  Barrow-CMDL_US-Brw_tower  2022     11 2022-11-01\n",
       "627403  Barrow-CMDL_US-Brw_tower  2022     12 2022-12-01\n",
       "627404  Barrow-CMDL_US-Brw_tower  2022     12 2022-12-01\n",
       "627405  Barrow-CMDL_US-Brw_tower  2022     12 2022-12-01\n",
       "627406  Barrow-CMDL_US-Brw_tower  2022     12 2022-12-01\n",
       "\n",
       "[432 rows x 4 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Place this code after loading the CSV\n",
    "df = pd.read_csv(\"/explore/nobackup/people/spotter5/anna_v/v2/v2_model_training_alt_soil_lc_co2.csv\")\n",
    "# df = pd.read_csv(\"/explore/nobackup/people/spotter5/anna_v/v2/v2_model_training_data_v2.csv\")\n",
    "\n",
    "\n",
    "df = df[df['site_reference'] == 'Barrow-CMDL_US-Brw_tower']\n",
    "\n",
    "df = df[df['flux_method'] == 'EC']\n",
    "\n",
    "\n",
    "# --- Data Preprocessing ---\n",
    "\n",
    "# Create tmean_C and date\n",
    "df['tmean_C'] = df[['tmmx', 'tmmn']].mean(axis=1)\n",
    "df['date'] = pd.to_datetime(df[['year', 'month']].assign(day=1))\n",
    "\n",
    "df = df[['site_reference', 'year', 'month', 'date']]\n",
    "\n",
    "df\n",
    "# # Find and display any rows with duplicate site_reference and date\n",
    "# duplicate_rows = df[df.duplicated(subset=['site_reference', 'date'], keep=False)]\n",
    "\n",
    "# if not duplicate_rows.empty:\n",
    "#     print(\"ðŸš¨ Found duplicate rows based on site_reference and date:\")\n",
    "#     print(duplicate_rows.sort_values(['site_reference', 'date']))\n",
    "# else:\n",
    "#     print(\"âœ… No duplicate rows found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51b34c97-38ef-4137-8486-edea4b185063",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Total predictors selected: 24\n",
      "\n",
      "ðŸš€ Processing chunk 1 with predictors: ['EVI', 'NDVI', 'sur_refl_b01', 'sur_refl_b02', 'sur_refl_b03', 'sur_refl_b07', 'NDWI', 'aet', 'def', 'pdsi']\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 487. GiB for an array with shape (50, 1307431757) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 80\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fname\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.parquet\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     79\u001b[0m     chunk_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_parquet(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(interim_dir, fname))\n\u001b[0;32m---> 80\u001b[0m     df_out \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmerge\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_out\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_cols\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mleft\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     81\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mðŸ§© Merged \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m chunk_df\n",
      "File \u001b[0;32m~/.conda/envs/xgboost_gpu/lib/python3.9/site-packages/pandas/core/reshape/merge.py:124\u001b[0m, in \u001b[0;36mmerge\u001b[0;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;129m@Substitution\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mleft : DataFrame or named Series\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     94\u001b[0m \u001b[38;5;129m@Appender\u001b[39m(_merge_doc, indents\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmerge\u001b[39m(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    108\u001b[0m     validate: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    109\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame:\n\u001b[1;32m    110\u001b[0m     op \u001b[38;5;241m=\u001b[39m _MergeOperation(\n\u001b[1;32m    111\u001b[0m         left,\n\u001b[1;32m    112\u001b[0m         right,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    122\u001b[0m         validate\u001b[38;5;241m=\u001b[39mvalidate,\n\u001b[1;32m    123\u001b[0m     )\n\u001b[0;32m--> 124\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/xgboost_gpu/lib/python3.9/site-packages/pandas/core/reshape/merge.py:775\u001b[0m, in \u001b[0;36m_MergeOperation.get_result\u001b[0;34m(self, copy)\u001b[0m\n\u001b[1;32m    771\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mleft, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mright \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_indicator_pre_merge(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mleft, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mright)\n\u001b[1;32m    773\u001b[0m join_index, left_indexer, right_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_join_info()\n\u001b[0;32m--> 775\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reindex_and_concat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    776\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjoin_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mleft_indexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mright_indexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\n\u001b[1;32m    777\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    778\u001b[0m result \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_merge_type)\n\u001b[1;32m    780\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindicator:\n",
      "File \u001b[0;32m~/.conda/envs/xgboost_gpu/lib/python3.9/site-packages/pandas/core/reshape/merge.py:750\u001b[0m, in \u001b[0;36m_MergeOperation._reindex_and_concat\u001b[0;34m(self, join_index, left_indexer, right_indexer, copy)\u001b[0m\n\u001b[1;32m    747\u001b[0m left\u001b[38;5;241m.\u001b[39mindex \u001b[38;5;241m=\u001b[39m join_index\n\u001b[1;32m    749\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m right_indexer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 750\u001b[0m     rmgr \u001b[38;5;241m=\u001b[39m \u001b[43mright\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreindex_indexer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    751\u001b[0m \u001b[43m        \u001b[49m\u001b[43mjoin_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    752\u001b[0m \u001b[43m        \u001b[49m\u001b[43mright_indexer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    753\u001b[0m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    754\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    755\u001b[0m \u001b[43m        \u001b[49m\u001b[43monly_slice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    756\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_dups\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    757\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_na_proxy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    758\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    759\u001b[0m     right \u001b[38;5;241m=\u001b[39m right\u001b[38;5;241m.\u001b[39m_constructor(rmgr)\n\u001b[1;32m    760\u001b[0m right\u001b[38;5;241m.\u001b[39mindex \u001b[38;5;241m=\u001b[39m join_index\n",
      "File \u001b[0;32m~/.conda/envs/xgboost_gpu/lib/python3.9/site-packages/pandas/core/internals/managers.py:751\u001b[0m, in \u001b[0;36mBaseBlockManager.reindex_indexer\u001b[0;34m(self, new_axis, indexer, axis, fill_value, allow_dups, copy, only_slice, use_na_proxy)\u001b[0m\n\u001b[1;32m    749\u001b[0m     parent \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m com\u001b[38;5;241m.\u001b[39mall_none(\u001b[38;5;241m*\u001b[39mnew_refs) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n\u001b[1;32m    750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 751\u001b[0m     new_blocks \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    752\u001b[0m         blk\u001b[38;5;241m.\u001b[39mtake_nd(\n\u001b[1;32m    753\u001b[0m             indexer,\n\u001b[1;32m    754\u001b[0m             axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m    755\u001b[0m             fill_value\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    756\u001b[0m                 fill_value \u001b[38;5;28;01mif\u001b[39;00m fill_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m blk\u001b[38;5;241m.\u001b[39mfill_value\n\u001b[1;32m    757\u001b[0m             ),\n\u001b[1;32m    758\u001b[0m         )\n\u001b[1;32m    759\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m blk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks\n\u001b[1;32m    760\u001b[0m     ]\n\u001b[1;32m    761\u001b[0m     new_refs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    762\u001b[0m     parent \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/xgboost_gpu/lib/python3.9/site-packages/pandas/core/internals/managers.py:752\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    749\u001b[0m     parent \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m com\u001b[38;5;241m.\u001b[39mall_none(\u001b[38;5;241m*\u001b[39mnew_refs) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n\u001b[1;32m    750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    751\u001b[0m     new_blocks \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m--> 752\u001b[0m         \u001b[43mblk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake_nd\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    753\u001b[0m \u001b[43m            \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    754\u001b[0m \u001b[43m            \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    755\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    756\u001b[0m \u001b[43m                \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mblk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfill_value\u001b[49m\n\u001b[1;32m    757\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    758\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    759\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m blk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks\n\u001b[1;32m    760\u001b[0m     ]\n\u001b[1;32m    761\u001b[0m     new_refs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    762\u001b[0m     parent \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/xgboost_gpu/lib/python3.9/site-packages/pandas/core/internals/blocks.py:880\u001b[0m, in \u001b[0;36mBlock.take_nd\u001b[0;34m(self, indexer, axis, new_mgr_locs, fill_value)\u001b[0m\n\u001b[1;32m    877\u001b[0m     allow_fill \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    879\u001b[0m \u001b[38;5;66;03m# Note: algos.take_nd has upcast logic similar to coerce_to_target_dtype\u001b[39;00m\n\u001b[0;32m--> 880\u001b[0m new_values \u001b[38;5;241m=\u001b[39m \u001b[43malgos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake_nd\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_fill\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_fill\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfill_value\u001b[49m\n\u001b[1;32m    882\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    884\u001b[0m \u001b[38;5;66;03m# Called from three places in managers, all of which satisfy\u001b[39;00m\n\u001b[1;32m    885\u001b[0m \u001b[38;5;66;03m#  this assertion\u001b[39;00m\n\u001b[1;32m    886\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (axis \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m new_mgr_locs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[0;32m~/.conda/envs/xgboost_gpu/lib/python3.9/site-packages/pandas/core/array_algos/take.py:117\u001b[0m, in \u001b[0;36mtake_nd\u001b[0;34m(arr, indexer, axis, fill_value, allow_fill)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mtake(indexer, fill_value\u001b[38;5;241m=\u001b[39mfill_value, allow_fill\u001b[38;5;241m=\u001b[39mallow_fill)\n\u001b[1;32m    116\u001b[0m arr \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(arr)\n\u001b[0;32m--> 117\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_take_nd_ndarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_fill\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/xgboost_gpu/lib/python3.9/site-packages/pandas/core/array_algos/take.py:158\u001b[0m, in \u001b[0;36m_take_nd_ndarray\u001b[0;34m(arr, indexer, axis, fill_value, allow_fill)\u001b[0m\n\u001b[1;32m    156\u001b[0m     out \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mempty(out_shape, dtype\u001b[38;5;241m=\u001b[39mdtype, order\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mF\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 158\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mempty\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    160\u001b[0m func \u001b[38;5;241m=\u001b[39m _get_take_nd_function(\n\u001b[1;32m    161\u001b[0m     arr\u001b[38;5;241m.\u001b[39mndim, arr\u001b[38;5;241m.\u001b[39mdtype, out\u001b[38;5;241m.\u001b[39mdtype, axis\u001b[38;5;241m=\u001b[39maxis, mask_info\u001b[38;5;241m=\u001b[39mmask_info\n\u001b[1;32m    162\u001b[0m )\n\u001b[1;32m    163\u001b[0m func(arr, indexer, out, fill_value)\n",
      "\u001b[0;31mMemoryError\u001b[0m: Unable to allocate 487. GiB for an array with shape (50, 1307431757) and data type float64"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import gc\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# --- Configuration ---\n",
    "input_csv = \"/explore/nobackup/people/spotter5/anna_v/v2/v2_model_training_alt_soil_lc_co2.csv\"\n",
    "interim_dir = \"/explore/nobackup/people/spotter5/anna_v/v2/temp_chunks\"\n",
    "final_parquet = \"/explore/nobackup/people/spotter5/anna_v/v2/v2_model_training_data_with_lags.parquet\"\n",
    "os.makedirs(interim_dir, exist_ok=True)\n",
    "\n",
    "chunk_size = 10  # Number of predictors per chunk\n",
    "\n",
    "# --- Load and Preprocess ---\n",
    "df = pd.read_csv(input_csv)\n",
    "df['tmean_C'] = df[['tmmx', 'tmmn']].mean(axis=1)\n",
    "df['date'] = pd.to_datetime(df[['year', 'month']].assign(day=1))\n",
    "df = df.sort_values(['site_reference', 'date'])\n",
    "\n",
    "base_cols = ['site_reference', 'date']\n",
    "\n",
    "# Define exclusions\n",
    "base_exclusions = {\n",
    "    'site_name', 'site_reference', 'latitude', 'longitude', 'flux_method',\n",
    "    'country', 'land_cover_eco', 'land_cover_plot', 'bawld_class', 'year',\n",
    "    'month', 'SummaryQA', 'ch4_flux_total', 'land_cover', 'co2_cont', 'date', 'ALT', 'soil', 'siteID'\n",
    "}\n",
    "suffix_exclusions = {col for col in df.columns if col.endswith('_100cm')}\n",
    "all_exclusions = base_exclusions.union(suffix_exclusions)\n",
    "predictors = [col for col in df.columns if col not in all_exclusions]\n",
    "\n",
    "print(f\"âœ… Total predictors selected: {len(predictors)}\")\n",
    "\n",
    "# --- Generate Features in Chunks ---\n",
    "for i in range(0, len(predictors), chunk_size):\n",
    "    chunk = predictors[i:i + chunk_size]\n",
    "    print(f\"\\nðŸš€ Processing chunk {i // chunk_size + 1} with predictors: {chunk}\")\n",
    "\n",
    "    df_chunk = df[base_cols].copy()\n",
    "\n",
    "    for var in chunk:\n",
    "        grouped = df.groupby('site_reference')[var]\n",
    "\n",
    "        df_chunk[f'{var}_lag1'] = grouped.shift(1)\n",
    "        df_chunk[f'{var}_lag2'] = grouped.shift(2)\n",
    "        df_chunk[f'{var}_lag3'] = grouped.shift(3)\n",
    "\n",
    "        df_chunk[f'{var}_roll3_mean'] = (\n",
    "            grouped.apply(lambda x: x.shift(1).rolling(window=3, min_periods=1).mean())\n",
    "            .reset_index(level=0, drop=True)\n",
    "        )\n",
    "        df_chunk[f'{var}_roll3_std'] = (\n",
    "            grouped.apply(lambda x: x.shift(1).rolling(window=3, min_periods=1).std())\n",
    "            .reset_index(level=0, drop=True)\n",
    "        )\n",
    "\n",
    "        del grouped\n",
    "        gc.collect()\n",
    "\n",
    "    # Save engineered chunk to Parquet\n",
    "    chunk_parquet = os.path.join(interim_dir, f\"features_chunk_{i // chunk_size + 1}.parquet\")\n",
    "    df_chunk.to_parquet(chunk_parquet, index=False)\n",
    "    print(f\"ðŸ’¾ Saved chunk to {chunk_parquet}\")\n",
    "    del df_chunk\n",
    "    gc.collect()\n",
    "\n",
    "# --- Merge all chunks and write Parquet ---\n",
    "print(\"\\nðŸ”— Merging all feature chunks...\")\n",
    "\n",
    "# Start with base\n",
    "df_out = df[base_cols + [c for c in df.columns if c not in all_exclusions]].copy()\n",
    "\n",
    "# Merge chunk by chunk\n",
    "for fname in sorted(os.listdir(interim_dir)):\n",
    "    if fname.endswith(\".parquet\"):\n",
    "        chunk_df = pd.read_parquet(os.path.join(interim_dir, fname))\n",
    "        df_out = pd.merge(df_out, chunk_df, on=base_cols, how='left')\n",
    "        print(f\"ðŸ§© Merged {fname}\")\n",
    "        del chunk_df\n",
    "        gc.collect()\n",
    "\n",
    "# Final save to Parquet\n",
    "df_out.to_parquet(final_parquet, index=False)\n",
    "print(f\"\\nâœ… Final Parquet saved to: {final_parquet}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1c3cf3c-c93b-4345-a0b5-635d79b9f5b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(652899, 52)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cab2e04-2933-4544-b5d7-433e2ecc27dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0253522c-3a7b-4fc2-baa6-7c97724617e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load data\n",
    "# Ensure the path to your CSV is correct\n",
    "\n",
    "df = pd.read_csv(\"/explore/nobackup/people/spotter5/anna_v/v2/v2_model_training_alt_soil_lc_co2.csv\")\n",
    "\n",
    "\n",
    "# --- Data Preprocessing ---\n",
    "\n",
    "# Fill ALT with 2000 for NaNs between 1997â€“2021\n",
    "# This assumes 'ALT' column exists.\n",
    "if 'ALT' in df.columns:\n",
    "    time_interval_mask = (df['year'] >= 1997) & (df['year'] <= 2021)\n",
    "    df.loc[time_interval_mask, 'ALT'] = df.loc[time_interval_mask, 'ALT'].fillna(2000)\n",
    "\n",
    "# Create tmean_C and date\n",
    "# Note: Assuming 'tmmx' and 'tmmn' are the correct column names instead of tmax_C/tmin_C\n",
    "df['tmean_C'] = df[['tmmx', 'tmmn']].mean(axis=1)\n",
    "df['date'] = pd.to_datetime(df[['year', 'month']].assign(day=1))\n",
    "\n",
    "# --- Dynamic Feature Selection ---\n",
    "\n",
    "# Define columns to EXCLUDE from lag/rolling feature generation\n",
    "# This includes identifiers, categorical data, dates, and user-specified exclusions\n",
    "base_exclusions = {\n",
    "    'site_name', 'site_reference', 'latitude', 'longitude', 'flux_method',\n",
    "    'country', 'land_cover_eco', 'land_cover_plot', 'bawld_class', 'year',\n",
    "    'month', 'SummaryQA', 'ch4_flux_total', 'land_cover', 'co2_cont', 'date', 'ALT', 'soil', 'land_cover',\n",
    "    'siteID'\n",
    "}\n",
    "\n",
    "# Find all columns that end with the '_100cm' suffix\n",
    "suffix_exclusions = {col for col in df.columns if col.endswith('_100cm')}\n",
    "\n",
    "# Combine all sets of exclusions\n",
    "all_exclusions = base_exclusions.union(suffix_exclusions)\n",
    "\n",
    "# Dynamically create the list of predictors to expand\n",
    "predictors_to_expand = [col for col in df.columns if col not in all_exclusions]\n",
    "print(f\"âœ… Columns selected for feature engineering: {predictors_to_expand}\")\n",
    "\n",
    "# --- Feature Engineering ---\n",
    "\n",
    "# Make a copy to store engineered features\n",
    "df_engineered = df[['site_reference', 'date']].copy()\n",
    "\n",
    "# Sort by site and date for consistent time-series calculations\n",
    "df = df.sort_values(['site_reference', 'date'])\n",
    "\n",
    "# Generate lag, rolling mean, and rolling std features\n",
    "for var in predictors_to_expand:\n",
    "    for lag in [1, 2, 3]:\n",
    "        df_engineered[f'{var}_lag{lag}'] = (\n",
    "            df.groupby('site_reference')[var]\n",
    "            .shift(lag)\n",
    "        )\n",
    "    \n",
    "    df_engineered[f'{var}_roll3_mean'] = (\n",
    "        df.groupby('site_reference')[var]\n",
    "        .shift(1)\n",
    "        .rolling(window=3, min_periods=1)\n",
    "        .mean()\n",
    "        .reset_index(level=0, drop=True) # Add reset_index to align correctly\n",
    "    )\n",
    "    \n",
    "    df_engineered[f'{var}_roll3_std'] = (\n",
    "        df.groupby('site_reference')[var]\n",
    "        .shift(1)\n",
    "        .rolling(window=3, min_periods=1)\n",
    "        .std()\n",
    "        .reset_index(level=0, drop=True) # Add reset_index to align correctly\n",
    "    )\n",
    "\n",
    "# --- Finalizing DataFrame ---\n",
    "\n",
    "# Merge engineered features back into original dataframe\n",
    "df_full = pd.merge(df, df_engineered, on=['site_reference', 'date'], how='left')\n",
    "\n",
    "# Save to new CSV\n",
    "output_path = \"/explore/nobackup/people/spotter5/anna_v/v2/v2_model_training_data_with_lags_alt.csv\"\n",
    "df_full.to_csv(output_path, index=False)\n",
    "print(f\"ðŸŽ‰ Successfully saved data with new features to: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8976b3d2-3fa8-4e0d-8725-a6eaedd52fb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "site_name\n",
      "site_reference\n",
      "latitude\n",
      "longitude\n",
      "flux_method\n",
      "country\n",
      "land_cover_eco\n",
      "land_cover_plot\n",
      "bawld_class\n",
      "year\n",
      "month\n",
      "EVI\n",
      "NDVI\n",
      "SummaryQA\n",
      "sur_refl_b01\n",
      "sur_refl_b02\n",
      "sur_refl_b03\n",
      "sur_refl_b07\n",
      "NDWI\n",
      "pdsi\n",
      "soil\n",
      "srad\n",
      "swe\n",
      "tmmn\n",
      "tmmx\n",
      "vap\n",
      "vs\n",
      "nee\n",
      "gpp\n",
      "reco\n",
      "ch4_flux_total\n",
      "bdod_0_100cm\n",
      "cec_0_100cm\n",
      "cfvo_0_100cm\n",
      "clay_0_100cm\n",
      "nitrogen_0_100cm\n",
      "ocd_0_100cm\n",
      "phh2o_0_100cm\n",
      "sand_0_100cm\n",
      "silt_0_100cm\n",
      "soc_0_100cm\n",
      "land_cover\n",
      "co2_cont\n",
      "tmean_C\n",
      "date\n",
      "EVI_lag1\n",
      "EVI_lag2\n",
      "EVI_lag3\n",
      "EVI_roll3_mean\n",
      "EVI_roll3_std\n",
      "NDVI_lag1\n",
      "NDVI_lag2\n",
      "NDVI_lag3\n",
      "NDVI_roll3_mean\n",
      "NDVI_roll3_std\n",
      "sur_refl_b01_lag1\n",
      "sur_refl_b01_lag2\n",
      "sur_refl_b01_lag3\n",
      "sur_refl_b01_roll3_mean\n",
      "sur_refl_b01_roll3_std\n",
      "sur_refl_b02_lag1\n",
      "sur_refl_b02_lag2\n",
      "sur_refl_b02_lag3\n",
      "sur_refl_b02_roll3_mean\n",
      "sur_refl_b02_roll3_std\n",
      "sur_refl_b03_lag1\n",
      "sur_refl_b03_lag2\n",
      "sur_refl_b03_lag3\n",
      "sur_refl_b03_roll3_mean\n",
      "sur_refl_b03_roll3_std\n",
      "sur_refl_b07_lag1\n",
      "sur_refl_b07_lag2\n",
      "sur_refl_b07_lag3\n",
      "sur_refl_b07_roll3_mean\n",
      "sur_refl_b07_roll3_std\n",
      "NDWI_lag1\n",
      "NDWI_lag2\n",
      "NDWI_lag3\n",
      "NDWI_roll3_mean\n",
      "NDWI_roll3_std\n",
      "pdsi_lag1\n",
      "pdsi_lag2\n",
      "pdsi_lag3\n",
      "pdsi_roll3_mean\n",
      "pdsi_roll3_std\n",
      "soil_lag1\n",
      "soil_lag2\n",
      "soil_lag3\n",
      "soil_roll3_mean\n",
      "soil_roll3_std\n",
      "srad_lag1\n",
      "srad_lag2\n",
      "srad_lag3\n",
      "srad_roll3_mean\n",
      "srad_roll3_std\n",
      "swe_lag1\n",
      "swe_lag2\n",
      "swe_lag3\n",
      "swe_roll3_mean\n",
      "swe_roll3_std\n",
      "tmmn_lag1\n",
      "tmmn_lag2\n",
      "tmmn_lag3\n",
      "tmmn_roll3_mean\n",
      "tmmn_roll3_std\n",
      "tmmx_lag1\n",
      "tmmx_lag2\n",
      "tmmx_lag3\n",
      "tmmx_roll3_mean\n",
      "tmmx_roll3_std\n",
      "vap_lag1\n",
      "vap_lag2\n",
      "vap_lag3\n",
      "vap_roll3_mean\n",
      "vap_roll3_std\n",
      "vs_lag1\n",
      "vs_lag2\n",
      "vs_lag3\n",
      "vs_roll3_mean\n",
      "vs_roll3_std\n",
      "nee_lag1\n",
      "nee_lag2\n",
      "nee_lag3\n",
      "nee_roll3_mean\n",
      "nee_roll3_std\n",
      "gpp_lag1\n",
      "gpp_lag2\n",
      "gpp_lag3\n",
      "gpp_roll3_mean\n",
      "gpp_roll3_std\n",
      "reco_lag1\n",
      "reco_lag2\n",
      "reco_lag3\n",
      "reco_roll3_mean\n",
      "reco_roll3_std\n",
      "tmean_C_lag1\n",
      "tmean_C_lag2\n",
      "tmean_C_lag3\n",
      "tmean_C_roll3_mean\n",
      "tmean_C_roll3_std\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('/explore/nobackup/people/spotter5/anna_v/v2/v2_model_training_data_with_lags.csv')\n",
    "\n",
    "for i in df.columns:\n",
    "\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1745d1-d30d-48be-99b5-afcda07691f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-xgboost_gpu]",
   "language": "python",
   "name": "conda-env-.conda-xgboost_gpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
