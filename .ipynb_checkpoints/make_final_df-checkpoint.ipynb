{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "165e9317-90a1-4278-a30a-c4b3b14bba9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/explore/nobackup/people/spotter5/temp_dir/ipykernel_129052/2546976097.py:5: DtypeWarning: Columns (47) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  input_data = pd.read_csv(\"/explore/nobackup/people/spotter5/anna_v/v2/v2_model_training_data_v4.csv\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial shape: (56731, 51)\n",
      "After soil merge: (56731, 61)\n",
      "After landcover merge: (56731, 62)\n",
      "After CO2 merge: (56731, 63)\n",
      "After ALT merge: (56731, 64)\n",
      "After soil moisture merge: (56731, 66)\n",
      "\n",
      "Successfully merged all data and saved to: /explore/nobackup/people/spotter5/anna_v/v2/v2_model_training_final.csv\n",
      "Final DataFrame head:\n",
      "                                           site_name  \\\n",
      "0                                         Skyttorp 2   \n",
      "1                                  Wolf_creek_forest   \n",
      "2  Alberta - Western Peatland - LaBiche River,Bla...   \n",
      "3                             Elgeeii forest station   \n",
      "4                                           Faejemyr   \n",
      "\n",
      "                                      site_reference   latitude   longitude  \\\n",
      "0                            Skyttorp 2_SE-Sk2_tower  60.129667   17.840056   \n",
      "1                     Wolf_creek_forest_CA-WCF_tower  60.596886 -134.952833   \n",
      "2  Alberta - Western Peatland - LaBiche River,Bla...  54.953840 -112.466980   \n",
      "3                Elgeeii forest station_RU-Ege_tower  60.015516  133.824012   \n",
      "4                              Faejemyr_SE-Faj_tower  56.265500   13.553500   \n",
      "\n",
      "  flux_method country  land_cover_eco  land_cover_plot    bawld_class  year  \\\n",
      "0          EC  Sweden            70.0             70.0  Boreal Forest  2000   \n",
      "1          EC  Canada            70.0             70.0  Boreal Forest  2000   \n",
      "2          EC  Canada           160.0            160.0            Fen  2000   \n",
      "3          EC  Russia            90.0             90.0  Boreal Forest  2000   \n",
      "4          EC  Sweden           180.0            180.0            Bog  2000   \n",
      "\n",
      "   ...  ocd_0_100cm phh2o_0_100cm  sand_0_100cm  silt_0_100cm  soc_0_100cm  \\\n",
      "0  ...          NaN           NaN           NaN           NaN          NaN   \n",
      "1  ...    21.602915      7.352632     55.487802     28.810267    11.866563   \n",
      "2  ...    13.246189      6.252252     50.121325     30.162230    35.182187   \n",
      "3  ...    17.086907      7.130071     41.499711     31.082185    53.577813   \n",
      "4  ...          NaN           NaN           NaN           NaN          NaN   \n",
      "\n",
      "   land_cover  co2_cont   ALT  sm_surface  sm_rootzone  \n",
      "0           7    376.89   NaN    0.339861     0.331989  \n",
      "1           6    376.89  1.10    0.253667     0.185445  \n",
      "2           1    374.96   NaN    0.187536     0.207688  \n",
      "3           4    374.96  1.25    0.158696     0.128285  \n",
      "4          13    374.96   NaN    0.343885     0.308715  \n",
      "\n",
      "[5 rows x 66 columns]\n",
      "\n",
      "Final DataFrame columns:\n",
      "Index(['site_name', 'site_reference', 'latitude', 'longitude', 'flux_method',\n",
      "       'country', 'land_cover_eco', 'land_cover_plot', 'bawld_class', 'year',\n",
      "       'month', 'siteID', 'EVI', 'NDVI', 'SummaryQA', 'sur_refl_b01',\n",
      "       'sur_refl_b02', 'sur_refl_b03', 'sur_refl_b07', 'NDWI', 'aet', 'def',\n",
      "       'pdsi', 'pet', 'pr', 'ro', 'soil', 'srad', 'swe', 'tmmn', 'tmmx', 'vap',\n",
      "       'vpd', 'vs', 'lai', 'fpar', 'Percent_NonTree_Vegetation',\n",
      "       'Percent_NonVegetated', 'Percent_Tree_Cover', 'snow_cover',\n",
      "       'snow_depth', 'NDSI_snow_cover', 'nee', 'gpp', 'reco', 'ch4_flux_total',\n",
      "       'expert_flag_co2', 'expert_flag_ch4', 'expert_flag_gpp',\n",
      "       'expert_flag_reco', 'Flux', 'bdod_0_100cm', 'cec_0_100cm',\n",
      "       'cfvo_0_100cm', 'clay_0_100cm', 'nitrogen_0_100cm', 'ocd_0_100cm',\n",
      "       'phh2o_0_100cm', 'sand_0_100cm', 'silt_0_100cm', 'soc_0_100cm',\n",
      "       'land_cover', 'co2_cont', 'ALT', 'sm_surface', 'sm_rootzone'],\n",
      "      dtype='object')\n",
      "\n",
      "Data type of 'land_cover' column: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# --- Load all data sources ---\n",
    "input_data = pd.read_csv(\"/explore/nobackup/people/spotter5/anna_v/v2/v2_model_training_data_v4.csv\")\n",
    "soil       = pd.read_csv(\"/explore/nobackup/people/spotter5/anna_v/v2/integrated_soil_data_1km_v2_sites.csv\")\n",
    "landcover  = pd.read_csv(\"/explore/nobackup/people/spotter5/anna_v/v2/extracted_landcover_values_v2.csv\")\n",
    "# keep these for potential use, but we‚Äôll rename properly below\n",
    "landcover  = landcover[['site_refer', 'land_cover_code']]\n",
    "\n",
    "sm         = pd.read_csv(\"/explore/nobackup/people/spotter5/anna_v/v2/soil_moisture_by_site_monthly_2000_2023.csv\")\n",
    "cont       = pd.read_csv(\"/explore/nobackup/people/spotter5/anna_v/v2/co2_cont.csv\")\n",
    "alt        = pd.read_csv(\"/explore/nobackup/people/spotter5/anna_v/v2/ALT_by_site.csv\")\n",
    "\n",
    "# --- Initial Data Cleaning ---\n",
    "# Keep EC only, drop rows without site_reference\n",
    "input_data = input_data[input_data['flux_method'] == 'EC'].copy()\n",
    "input_data = input_data.dropna(subset=['site_reference'])\n",
    "\n",
    "# Drop rows without keys in other tables\n",
    "soil      = soil.dropna(subset=['site_refer']).copy()\n",
    "landcover = landcover.dropna(subset=['site_refer']).copy()\n",
    "\n",
    "# Ensure key types are consistent before merging\n",
    "for df in [input_data, alt, sm]:\n",
    "    if 'site_reference' in df.columns:\n",
    "        df['site_reference'] = df['site_reference'].astype(str)\n",
    "for df in [soil, landcover]:\n",
    "    if 'site_refer' in df.columns:\n",
    "        df['site_refer'] = df['site_refer'].astype(str)\n",
    "\n",
    "for df in [input_data, alt, sm, cont]:\n",
    "    if 'year' in df.columns:\n",
    "        df['year'] = pd.to_numeric(df['year'], errors='coerce').astype('Int64')\n",
    "    if 'month' in df.columns:\n",
    "        df['month'] = pd.to_numeric(df['month'], errors='coerce').astype('Int64')\n",
    "\n",
    "# Deduplicate on merge keys\n",
    "input_data = input_data.drop_duplicates(subset=['site_reference', 'year', 'month'])\n",
    "soil       = soil.drop_duplicates(subset=['site_refer'])\n",
    "landcover  = landcover.drop_duplicates(subset=['site_refer'])\n",
    "alt        = alt.drop_duplicates(subset=['site_reference', 'year'])\n",
    "sm         = sm.drop_duplicates(subset=['site_reference', 'year', 'month'])\n",
    "\n",
    "print(f\"Initial shape: {input_data.shape}\")\n",
    "\n",
    "# --- Prepare and Merge Soil (static) ---\n",
    "# keep only 100 cm depth columns; carry site_reference for join\n",
    "soil_filtered = soil.filter(regex='100cm$').copy()\n",
    "soil_filtered[\"site_reference\"] = soil[\"site_refer\"].values\n",
    "input_data = input_data.merge(soil_filtered, on=\"site_reference\", how=\"left\", validate=\"m:1\")\n",
    "print(f\"After soil merge: {input_data.shape}\")\n",
    "\n",
    "# --- Prepare and Merge Land Cover (static) ---\n",
    "# rename to match keys and column you want in final data\n",
    "landcover = landcover.rename(columns={'site_refer': 'site_reference',\n",
    "                                      'land_cover_code': 'land_cover'})\n",
    "# keep only the necessary columns (keep lat/lon too if you want them in final)\n",
    "landcover = landcover[['site_reference', 'land_cover']]\n",
    "input_data = input_data.merge(landcover, on=\"site_reference\", how=\"left\", validate=\"m:1\")\n",
    "print(f\"After landcover merge: {input_data.shape}\")\n",
    "\n",
    "# --- Prepare and Merge CO2 (time-varying by year/month) ---\n",
    "co2_to_merge = cont[['year', 'month', 'value']].copy()\n",
    "co2_to_merge = co2_to_merge.rename(columns={'value': 'co2_cont'})\n",
    "co2_to_merge = co2_to_merge.drop_duplicates(subset=['year', 'month'])\n",
    "input_data = input_data.merge(co2_to_merge, on=['year', 'month'], how='left', validate=\"m:1\")\n",
    "print(f\"After CO2 merge: {input_data.shape}\")\n",
    "\n",
    "# --- Prepare and Merge ALT (time-varying by site/year) ---\n",
    "alt_to_merge = alt[['site_reference', 'year', 'ALT']].copy()\n",
    "alt_to_merge = alt_to_merge.drop_duplicates(subset=['site_reference', 'year'])\n",
    "input_data = input_data.merge(alt_to_merge, on=['site_reference', 'year'], how='left', validate=\"m:1\")\n",
    "print(f\"After ALT merge: {input_data.shape}\")\n",
    "\n",
    "# --- Prepare and Merge Soil Moisture (time-varying by site/year/month) ---\n",
    "# Expecting columns: site_reference, year, month, sm_surface, sm_rootzone\n",
    "needed_cols = {'site_reference', 'year', 'month', 'sm_surface', 'sm_rootzone'}\n",
    "missing = needed_cols.difference(set(sm.columns))\n",
    "if missing:\n",
    "    raise ValueError(f\"Soil moisture CSV is missing expected columns: {missing}\")\n",
    "\n",
    "input_data = input_data.merge(\n",
    "    sm[['site_reference', 'year', 'month', 'sm_surface', 'sm_rootzone']],\n",
    "    on=['site_reference', 'year', 'month'],\n",
    "    how='left',\n",
    "    validate='m:1'\n",
    ")\n",
    "print(f\"After soil moisture merge: {input_data.shape}\")\n",
    "\n",
    "# --- Final Data Type Conversion for Land Cover ---\n",
    "# Fill any missing values (NaN) in 'land_cover' with -9999 and cast to int\n",
    "if 'land_cover' in input_data.columns:\n",
    "    input_data['land_cover'] = input_data['land_cover'].fillna(-9999).astype(int)\n",
    "\n",
    "# --- Save Final Combined Data ---\n",
    "output_path_final = \"/explore/nobackup/people/spotter5/anna_v/v2/v2_model_training_final.csv\"\n",
    "input_data.to_csv(output_path_final, index=False)\n",
    "\n",
    "print(f\"\\nSuccessfully merged all data and saved to: {output_path_final}\")\n",
    "print(\"Final DataFrame head:\")\n",
    "print(input_data.head())\n",
    "print(\"\\nFinal DataFrame columns:\")\n",
    "print(input_data.columns)\n",
    "if 'land_cover' in input_data.columns:\n",
    "    print(f\"\\nData type of 'land_cover' column: {input_data['land_cover'].dtype}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e3b1db0-2584-4e92-9b23-b61d06ab9399",
   "metadata": {},
   "source": [
    "With depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626142da-136d-4aee-885b-bae06eb37ba7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "üöÄ STARTING ANALYSIS | TARGET: 'below.ground.carbon.combusted'\n",
      "======================================================================\n",
      "Step 1: Loading and preparing data...\n",
      "Step 2: Training Random Forest models on 'below.ground.carbon.combusted'...\n",
      "  - Training on 'All Data' (1877 rows)...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.inspection import PartialDependenceDisplay\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "def run_pdp_analysis(target_variable, output_directory, input_csv_path, old_ids_csv_path):\n",
    "    \"\"\"\n",
    "    Runs the full modeling and PDP generation pipeline for a specific target variable.\n",
    "\n",
    "    Args:\n",
    "        target_variable (str): The name of the column to use as the target.\n",
    "        output_directory (str): The path to save the generated PDP images.\n",
    "        input_csv_path (str): Path to the main input CSV file.\n",
    "        old_ids_csv_path (str): Path to the CSV containing old plot IDs and, if available, burn.depth.\n",
    "    \"\"\"\n",
    "    # --- Setup & Introduction ---\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"üöÄ STARTING ANALYSIS | TARGET: '{target_variable}'\")\n",
    "    print(f\"{'='*70}\")\n",
    "\n",
    "    warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "    warnings.filterwarnings('ignore', category=UserWarning)\n",
    "    os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "    # --- 1. Data Preparation ---\n",
    "    print(\"Step 1: Loading and preparing data...\")\n",
    "    try:\n",
    "        df_main = pd.read_csv(input_csv_path)\n",
    "        df_old_ids = pd.read_csv(old_ids_csv_path)\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"‚ùå Error: Could not find input file. {e}\")\n",
    "        return\n",
    "\n",
    "    # Normalize 'id' types & dedupe\n",
    "    if 'id' not in df_main.columns or 'id' not in df_old_ids.columns:\n",
    "        print(\"‚ùå Error: Both input files must have an 'id' column.\")\n",
    "        return\n",
    "\n",
    "    for df in (df_main, df_old_ids):\n",
    "        # coerce to numeric if possible (keeps strings if not)\n",
    "        try:\n",
    "            df['id'] = pd.to_numeric(df['id'], errors='ignore')\n",
    "        except Exception:\n",
    "            pass\n",
    "    df_old_ids = df_old_ids.drop_duplicates(subset='id')\n",
    "\n",
    "    # If we're modeling burn.depth, try to backfill from old file (if present there)\n",
    "    if target_variable == 'burn.depth' and 'burn.depth' in df_old_ids.columns:\n",
    "        print(\"  - Backfilling 'burn.depth' from old CSV where missing in main...\")\n",
    "        before_na = df_main['burn.depth'].isna().sum() if 'burn.depth' in df_main.columns else None\n",
    "        if 'burn.depth' not in df_main.columns:\n",
    "            df_main['burn.depth'] = np.nan\n",
    "        df_main = df_main.set_index('id')\n",
    "        df_old_depth = df_old_ids.set_index('id')['burn.depth']\n",
    "        df_main['burn.depth'] = df_main['burn.depth'].combine_first(df_old_depth)\n",
    "        df_main = df_main.reset_index()\n",
    "        after_na = df_main['burn.depth'].isna().sum()\n",
    "        if before_na is not None:\n",
    "            print(f\"    Filled {before_na - after_na} missing burn.depth values.\")\n",
    "\n",
    "    # Define columns to exclude from predictors (X)\n",
    "    POTENTIAL_TARGETS = ['below.ground.carbon.combusted', 'above.carbon.combusted', 'burn.depth']\n",
    "    METADATA_COLUMNS = [\n",
    "        'burn_year', 'project.name', 'latitude', 'longitude', 'Date', 'id', 'CNA_MAR'\n",
    "    ]\n",
    "    COLS_TO_DROP_FROM_X = POTENTIAL_TARGETS + METADATA_COLUMNS\n",
    "\n",
    "    # Prepare splits\n",
    "    old_ids = df_old_ids['id'].unique()\n",
    "    data_splits = {\n",
    "        \"All Data\": df_main,\n",
    "        \"Old Data\": df_main[df_main['id'].isin(old_ids)],\n",
    "        \"New Data\": df_main[~df_main['id'].isin(old_ids)]\n",
    "    }\n",
    "\n",
    "    # --- 2. Model Training ---\n",
    "    print(f\"Step 2: Training Random Forest models on '{target_variable}'...\")\n",
    "    models = {}\n",
    "    for name, data in data_splits.items():\n",
    "        print(f\"  - Training on '{name}' ({len(data)} rows)...\")\n",
    "\n",
    "        if target_variable not in data.columns:\n",
    "            print(f\"    ‚ö†Ô∏è Skipping '{name}' ‚Äì target '{target_variable}' not in columns.\")\n",
    "            continue\n",
    "\n",
    "        # Drop rows missing the CURRENT target variable\n",
    "        df_clean = data.dropna(subset=[target_variable]).copy()\n",
    "\n",
    "        # Build X (numeric only), drop constant/all-NaN cols\n",
    "        X = df_clean.drop(columns=COLS_TO_DROP_FROM_X, errors='ignore')\n",
    "        X = X.select_dtypes(include=[np.number])\n",
    "        if X.shape[1] == 0:\n",
    "            print(f\"    ‚ö†Ô∏è Skipping '{name}' ‚Äì no numeric predictors after cleaning.\")\n",
    "            continue\n",
    "        # drop all-NaN columns\n",
    "        X = X.loc[:, X.notna().any(axis=0)]\n",
    "        # drop constant columns\n",
    "        constant_cols = [c for c in X.columns if X[c].nunique(dropna=True) <= 1]\n",
    "        if constant_cols:\n",
    "            X = X.drop(columns=constant_cols)\n",
    "\n",
    "        y = df_clean[target_variable].astype(float)\n",
    "\n",
    "        n = len(y)\n",
    "        if n < 2:\n",
    "            print(f\"    ‚ö†Ô∏è Skipping '{name}' ‚Äì insufficient samples after dropna (n={n}).\")\n",
    "            continue\n",
    "\n",
    "        # Enable OOB only when there are enough samples to make it meaningful\n",
    "        use_oob = n > 10\n",
    "        rf = RandomForestRegressor(\n",
    "            n_estimators=500,\n",
    "            random_state=42,\n",
    "            n_jobs=-1,\n",
    "            oob_score=use_oob\n",
    "        )\n",
    "        rf.fit(X, y)\n",
    "        if use_oob:\n",
    "            print(f\"    ...Done. Model OOB Score (R¬≤): {rf.oob_score_:.3f}\")\n",
    "        else:\n",
    "            print(f\"    ...Done. (OOB disabled; n={n})\")\n",
    "\n",
    "        models[name] = {'model': rf, 'X': X}\n",
    "\n",
    "    if \"All Data\" not in models:\n",
    "        print(\"‚ùå No trainable 'All Data' model. Aborting PDP stage.\")\n",
    "        return\n",
    "\n",
    "    # --- 3. Generate and Save Partial Dependence Plots ---\n",
    "    feature_list = models['All Data']['X'].columns\n",
    "    print(f\"\\nStep 3: Generating {len(feature_list)} Partial Dependence Plots...\")\n",
    "\n",
    "    # Only include splits that successfully trained\n",
    "    trained_order = [k for k in [\"All Data\", \"Old Data\", \"New Data\"] if k in models]\n",
    "    if not trained_order:\n",
    "        print(\"‚ùå No trained models available for PDP.\")\n",
    "        return\n",
    "\n",
    "    for feature in feature_list:\n",
    "        print(f\"  - Plotting for: {feature}\")\n",
    "        fig, axes = plt.subplots(len(trained_order), 1, figsize=(8, 4 * len(trained_order)), sharex=True)\n",
    "        if len(trained_order) == 1:\n",
    "            axes = [axes]\n",
    "        fig.suptitle(f'Partial Dependence on: {feature}\\n(Target: {target_variable})', fontsize=16, y=0.96)\n",
    "\n",
    "        for ax, model_name in zip(axes, trained_order):\n",
    "            model_info = models[model_name]\n",
    "            try:\n",
    "                PartialDependenceDisplay.from_estimator(\n",
    "                    estimator=model_info['model'], X=model_info['X'], features=[feature],\n",
    "                    ax=ax, line_kw={\"color\": \"darkcyan\", \"linewidth\": 2.5}\n",
    "                )\n",
    "                ax.set_title(f\"{model_name} (n={len(model_info['X'])})\")\n",
    "                ax.set_ylabel(\"Partial Dependence\")\n",
    "                ax.grid(True, linestyle='--', alpha=0.6)\n",
    "            except Exception as e:\n",
    "                ax.set_title(f\"{model_name} ‚Äì PDP failed for '{feature}' ({e})\")\n",
    "                ax.axis('off')\n",
    "\n",
    "        save_path = os.path.join(output_directory, f'{feature}.png')\n",
    "        plt.tight_layout(rect=[0, 0, 1, 0.94])\n",
    "        plt.savefig(save_path)\n",
    "        plt.close(fig)\n",
    "\n",
    "    print(f\"\\n‚úÖ ANALYSIS COMPLETE for '{target_variable}'. Plots saved to: {output_directory}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # --- Master Configuration ---\n",
    "    INPUT_CSV = \"/explore/nobackup/people/spotter5/new_combustion/2025-08-13_LC_FISL_Original_combustionModelPredictors.csv\"\n",
    "    OLD_PREDICTORS_CSV = \"/explore/nobackup/people/spotter5/new_combustion/Combustion_SynthesisData_05042018_XJW.csv\"\n",
    "    BASE_OUT_PATH = \"/explore/nobackup/people/spotter5/new_combustion\"\n",
    "\n",
    "    analyses = {\n",
    "        'below.ground.carbon.combusted': os.path.join(BASE_OUT_PATH, \"pdp_belowground\"),\n",
    "        'above.carbon.combusted': os.path.join(BASE_OUT_PATH, \"pdp_aboveground\"),\n",
    "        'burn.depth': os.path.join(BASE_OUT_PATH, \"pdp_depth\")\n",
    "    }\n",
    "\n",
    "    for target_col, out_dir in analyses.items():\n",
    "        run_pdp_analysis(\n",
    "            target_variable=target_col,\n",
    "            output_directory=out_dir,\n",
    "            input_csv_path=INPUT_CSV,\n",
    "            old_ids_csv_path=OLD_PREDICTORS_CSV\n",
    "        )\n",
    "\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"üéâ All tasks finished.\")\n",
    "    print(f\"{'='*70}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ae019e67-8998-44d0-a3fc-8e0fe33453da",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/explore/nobackup/people/spotter5/anna_v/v2/v2_model_training_data_final.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m input_data \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/explore/nobackup/people/spotter5/anna_v/v2/v2_model_training_data_final.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m input_data \u001b[38;5;241m=\u001b[39m input_data[input_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msite_reference\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mZackenberg Heath_GL-ZaH_tower\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      5\u001b[0m input_data \u001b[38;5;241m=\u001b[39m input_data[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124myear\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmonth\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnee\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtmmx\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtmmn\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpr\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n",
      "File \u001b[0;32m~/.conda/envs/xgboost_gpu/lib/python3.9/site-packages/pandas/util/_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[38;5;241m=\u001b[39m new_arg_value\n\u001b[0;32m--> 211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/xgboost_gpu/lib/python3.9/site-packages/pandas/util/_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    330\u001b[0m     )\n\u001b[0;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/xgboost_gpu/lib/python3.9/site-packages/pandas/io/parsers/readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    946\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m    947\u001b[0m )\n\u001b[1;32m    948\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 950\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/xgboost_gpu/lib/python3.9/site-packages/pandas/io/parsers/readers.py:605\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    602\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    604\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 605\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    607\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    608\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/.conda/envs/xgboost_gpu/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1442\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1439\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1441\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1442\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/xgboost_gpu/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1735\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1733\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1734\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1735\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1736\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1737\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1738\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1739\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1740\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1741\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1742\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1743\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1744\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/.conda/envs/xgboost_gpu/lib/python3.9/site-packages/pandas/io/common.py:856\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    851\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    852\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    853\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    854\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    855\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 856\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    857\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    858\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    859\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    860\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    861\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    862\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    863\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    864\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    865\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/explore/nobackup/people/spotter5/anna_v/v2/v2_model_training_data_final.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "input_data = pd.read_csv(\"/explore/nobackup/people/spotter5/anna_v/v2/v2_model_training_data_final.csv\")\n",
    "input_data = input_data[input_data['site_reference'] == 'Zackenberg Heath_GL-ZaH_tower']\n",
    "\n",
    "input_data = input_data[['year', 'month', 'nee', 'tmmx', 'tmmn', 'pr']]\n",
    "input_data.sort_values(by = 'year')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "81af1eca-23ec-4af4-adfa-c4c8057a483d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>site_reference</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>land_cover_eco</th>\n",
       "      <th>land_cover_plot</th>\n",
       "      <th>bawld_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Skyttorp 2_SE-Sk2_tower</td>\n",
       "      <td>60.129667</td>\n",
       "      <td>17.840056</td>\n",
       "      <td>70.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>Boreal Forest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Wolf_creek_forest_CA-WCF_tower</td>\n",
       "      <td>60.596886</td>\n",
       "      <td>-134.952833</td>\n",
       "      <td>70.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>Boreal Forest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Alberta - Western Peatland - LaBiche River,Bla...</td>\n",
       "      <td>54.953840</td>\n",
       "      <td>-112.466980</td>\n",
       "      <td>160.0</td>\n",
       "      <td>160.0</td>\n",
       "      <td>Fen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Elgeeii forest station_RU-Ege_tower</td>\n",
       "      <td>60.015516</td>\n",
       "      <td>133.824012</td>\n",
       "      <td>90.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>Boreal Forest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Faejemyr_SE-Faj_tower</td>\n",
       "      <td>56.265500</td>\n",
       "      <td>13.553500</td>\n",
       "      <td>180.0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>Bog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>429</th>\n",
       "      <td>ARM-NSA-Barrow_US-A10_tower</td>\n",
       "      <td>71.323000</td>\n",
       "      <td>-156.609000</td>\n",
       "      <td>153.0</td>\n",
       "      <td>153.0</td>\n",
       "      <td>Wet Tundra</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>445</th>\n",
       "      <td>Barrow-CMDL_US-Brw_tower</td>\n",
       "      <td>71.322525</td>\n",
       "      <td>-156.609200</td>\n",
       "      <td>180.0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>Wet Tundra</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>446</th>\n",
       "      <td>Bayelva, Spitsbergen_SJ-Blv_tower</td>\n",
       "      <td>78.921600</td>\n",
       "      <td>11.831100</td>\n",
       "      <td>130.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>Dry Tundra</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>457</th>\n",
       "      <td>Central Marsh_US-Cms_tower</td>\n",
       "      <td>71.320190</td>\n",
       "      <td>-156.622270</td>\n",
       "      <td>180.0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>Wet Tundra</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>476</th>\n",
       "      <td>Finse west_NO-Fns_tower</td>\n",
       "      <td>60.110000</td>\n",
       "      <td>7.530000</td>\n",
       "      <td>120.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>Wet Tundra</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>188 rows √ó 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        site_reference   latitude   longitude  \\\n",
       "0                              Skyttorp 2_SE-Sk2_tower  60.129667   17.840056   \n",
       "1                       Wolf_creek_forest_CA-WCF_tower  60.596886 -134.952833   \n",
       "2    Alberta - Western Peatland - LaBiche River,Bla...  54.953840 -112.466980   \n",
       "3                  Elgeeii forest station_RU-Ege_tower  60.015516  133.824012   \n",
       "4                                Faejemyr_SE-Faj_tower  56.265500   13.553500   \n",
       "..                                                 ...        ...         ...   \n",
       "429                        ARM-NSA-Barrow_US-A10_tower  71.323000 -156.609000   \n",
       "445                           Barrow-CMDL_US-Brw_tower  71.322525 -156.609200   \n",
       "446                  Bayelva, Spitsbergen_SJ-Blv_tower  78.921600   11.831100   \n",
       "457                         Central Marsh_US-Cms_tower  71.320190 -156.622270   \n",
       "476                            Finse west_NO-Fns_tower  60.110000    7.530000   \n",
       "\n",
       "     land_cover_eco  land_cover_plot    bawld_class  \n",
       "0              70.0             70.0  Boreal Forest  \n",
       "1              70.0             70.0  Boreal Forest  \n",
       "2             160.0            160.0            Fen  \n",
       "3              90.0             90.0  Boreal Forest  \n",
       "4             180.0            180.0            Bog  \n",
       "..              ...              ...            ...  \n",
       "429           153.0            153.0     Wet Tundra  \n",
       "445           180.0            180.0     Wet Tundra  \n",
       "446           130.0            130.0     Dry Tundra  \n",
       "457           180.0            180.0     Wet Tundra  \n",
       "476           120.0            120.0     Wet Tundra  \n",
       "\n",
       "[188 rows x 6 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "input_data = pd.read_csv(\"/explore/nobackup/people/spotter5/anna_v/v2/v2_model_training_final.csv\")\n",
    "input_data = input_data.drop_duplicates(subset = 'site_reference')\n",
    "\n",
    "input_data = input_data[['site_reference', 'latitude', 'longitude', 'land_cover_eco', 'land_cover_plot', 'bawld_class']]\n",
    "input_data\n",
    "\n",
    "# input_data = input_data[['year', 'month', 'nee', 'tmmx', 'tmmn', 'pr']]\n",
    "# input_data.sort_values(by = 'year')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1011045-98b9-4449-9853-e10edb5f56b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['site_name', 'site_reference', 'latitude', 'longitude', 'flux_method',\n",
       "       'country', 'land_cover_eco', 'land_cover_plot', 'bawld_class', 'year',\n",
       "       'month', 'siteID', 'EVI', 'NDVI', 'SummaryQA', 'sur_refl_b01',\n",
       "       'sur_refl_b02', 'sur_refl_b03', 'sur_refl_b07', 'NDWI', 'aet', 'def',\n",
       "       'pdsi', 'pet', 'pr', 'ro', 'soil', 'srad', 'swe', 'tmmn', 'tmmx', 'vap',\n",
       "       'vpd', 'vs', 'lai', 'fpar', 'Percent_NonTree_Vegetation',\n",
       "       'Percent_NonVegetated', 'Percent_Tree_Cover', 'nee', 'gpp', 'reco',\n",
       "       'ch4_flux_total', 'Flux'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dbc9b3c-f0ab-484f-b2f6-0c5e571d9303",
   "metadata": {},
   "source": [
    "same thing but use thew 16day modis data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50cf0d95-6620-467b-925a-d68f7dbaa0b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial shape: (56731, 39)\n",
      "After replacing reflectance bands: (56731, 43)\n",
      "After soil merge: (56731, 53)\n",
      "After landcover merge: (56731, 54)\n",
      "After CO2 merge: (56731, 55)\n",
      "After ALT merge: (56731, 56)\n",
      "\n",
      "Successfully merged all data and saved to: /explore/nobackup/people/spotter5/anna_v/v2/v2_model_training_final_mod16.csv\n",
      "Final DataFrame head:\n",
      "                                           site_name  \\\n",
      "0                                     ARM-NSA-Barrow   \n",
      "1                                    ARM-NSA-Oliktok   \n",
      "2                      Abisko Stordalen birch forest   \n",
      "3                                        Adventdalen   \n",
      "4  Alberta - Western Peatland - LaBiche River,Bla...   \n",
      "\n",
      "                                      site_reference   latitude   longitude  \\\n",
      "0                        ARM-NSA-Barrow_US-A10_tower  71.323000 -156.609000   \n",
      "1                       ARM-NSA-Oliktok_US-A03_tower  70.495000 -149.886000   \n",
      "2                Abisko Stordalen birch forest_tower  68.347939   19.049769   \n",
      "3                           Adventdalen_SJ-Adv_tower  78.186000   15.923000   \n",
      "4  Alberta - Western Peatland - LaBiche River,Bla...  54.953840 -112.466980   \n",
      "\n",
      "  flux_method country  land_cover_eco  land_cover_plot    bawld_class  year  \\\n",
      "0          EC     USA           153.0            153.0     Wet Tundra  2000   \n",
      "1          EC     USA           153.0            153.0     Wet Tundra  2000   \n",
      "2          EC  Sweden            60.0             60.0  Boreal Forest  2000   \n",
      "3          EC  Norway           180.0            180.0     Wet Tundra  2000   \n",
      "4          EC  Canada           160.0            160.0            Fen  2000   \n",
      "\n",
      "   ...  clay_0_100cm nitrogen_0_100cm  ocd_0_100cm  phh2o_0_100cm  \\\n",
      "0  ...     22.227648        69.425669    65.170658       5.729838   \n",
      "1  ...     18.015117        78.211887    52.503460       6.471203   \n",
      "2  ...     13.238400        41.899736    23.136605       5.864295   \n",
      "3  ...     17.913802        66.421694    41.643476       6.580316   \n",
      "4  ...     19.723475        29.393525    13.246189       6.252252   \n",
      "\n",
      "   sand_0_100cm  silt_0_100cm  soc_0_100cm  land_cover  co2_cont  ALT  \n",
      "0     42.321892     35.450215   146.161786           4    376.89  0.1  \n",
      "1     47.787538     34.195582   120.345667           3    376.89  0.1  \n",
      "2     57.190494     29.567801    64.353214           1    376.89  NaN  \n",
      "3     45.149179     36.933076   110.023750           3    376.89  0.4  \n",
      "4     50.121325     30.162230    35.182187           4    376.89  NaN  \n",
      "\n",
      "[5 rows x 56 columns]\n",
      "\n",
      "Final DataFrame columns:\n",
      "Index(['site_name', 'site_reference', 'latitude', 'longitude', 'flux_method',\n",
      "       'country', 'land_cover_eco', 'land_cover_plot', 'bawld_class', 'year',\n",
      "       'month', 'siteID', 'NDVI', 'EVI', 'NDWI', 'aet', 'def', 'pdsi', 'pet',\n",
      "       'pr', 'ro', 'soil', 'srad', 'swe', 'tmmn', 'tmmx', 'vap', 'vpd', 'vs',\n",
      "       'lai', 'fpar', 'Percent_NonTree_Vegetation', 'Percent_NonVegetated',\n",
      "       'Percent_Tree_Cover', 'nee', 'gpp', 'reco', 'ch4_flux_total', 'Flux',\n",
      "       'sur_refl_b01', 'sur_refl_b02', 'sur_refl_b03', 'sur_refl_b07',\n",
      "       'bdod_0_100cm', 'cec_0_100cm', 'cfvo_0_100cm', 'clay_0_100cm',\n",
      "       'nitrogen_0_100cm', 'ocd_0_100cm', 'phh2o_0_100cm', 'sand_0_100cm',\n",
      "       'silt_0_100cm', 'soc_0_100cm', 'land_cover', 'co2_cont', 'ALT'],\n",
      "      dtype='object')\n",
      "\n",
      "Data type of 'land_cover' column: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# --- Load all data sources ---\n",
    "input_data = pd.read_csv(\"/explore/nobackup/people/spotter5/anna_v/v2/v2_model_training_data_16daymodis.csv\")\n",
    "input_data2 = pd.read_csv(\"/explore/nobackup/people/spotter5/anna_v/v2/v2_model_training_data_v3.csv\")\n",
    "soil = pd.read_csv(\"/explore/nobackup/people/spotter5/anna_v/v2/integrated_soil_data_1km_v2_sites.csv\")\n",
    "landcover = pd.read_csv(\"/explore/nobackup/people/spotter5/anna_v/v2/extracted_landcover_values.csv\")\n",
    "cont = pd.read_csv(\"/explore/nobackup/people/spotter5/anna_v/v2/co2_cont.csv\")\n",
    "alt = pd.read_csv(\"/explore/nobackup/people/spotter5/anna_v/v2/ALT_by_site.csv\")\n",
    "\n",
    "# --- Initial Data Cleaning ---\n",
    "input_data = input_data[input_data['flux_method'] == 'EC']\n",
    "input_data = input_data.dropna(subset=['site_reference'])\n",
    "soil = soil.dropna(subset=['site_refer'])\n",
    "landcover = landcover.dropna(subset=['site_refer'])\n",
    "\n",
    "input_data = input_data.drop_duplicates(subset=['site_reference', 'year', 'month'])\n",
    "soil = soil.drop_duplicates(subset=['site_refer'])\n",
    "landcover = landcover.drop_duplicates(subset=['site_refer'])\n",
    "\n",
    "print(f\"Initial shape: {input_data.shape}\")\n",
    "\n",
    "# --- START: EDITED SECTION ---\n",
    "# This section replaces the surface reflectance bands in input_data with those from input_data2\n",
    "\n",
    "# 1. Define the reflectance columns to be replaced and the keys for merging\n",
    "reflectance_cols = ['sur_refl_b01', 'sur_refl_b02', 'sur_refl_b03', 'sur_refl_b07']\n",
    "merge_keys = ['site_reference', 'year', 'month']\n",
    "\n",
    "# 2. Create a small DataFrame from input_data2 with only the keys and the desired reflectance columns\n",
    "reflectance_to_merge = input_data2[merge_keys + reflectance_cols].copy()\n",
    "reflectance_to_merge = reflectance_to_merge.drop_duplicates(subset=merge_keys)\n",
    "\n",
    "# 3. Drop the old reflectance columns from the main dataframe to avoid conflicts\n",
    "# input_data = input_data.drop(columns=reflectance_cols)\n",
    "\n",
    "# 4. Merge the new reflectance values into the main dataframe\n",
    "input_data = input_data.merge(reflectance_to_merge, on=merge_keys, how='left')\n",
    "\n",
    "print(f\"After replacing reflectance bands: {input_data.shape}\")\n",
    "# --- END: EDITED SECTION ---\n",
    "\n",
    "\n",
    "# --- Prepare and Merge Soil Data ---\n",
    "soil_filtered = soil.filter(regex='100cm$').copy()\n",
    "soil_filtered[\"site_reference\"] = soil[\"site_refer\"]\n",
    "input_data = input_data.merge(soil_filtered, on=\"site_reference\", how=\"left\")\n",
    "\n",
    "print(f\"After soil merge: {input_data.shape}\")\n",
    "\n",
    "# --- Prepare and Merge Land Cover Data ---\n",
    "landcover = landcover.rename(columns={'site_refer': 'site_reference'})\n",
    "landcover = landcover[['site_reference', 'land_cover']]\n",
    "input_data = input_data.merge(landcover, on=\"site_reference\", how=\"left\")\n",
    "\n",
    "print(f\"After landcover merge: {input_data.shape}\")\n",
    "\n",
    "# --- Prepare and Merge CO2 Data ---\n",
    "co2_to_merge = cont[['year', 'month', 'value']].copy()\n",
    "co2_to_merge = co2_to_merge.rename(columns={'value': 'co2_cont'})\n",
    "co2_to_merge = co2_to_merge.drop_duplicates(subset=['year', 'month'])\n",
    "input_data = input_data.merge(co2_to_merge, on=['year', 'month'], how='left')\n",
    "\n",
    "print(f\"After CO2 merge: {input_data.shape}\")\n",
    "\n",
    "# --- Prepare and Merge ALT Data ---\n",
    "alt_to_merge = alt[['site_reference', 'year', 'ALT']].copy()\n",
    "alt_to_merge = alt_to_merge.drop_duplicates(subset=['site_reference', 'year'])\n",
    "input_data = input_data.merge(alt_to_merge, on=['site_reference', 'year'], how='left')\n",
    "\n",
    "print(f\"After ALT merge: {input_data.shape}\")\n",
    "\n",
    "# --- Final Data Type Conversion for Land Cover ---\n",
    "# Fill any missing values (NaN) in 'land_cover' with -9999\n",
    "input_data['land_cover'] = input_data['land_cover'].fillna(-9999)\n",
    "\n",
    "# Convert the 'land_cover' column to integer type\n",
    "input_data['land_cover'] = input_data['land_cover'].astype(int)\n",
    "\n",
    "# --- Save Final Combined Data ---\n",
    "# Note: The output filename includes 'mod16', you may want to change this to reflect the new data source\n",
    "output_path_final = \"/explore/nobackup/people/spotter5/anna_v/v2/v2_model_training_final_mod16.csv\"\n",
    "input_data.to_csv(output_path_final, index=False)\n",
    "\n",
    "print(f\"\\nSuccessfully merged all data and saved to: {output_path_final}\")\n",
    "print(\"Final DataFrame head:\")\n",
    "print(input_data.head())\n",
    "print(\"\\nFinal DataFrame columns:\")\n",
    "print(input_data.columns)\n",
    "print(f\"\\nData type of 'land_cover' column: {input_data['land_cover'].dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a90397f-c7c7-402f-acaa-478f46c5472a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f7fc9b1-dd37-48b8-ac02-35c6c132032e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['site_name', 'site_reference', 'latitude', 'longitude', 'flux_method',\n",
       "       'country', 'land_cover_eco', 'land_cover_plot', 'bawld_class', 'year',\n",
       "       'month', 'siteID', 'EVI', 'NDVI', 'SummaryQA', 'sur_refl_b01',\n",
       "       'sur_refl_b02', 'sur_refl_b03', 'sur_refl_b07', 'NDWI', 'aet', 'def',\n",
       "       'pdsi', 'pet', 'pr', 'ro', 'soil', 'srad', 'swe', 'tmmn', 'tmmx', 'vap',\n",
       "       'vpd', 'vs', 'lai', 'fpar', 'Percent_NonTree_Vegetation',\n",
       "       'Percent_NonVegetated', 'Percent_Tree_Cover', 'nee', 'gpp', 'reco',\n",
       "       'ch4_flux_total', 'Flux'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060e0371-39cc-43cc-b6cb-e5bf733032de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-xgboost_gpu]",
   "language": "python",
   "name": "conda-env-.conda-xgboost_gpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
