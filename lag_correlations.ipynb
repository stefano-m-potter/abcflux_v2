{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "188e5b77-5ca9-4503-b1e0-bf8ccf6e9289",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/explore/nobackup/people/spotter5/temp_dir/ipykernel_3482424/2606193336.py:84: DtypeWarning: Columns (47) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(IN_CSV)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done. Results saved under: /explore/nobackup/people/spotter5/anna_v/v2/lag_correlation_scan\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\"\"\"\n",
    "NEE vs lagged predictors correlation scan (by calendar month), all sites.\n",
    "Includes lag 0 (same-month) through lag 12.\n",
    "\n",
    "Outputs (under OUT_DIR):\n",
    "  - corr_matrix_<var>.csv                 (target_month x lag_months [0..12])\n",
    "  - best_lag_per_month_all_variables.csv\n",
    "  - overall_best_lag_summary.csv\n",
    "  - corr_heatmap_<var>_annotated.png\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ----------------- Config -----------------\n",
    "IN_CSV  = \"/explore/nobackup/people/spotter5/anna_v/v2/v2_model_training_data_v4.csv\"\n",
    "OUT_DIR = \"/explore/nobackup/people/spotter5/anna_v/v2/lag_correlation_scan\"\n",
    "\n",
    "PREDICTORS = [\"tmean_C\", \"pr\", \"NDVI\", \"snow_cover\", \"snow_depth\"]\n",
    "MAX_LAG    = 12\n",
    "MIN_PAIRS  = 24   # min (x,y) pairs required for a Pearson r\n",
    "\n",
    "# ----------------- Helper functions -----------------\n",
    "def add_lags_for_site(g: pd.DataFrame, cols, max_lag=12) -> pd.DataFrame:\n",
    "    \"\"\"Add 1..max_lag month lags for selected columns within one site.\"\"\"\n",
    "    g = g.sort_values([\"year\", \"month\"]).copy()\n",
    "    g[\"date\"] = pd.to_datetime(dict(year=g[\"year\"], month=g[\"month\"], day=1))\n",
    "    g = g.sort_values(\"date\").reset_index(drop=True)\n",
    "    for col in cols:\n",
    "        if col in g.columns:\n",
    "            # lag 0 is just the same-month column; no need to add explicitly\n",
    "            for L in range(1, max_lag + 1):\n",
    "                g[f\"{col}_lag{L}\"] = g[col].shift(L)\n",
    "    return g\n",
    "\n",
    "def corr_with_min_pairs(x, y, min_pairs=MIN_PAIRS):\n",
    "    v = pd.DataFrame({\"x\": x, \"y\": y}).dropna()\n",
    "    return v[\"x\"].corr(v[\"y\"]) if len(v) >= min_pairs else np.nan\n",
    "\n",
    "def plot_corr_heatmap_with_values(corr_df: pd.DataFrame, var_name: str, out_file: Path):\n",
    "    \"\"\"\n",
    "    Draw a 12×13 heatmap (target months 1..12, lag 0..12) of NEE correlations with lagged `var_name`,\n",
    "    annotate r in each cell.\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "    im = ax.imshow(corr_df.values, cmap=\"bwr\", vmin=-1, vmax=1,\n",
    "                   origin=\"upper\", aspect=\"auto\")\n",
    "\n",
    "    cbar = fig.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
    "    cbar.set_label(f\"Pearson r (NEE vs {var_name} lag)\", fontsize=9)  # smaller legend text\n",
    "\n",
    "    ax.set_xticks(np.arange(len(corr_df.columns)))\n",
    "    ax.set_yticks(np.arange(len(corr_df.index)))\n",
    "    ax.set_xticklabels(corr_df.columns, fontsize=12)   # larger axis tick labels\n",
    "    ax.set_yticklabels(corr_df.index, fontsize=12)\n",
    "    ax.set_xlabel(\"Lag (months, 0 = same month)\", fontsize=12)\n",
    "    ax.set_ylabel(\"Target month\", fontsize=12)\n",
    "    ax.set_title(f\"NEE correlation vs. {var_name} lags\", fontsize=12, pad=16)\n",
    "\n",
    "    # annotate each cell with r (2 decimals)\n",
    "    for i in range(len(corr_df.index)):\n",
    "        for j in range(len(corr_df.columns)):\n",
    "            val = corr_df.iloc[i, j]\n",
    "            if pd.notna(val):\n",
    "                ax.text(j, i, f\"{val:.2f}\", ha=\"center\", va=\"center\",\n",
    "                        color=\"black\", fontsize=9)\n",
    "\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(out_file, dpi=200)\n",
    "    plt.close(fig)\n",
    "\n",
    "# ----------------- Main pipeline -----------------\n",
    "def main():\n",
    "    os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "    # ----- Load & prepare -----\n",
    "    df = pd.read_csv(IN_CSV)\n",
    "\n",
    "    # Normalize NEE column name and compute tmean_C if needed\n",
    "    if \"NEE\" not in df.columns and \"nee\" in df.columns:\n",
    "        df = df.rename(columns={\"nee\": \"NEE\"})\n",
    "    if \"tmean_C\" not in df.columns and {\"tmmn\", \"tmmx\"}.issubset(df.columns):\n",
    "        df[\"tmean_C\"] = df[[\"tmmn\", \"tmmx\"]].mean(axis=1)\n",
    "\n",
    "    # Filter and standardize types\n",
    "    df = df.dropna(subset=[\"site_reference\", \"year\", \"month\"])\n",
    "    df[\"year\"]  = pd.to_numeric(df[\"year\"], errors=\"coerce\").astype(int)\n",
    "    df[\"month\"] = pd.to_numeric(df[\"month\"], errors=\"coerce\").astype(int)\n",
    "    df = df[df[\"year\"] >= 2001].copy()\n",
    "\n",
    "    # Collapse to one row per site-year-month (mean if duplicates)\n",
    "    group_keys = [\"site_reference\", \"year\", \"month\"]\n",
    "    agg_map = {c: \"mean\" for c in set([\"NEE\"] + PREDICTORS) if c in df.columns}\n",
    "    dfm = (\n",
    "        df.groupby(group_keys, as_index=False)\n",
    "          .agg(agg_map)\n",
    "          .sort_values(group_keys)\n",
    "          .reset_index(drop=True)\n",
    "    )\n",
    "    dfm[\"target_month\"] = dfm[\"month\"]\n",
    "\n",
    "    # ----- Add lags within each site -----\n",
    "    df_lagged = (\n",
    "        dfm.groupby(\"site_reference\", group_keys=False)\n",
    "           .apply(lambda g: add_lags_for_site(g, PREDICTORS, MAX_LAG))\n",
    "           .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    # ----- Correlations (per target month × lag 0..12) -----\n",
    "    heatmaps = {\n",
    "        var: pd.DataFrame(index=range(1,13), columns=range(0,MAX_LAG+1), dtype=float)\n",
    "        for var in PREDICTORS\n",
    "    }\n",
    "\n",
    "    for var in PREDICTORS:\n",
    "        for m in range(1, 13):  # target month\n",
    "            sub = df_lagged[df_lagged[\"target_month\"] == m]\n",
    "            for L in range(0, MAX_LAG + 1):\n",
    "                if L == 0:\n",
    "                    series = sub.get(var)  # same-month predictor\n",
    "                else:\n",
    "                    series = sub.get(f\"{var}_lag{L}\")\n",
    "                r = corr_with_min_pairs(sub[\"NEE\"], series, MIN_PAIRS)\n",
    "                heatmaps[var].loc[m, L] = r\n",
    "\n",
    "    # ----- Save correlation matrices & summaries -----\n",
    "    best_rows = []\n",
    "    for var, mat in heatmaps.items():\n",
    "        mat.index.name = \"target_month\"\n",
    "        mat.columns.name = \"lag_months\"\n",
    "        mat.to_csv(Path(OUT_DIR) / f\"corr_matrix_{var}.csv\", float_format=\"%.4f\")\n",
    "\n",
    "        for m in mat.index:\n",
    "            row = mat.loc[m].dropna()\n",
    "            if row.empty:\n",
    "                best_rows.append({\"variable\": var, \"target_month\": m,\n",
    "                                  \"best_lag\": np.nan, \"corr\": np.nan})\n",
    "            else:\n",
    "                k = row.abs().idxmax()\n",
    "                best_rows.append({\"variable\": var, \"target_month\": m,\n",
    "                                  \"best_lag\": int(k), \"corr\": float(row[k])})\n",
    "\n",
    "    best_df = pd.DataFrame(best_rows)\n",
    "    best_df.to_csv(Path(OUT_DIR) / \"best_lag_per_month_all_variables.csv\",\n",
    "                   index=False, float_format=\"%.4f\")\n",
    "\n",
    "    summary = []\n",
    "    for var, mat in heatmaps.items():\n",
    "        mean_abs = mat.abs().mean(axis=0)\n",
    "        k = mean_abs.idxmax()\n",
    "        summary.append({\n",
    "            \"variable\": var,\n",
    "            \"overall_best_lag\": int(k),\n",
    "            \"overall_mean_abs_corr\": float(mean_abs[k])\n",
    "        })\n",
    "    pd.DataFrame(summary).to_csv(\n",
    "        Path(OUT_DIR) / \"overall_best_lag_summary.csv\",\n",
    "        index=False, float_format=\"%.4f\"\n",
    "    )\n",
    "\n",
    "    # ----- Plot annotated heatmaps -----\n",
    "    for var, mat in heatmaps.items():\n",
    "        plot_corr_heatmap_with_values(\n",
    "            mat,\n",
    "            var,\n",
    "            Path(OUT_DIR) / f\"corr_heatmap_{var}_annotated.png\"\n",
    "        )\n",
    "\n",
    "    print(\"Done. Results saved under:\", OUT_DIR)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb22e643-1e56-4501-8e69-def2ff986e81",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/explore/nobackup/people/spotter5/temp_dir/ipykernel_2243625/1083612340.py:104: DtypeWarning: Columns (47) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(IN_CSV)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Done target 'nee'. Results saved under: /explore/nobackup/people/spotter5/anna_v/v2/lag_correlation_scan/nee\n",
      "✓ Done target 'gpp'. Results saved under: /explore/nobackup/people/spotter5/anna_v/v2/lag_correlation_scan/gpp\n",
      "✓ Done target 'reco'. Results saved under: /explore/nobackup/people/spotter5/anna_v/v2/lag_correlation_scan/reco\n",
      "✓ Done target 'ch4_flux_total'. Results saved under: /explore/nobackup/people/spotter5/anna_v/v2/lag_correlation_scan/ch4_flux_total\n",
      "All requested targets processed.\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\"\"\"\n",
    "Lagged-predictor correlation scan (by calendar month), per target variable.\n",
    "Targets: 'nee', 'gpp', 'reco', 'ch4_flux_total'\n",
    "Lags: 0..12 months (0 = same-month)\n",
    "\n",
    "For each TARGET:\n",
    "  OUT_DIR/<target>/\n",
    "    - corr_matrix_<predictor>.csv                 (target_month x lag_months [0..12])\n",
    "    - best_lag_per_month_all_variables.csv\n",
    "    - overall_best_lag_summary.csv\n",
    "    - corr_heatmap_<predictor>_annotated.png\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ----------------- Config -----------------\n",
    "IN_CSV   = \"/explore/nobackup/people/spotter5/anna_v/v2/v2_model_training_data_v4.csv\"\n",
    "OUT_DIR  = \"/explore/nobackup/people/spotter5/anna_v/v2/lag_correlation_scan\"\n",
    "\n",
    "# predictors must exist in the data (case-insensitive); missing ones are skipped gracefully\n",
    "PREDICTORS = [\"tmean_C\", \"pr\", \"NDVI\", \"snow_cover\", \"snow_depth\"]\n",
    "TARGETS    = [\"nee\", \"gpp\", \"reco\", \"ch4_flux_total\"]\n",
    "\n",
    "MAX_LAG    = 12\n",
    "MIN_PAIRS  = 24   # min (x,y) pairs required for a Pearson r\n",
    "\n",
    "# ----------------- Helper functions -----------------\n",
    "def add_lags_for_site(g: pd.DataFrame, cols, max_lag=12) -> pd.DataFrame:\n",
    "    \"\"\"Add 1..max_lag month lags for selected columns within one site.\"\"\"\n",
    "    g = g.sort_values([\"year\", \"month\"]).copy()\n",
    "    g[\"date\"] = pd.to_datetime(dict(year=g[\"year\"], month=g[\"month\"], day=1))\n",
    "    g = g.sort_values(\"date\").reset_index(drop=True)\n",
    "    for col in cols:\n",
    "        if col in g.columns:\n",
    "            for L in range(1, max_lag + 1):\n",
    "                g[f\"{col}_lag{L}\"] = g[col].shift(L)\n",
    "    return g\n",
    "\n",
    "def corr_with_min_pairs(x, y, min_pairs=MIN_PAIRS):\n",
    "    v = pd.DataFrame({\"x\": x, \"y\": y}).dropna()\n",
    "    return v[\"x\"].corr(v[\"y\"]) if len(v) >= min_pairs else np.nan\n",
    "\n",
    "def plot_corr_heatmap_with_values(corr_df: pd.DataFrame, target_name: str, var_name: str, out_file: Path):\n",
    "    \"\"\"\n",
    "    Draw a 12×13 heatmap (target months 1..12, lag 0..12) of correlations:\n",
    "    target_name vs lagged var_name; annotate r in each cell.\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "    im = ax.imshow(corr_df.values, cmap=\"bwr\", vmin=-1, vmax=1,\n",
    "                   origin=\"upper\", aspect=\"auto\")\n",
    "\n",
    "    cbar = fig.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
    "    cbar.set_label(f\"Pearson r ({target_name} vs {var_name} lag)\", fontsize=11)\n",
    "\n",
    "    ax.set_xticks(np.arange(len(corr_df.columns)))\n",
    "    ax.set_yticks(np.arange(len(corr_df.index)))\n",
    "    ax.set_xticklabels(corr_df.columns, fontsize=12)\n",
    "    ax.set_yticklabels(corr_df.index, fontsize=12)\n",
    "    ax.set_xlabel(\"Lag (months, 0 = same month)\", fontsize=12)\n",
    "    ax.set_ylabel(\"Target month\", fontsize=12)\n",
    "    ax.set_title(f\"{target_name} correlation vs. {var_name} lags\", fontsize=12, pad=16)\n",
    "\n",
    "    # annotate each cell with r (2 decimals)\n",
    "    for i in range(len(corr_df.index)):\n",
    "        for j in range(len(corr_df.columns)):\n",
    "            val = corr_df.iloc[i, j]\n",
    "            if pd.notna(val):\n",
    "                ax.text(j, i, f\"{val:.2f}\", ha=\"center\", va=\"center\",\n",
    "                        color=\"black\", fontsize=9)\n",
    "\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(out_file, dpi=200)\n",
    "    plt.close(fig)\n",
    "\n",
    "def normalize_columns_case_insensitive(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Lower-case all column names for uniform access; keep original data.\"\"\"\n",
    "    df = df.copy()\n",
    "    df.columns = [c.lower() for c in df.columns]\n",
    "    return df\n",
    "\n",
    "def ensure_tmean(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Ensure 'tmean_c' exists.\n",
    "    If missing but 'tmmn' and 'tmmx' exist, create it as mean of tmmn/tmmx.\n",
    "    \"\"\"\n",
    "    if \"tmean_c\" not in df.columns:\n",
    "        if {\"tmmn\", \"tmmx\"}.issubset(df.columns):\n",
    "            df[\"tmean_c\"] = df[[\"tmmn\", \"tmmx\"]].mean(axis=1)\n",
    "    return df\n",
    "\n",
    "# ----------------- Main pipeline -----------------\n",
    "def main():\n",
    "    os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "    # ----- Load & prepare (once) -----\n",
    "    df = pd.read_csv(IN_CSV)\n",
    "    df = normalize_columns_case_insensitive(df)\n",
    "    df = ensure_tmean(df)\n",
    "\n",
    "    # Required indexing columns\n",
    "    required_idx = {\"site_reference\", \"year\", \"month\"}\n",
    "    if not required_idx.issubset(df.columns):\n",
    "        missing = sorted(list(required_idx - set(df.columns)))\n",
    "        raise ValueError(f\"Missing required columns: {missing}\")\n",
    "\n",
    "    # Filter and standardize types\n",
    "    df = df.dropna(subset=[\"site_reference\", \"year\", \"month\"])\n",
    "    df[\"year\"]  = pd.to_numeric(df[\"year\"], errors=\"coerce\").astype(\"Int64\")\n",
    "    df[\"month\"] = pd.to_numeric(df[\"month\"], errors=\"coerce\").astype(\"Int64\")\n",
    "    df = df[df[\"year\"] >= 2001].copy()\n",
    "\n",
    "    # Available predictors/targets (lowercase)\n",
    "    available_cols = set(df.columns)\n",
    "\n",
    "    # Compute once: collapse to one row per site-year-month using mean (handles accidental duplicates)\n",
    "    base_group_keys = [\"site_reference\", \"year\", \"month\"]\n",
    "\n",
    "    # We'll rebuild agg_map per target to include that target plus predictors that exist\n",
    "    for target in TARGETS:\n",
    "        target_lc = target.lower()\n",
    "        if target_lc not in available_cols:\n",
    "            print(f\"[WARN] Target '{target}' not found in data. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        # Per-target OUT_DIR\n",
    "        target_dir = Path(OUT_DIR) / target_lc\n",
    "        target_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # Use only predictors that exist\n",
    "        preds_present = []\n",
    "        for p in PREDICTORS:\n",
    "            p_lc = p.lower()\n",
    "            if p_lc in available_cols:\n",
    "                preds_present.append(p_lc)\n",
    "            else:\n",
    "                print(f\"[WARN] Predictor '{p}' not found. Skipping for target '{target}'.\")\n",
    "        if not preds_present:\n",
    "            print(f\"[WARN] No predictors available for target '{target}'. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        # Build aggregation map for groupby\n",
    "        agg_map = {c: \"mean\" for c in [target_lc] + preds_present if c in available_cols}\n",
    "\n",
    "        dfm = (\n",
    "            df.groupby(base_group_keys, as_index=False)\n",
    "              .agg(agg_map)\n",
    "              .sort_values(base_group_keys)\n",
    "              .reset_index(drop=True)\n",
    "        )\n",
    "        dfm[\"target_month\"] = dfm[\"month\"]\n",
    "\n",
    "        # Add lags within each site for the predictors only\n",
    "        df_lagged = (\n",
    "            dfm.groupby(\"site_reference\", group_keys=False)\n",
    "               .apply(lambda g: add_lags_for_site(g, preds_present, MAX_LAG))\n",
    "               .reset_index(drop=True)\n",
    "        )\n",
    "\n",
    "        # Pre-allocate heatmaps (per predictor)\n",
    "        heatmaps = {\n",
    "            var: pd.DataFrame(index=range(1,13), columns=range(0,MAX_LAG+1), dtype=float)\n",
    "            for var in preds_present\n",
    "        }\n",
    "\n",
    "        # Compute correlations: per target month x lag\n",
    "        for var in preds_present:\n",
    "            for m in range(1, 13):  # target month\n",
    "                sub = df_lagged[df_lagged[\"target_month\"] == m]\n",
    "                for L in range(0, MAX_LAG + 1):\n",
    "                    if L == 0:\n",
    "                        series = sub.get(var)  # same-month predictor\n",
    "                    else:\n",
    "                        series = sub.get(f\"{var}_lag{L}\")\n",
    "                    r = corr_with_min_pairs(sub[target_lc], series, MIN_PAIRS)\n",
    "                    heatmaps[var].loc[m, L] = r\n",
    "\n",
    "        # ----- Save correlation matrices & summaries -----\n",
    "        best_rows = []\n",
    "        for var, mat in heatmaps.items():\n",
    "            mat.index.name = \"target_month\"\n",
    "            mat.columns.name = \"lag_months\"\n",
    "            mat.to_csv(target_dir / f\"corr_matrix_{var}.csv\", float_format=\"%.4f\")\n",
    "\n",
    "            for m in mat.index:\n",
    "                row = mat.loc[m].dropna()\n",
    "                if row.empty:\n",
    "                    best_rows.append({\"variable\": var, \"target_month\": m,\n",
    "                                      \"best_lag\": np.nan, \"corr\": np.nan})\n",
    "                else:\n",
    "                    k = row.abs().idxmax()\n",
    "                    best_rows.append({\"variable\": var, \"target_month\": m,\n",
    "                                      \"best_lag\": int(k), \"corr\": float(row[k])})\n",
    "\n",
    "        best_df = pd.DataFrame(best_rows)\n",
    "        best_df.to_csv(target_dir / \"best_lag_per_month_all_variables.csv\",\n",
    "                       index=False, float_format=\"%.4f\")\n",
    "\n",
    "        summary = []\n",
    "        for var, mat in heatmaps.items():\n",
    "            mean_abs = mat.abs().mean(axis=0)\n",
    "            k = mean_abs.idxmax()\n",
    "            summary.append({\n",
    "                \"variable\": var,\n",
    "                \"overall_best_lag\": int(k),\n",
    "                \"overall_mean_abs_corr\": float(mean_abs[k])\n",
    "            })\n",
    "        pd.DataFrame(summary).to_csv(\n",
    "            target_dir / \"overall_best_lag_summary.csv\",\n",
    "            index=False, float_format=\"%.4f\"\n",
    "        )\n",
    "\n",
    "        # ----- Plot annotated heatmaps -----\n",
    "        for var, mat in heatmaps.items():\n",
    "            plot_corr_heatmap_with_values(\n",
    "                mat,\n",
    "                target_name=target_lc,\n",
    "                var_name=var,\n",
    "                out_file=target_dir / f\"corr_heatmap_{var}_annotated.png\"\n",
    "            )\n",
    "\n",
    "        print(f\"✓ Done target '{target}'. Results saved under: {target_dir}\")\n",
    "\n",
    "    print(\"All requested targets processed.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0efa3e3-b587-4185-9a46-3fdcd0984e3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Done target 'nee'. Results saved under: /explore/nobackup/people/spotter5/anna_v/v2/lag_correlation_scan_multiple/nee\n",
      "✓ Done target 'gpp'. Results saved under: /explore/nobackup/people/spotter5/anna_v/v2/lag_correlation_scan_multiple/gpp\n",
      "✓ Done target 'reco'. Results saved under: /explore/nobackup/people/spotter5/anna_v/v2/lag_correlation_scan_multiple/reco\n",
      "✓ Done target 'ch4_flux_total'. Results saved under: /explore/nobackup/people/spotter5/anna_v/v2/lag_correlation_scan_multiple/ch4_flux_total\n",
      "All requested targets processed.\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\"\"\"\n",
    "Lagged-predictor multiple regression scan (by calendar month), per target variable.\n",
    "Targets: 'nee', 'gpp', 'reco', 'ch4_flux_total'\n",
    "Lags: 0..12 months (0 = same-month)\n",
    "\n",
    "For each TARGET:\n",
    "  OUT_DIR/<target>/\n",
    "    - coef_matrix_<predictor>.csv                 (target_month x lag_months [0..12])\n",
    "    - pval_matrix_<predictor>.csv                 (target_month x lag_months [0..12])\n",
    "    - best_lag_per_month_all_variables.csv\n",
    "    - overall_best_lag_summary.csv\n",
    "    - coef_heatmap_<predictor>_annotated.png\n",
    "\n",
    "Notes:\n",
    "- Within each (target_month m, lag L) slice, we regress z(target) on z(predictor_lagL) with\n",
    "  categorical controls for the predictor's month-of-year (pred_month), i.e., C(pred_month).\n",
    "- z(·) denotes standardization within the slice to make coefficients comparable across predictors.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "\n",
    "# statsmodels for OLS with categorical controls\n",
    "import statsmodels.api as sm\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ----------------- Config -----------------\n",
    "IN_CSV   = \"/explore/nobackup/people/spotter5/anna_v/v2/v2_model_training_data_v4.csv\"\n",
    "OUT_DIR  = \"/explore/nobackup/people/spotter5/anna_v/v2/lag_correlation_scan_multiple\"\n",
    "\n",
    "# predictors must exist in the data (case-insensitive); missing ones are skipped gracefully\n",
    "PREDICTORS = [\"tmean_C\", \"pr\", \"NDVI\", \"snow_cover\", \"snow_depth\"]\n",
    "TARGETS    = [\"nee\", \"gpp\", \"reco\", \"ch4_flux_total\"]\n",
    "\n",
    "MAX_LAG    = 12\n",
    "MIN_PAIRS  = 24   # min rows required for a regression fit\n",
    "\n",
    "# ----------------- Helper functions -----------------\n",
    "def add_lags_for_site(g: pd.DataFrame, cols, max_lag=12) -> pd.DataFrame:\n",
    "    \"\"\"Add 1..max_lag month lags for selected columns within one site.\"\"\"\n",
    "    g = g.sort_values([\"year\", \"month\"]).copy()\n",
    "    g[\"date\"] = pd.to_datetime(dict(year=g[\"year\"], month=g[\"month\"], day=1))\n",
    "    g = g.sort_values(\"date\").reset_index(drop=True)\n",
    "    for col in cols:\n",
    "        if col in g.columns:\n",
    "            for L in range(1, max_lag + 1):\n",
    "                g[f\"{col}_lag{L}\"] = g[col].shift(L)\n",
    "    return g\n",
    "\n",
    "def month_minus_lag(month_series: pd.Series, L: int) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Given 1..12 months and a lag L, compute the month-of-year of the lagged value:\n",
    "    pred_month = ((month - L - 1) % 12) + 1\n",
    "    \"\"\"\n",
    "    return ((month_series.astype(int) - L - 1) % 12) + 1\n",
    "\n",
    "def standardize(s: pd.Series) -> pd.Series:\n",
    "    mu = s.mean()\n",
    "    sd = s.std(ddof=0)\n",
    "    if sd == 0 or np.isnan(sd):\n",
    "        return pd.Series(np.nan, index=s.index)\n",
    "    return (s - mu) / sd\n",
    "\n",
    "def run_partial_regression(y: pd.Series, x: pd.Series, pred_month: pd.Series, min_rows: int = MIN_PAIRS):\n",
    "    \"\"\"\n",
    "    Fit OLS: z(y) ~ z(x) + C(pred_month). Return (coef, pval) for z(x).\n",
    "    If not enough data or degenerate, return (np.nan, np.nan).\n",
    "    \"\"\"\n",
    "    data = pd.DataFrame({\"y\": y, \"x\": x, \"pred_month\": pred_month}).dropna()\n",
    "    if len(data) < min_rows:\n",
    "        return np.nan, np.nan\n",
    "\n",
    "    # Standardize within slice so the coefficient is on a comparable scale\n",
    "    data[\"y_z\"] = standardize(data[\"y\"])\n",
    "    data[\"x_z\"] = standardize(data[\"x\"])\n",
    "\n",
    "    data = data.dropna(subset=[\"y_z\", \"x_z\"])\n",
    "    if len(data) < min_rows:\n",
    "        return np.nan, np.nan\n",
    "\n",
    "    # Categorical controls for predictor's month-of-year\n",
    "    dummies = pd.get_dummies(data[\"pred_month\"].astype(int), prefix=\"pm\", drop_first=True)\n",
    "    X = pd.concat([data[\"x_z\"], dummies], axis=1)\n",
    "    X = sm.add_constant(X, has_constant=\"add\")\n",
    "    model = sm.OLS(data[\"y_z\"], X, missing=\"drop\")\n",
    "\n",
    "    try:\n",
    "        res = model.fit()\n",
    "    except Exception:\n",
    "        return np.nan, np.nan\n",
    "\n",
    "    if \"x_z\" not in res.params:\n",
    "        return np.nan, np.nan\n",
    "\n",
    "    return float(res.params[\"x_z\"]), float(res.pvalues.get(\"x_z\", np.nan))\n",
    "\n",
    "def plot_coef_heatmap_with_values(df_coef: pd.DataFrame, target_name: str, var_name: str, out_file: Path):\n",
    "    \"\"\"\n",
    "    Draw a 12×13 heatmap (target months 1..12, lag 0..12) of standardized regression coefficients:\n",
    "    coef on z(var_lagL) controlling for predictor's month-of-year.\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "    im = ax.imshow(df_coef.values, cmap=\"bwr\", vmin=-1, vmax=1,\n",
    "                   origin=\"upper\", aspect=\"auto\")\n",
    "\n",
    "    cbar = fig.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
    "    cbar.set_label(f\"Std. coef: {var_name} (lag) → {target_name}\", fontsize=11)\n",
    "\n",
    "    ax.set_xticks(np.arange(len(df_coef.columns)))\n",
    "    ax.set_yticks(np.arange(len(df_coef.index)))\n",
    "    ax.set_xticklabels(df_coef.columns, fontsize=12)\n",
    "    ax.set_yticklabels(df_coef.index, fontsize=12)\n",
    "    ax.set_xlabel(\"Lag (months, 0 = same month)\", fontsize=12)\n",
    "    ax.set_ylabel(\"Target month\", fontsize=12)\n",
    "    ax.set_title(f\"{target_name}: partial effect of lagged {var_name}\\n\"\n",
    "                 f\"(controls: predictor month-of-year)\", fontsize=12, pad=16)\n",
    "\n",
    "    # annotate each cell with coef (2 decimals)\n",
    "    for i in range(len(df_coef.index)):\n",
    "        for j in range(len(df_coef.columns)):\n",
    "            val = df_coef.iloc[i, j]\n",
    "            if pd.notna(val):\n",
    "                ax.text(j, i, f\"{val:.2f}\", ha=\"center\", va=\"center\",\n",
    "                        color=\"black\", fontsize=9)\n",
    "\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(out_file, dpi=200)\n",
    "    plt.close(fig)\n",
    "\n",
    "def normalize_columns_case_insensitive(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df.columns = [c.lower() for c in df.columns]\n",
    "    return df\n",
    "\n",
    "def ensure_tmean(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Ensure 'tmean_c' exists; if 'tmmn' & 'tmmx' exist, create it as their mean.\"\"\"\n",
    "    if \"tmean_c\" not in df.columns and {\"tmmn\", \"tmmx\"}.issubset(df.columns):\n",
    "        df[\"tmean_c\"] = df[[\"tmmn\", \"tmmx\"]].mean(axis=1)\n",
    "    return df\n",
    "\n",
    "# ----------------- Main pipeline -----------------\n",
    "def main():\n",
    "    os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "    # ----- Load & prepare (once) -----\n",
    "    df = pd.read_csv(IN_CSV)\n",
    "    df = normalize_columns_case_insensitive(df)\n",
    "    df = ensure_tmean(df)\n",
    "\n",
    "    # Required indexing columns\n",
    "    required_idx = {\"site_reference\", \"year\", \"month\"}\n",
    "    if not required_idx.issubset(df.columns):\n",
    "        missing = sorted(list(required_idx - set(df.columns)))\n",
    "        raise ValueError(f\"Missing required columns: {missing}\")\n",
    "\n",
    "    # Filter and standardize types\n",
    "    df = df.dropna(subset=[\"site_reference\", \"year\", \"month\"])\n",
    "    df[\"year\"]  = pd.to_numeric(df[\"year\"], errors=\"coerce\").astype(\"Int64\")\n",
    "    df[\"month\"] = pd.to_numeric(df[\"month\"], errors=\"coerce\").astype(\"Int64\")\n",
    "    df = df[df[\"year\"] >= 2001].copy()\n",
    "\n",
    "    available_cols = set(df.columns)\n",
    "\n",
    "    base_group_keys = [\"site_reference\", \"year\", \"month\"]\n",
    "\n",
    "    for target in TARGETS:\n",
    "        target_lc = target.lower()\n",
    "        if target_lc not in available_cols:\n",
    "            print(f\"[WARN] Target '{target}' not found in data. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        target_dir = Path(OUT_DIR) / target_lc\n",
    "        target_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        preds_present = []\n",
    "        for p in PREDICTORS:\n",
    "            p_lc = p.lower()\n",
    "            if p_lc in available_cols:\n",
    "                preds_present.append(p_lc)\n",
    "            else:\n",
    "                print(f\"[WARN] Predictor '{p}' not found. Skipping for target '{target}'.\")\n",
    "        if not preds_present:\n",
    "            print(f\"[WARN] No predictors available for target '{target}'. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        # Aggregate to one row per site-year-month\n",
    "        agg_map = {c: \"mean\" for c in [target_lc] + preds_present if c in available_cols}\n",
    "        dfm = (\n",
    "            df.groupby(base_group_keys, as_index=False)\n",
    "              .agg(agg_map)\n",
    "              .sort_values(base_group_keys)\n",
    "              .reset_index(drop=True)\n",
    "        )\n",
    "        dfm[\"target_month\"] = dfm[\"month\"]\n",
    "\n",
    "        # Add lags within each site for the predictors only\n",
    "        df_lagged = (\n",
    "            dfm.groupby(\"site_reference\", group_keys=False)\n",
    "               .apply(lambda g: add_lags_for_site(g, preds_present, MAX_LAG))\n",
    "               .reset_index(drop=True)\n",
    "        )\n",
    "\n",
    "        # Pre-allocate matrices\n",
    "        coef_mats = {var: pd.DataFrame(index=range(1,13), columns=range(0,MAX_LAG+1), dtype=float)\n",
    "                     for var in preds_present}\n",
    "        pval_mats = {var: pd.DataFrame(index=range(1,13), columns=range(0,MAX_LAG+1), dtype=float)\n",
    "                     for var in preds_present}\n",
    "\n",
    "        # Compute partial regression coef per target month x lag\n",
    "        for var in preds_present:\n",
    "            for m in range(1, 13):  # target month\n",
    "                sub = df_lagged[df_lagged[\"target_month\"] == m].copy()\n",
    "\n",
    "                for L in range(0, MAX_LAG + 1):\n",
    "                    if L == 0:\n",
    "                        x_series = sub.get(var)\n",
    "                        pred_mo = sub[\"target_month\"]  # predictor month == target month when L=0\n",
    "                    else:\n",
    "                        x_series = sub.get(f\"{var}_lag{L}\")\n",
    "                        pred_mo = month_minus_lag(sub[\"target_month\"], L)\n",
    "\n",
    "                    coef, pval = run_partial_regression(\n",
    "                        y=sub[target_lc],\n",
    "                        x=x_series,\n",
    "                        pred_month=pred_mo,\n",
    "                        min_rows=MIN_PAIRS\n",
    "                    )\n",
    "                    coef_mats[var].loc[m, L] = coef\n",
    "                    pval_mats[var].loc[m, L] = pval\n",
    "\n",
    "        # ----- Save matrices & summaries -----\n",
    "        best_rows = []\n",
    "        for var in preds_present:\n",
    "            cm = coef_mats[var]\n",
    "            pm = pval_mats[var]\n",
    "\n",
    "            cm.index.name = \"target_month\"\n",
    "            cm.columns.name = \"lag_months\"\n",
    "            pm.index.name = \"target_month\"\n",
    "            pm.columns.name = \"lag_months\"\n",
    "\n",
    "            cm.to_csv(target_dir / f\"coef_matrix_{var}.csv\", float_format=\"%.4f\")\n",
    "            pm.to_csv(target_dir / f\"pval_matrix_{var}.csv\", float_format=\"%.4g\")\n",
    "\n",
    "            # Best (by |coef|) per target month\n",
    "            for m in cm.index:\n",
    "                row = cm.loc[m].dropna()\n",
    "                if row.empty:\n",
    "                    best_rows.append({\"variable\": var, \"target_month\": m,\n",
    "                                      \"best_lag\": np.nan, \"std_coef\": np.nan, \"pval\": np.nan})\n",
    "                else:\n",
    "                    k = row.abs().idxmax()\n",
    "                    best_rows.append({\"variable\": var, \"target_month\": m,\n",
    "                                      \"best_lag\": int(k),\n",
    "                                      \"std_coef\": float(row[k]),\n",
    "                                      \"pval\": float(pval_mats[var].loc[m, k])})\n",
    "\n",
    "        best_df = pd.DataFrame(best_rows)\n",
    "        best_df.to_csv(target_dir / \"best_lag_per_month_all_variables.csv\",\n",
    "                       index=False, float_format=\"%.4f\")\n",
    "\n",
    "        # Overall summary: which lag maximizes mean |coef| across months\n",
    "        summary = []\n",
    "        for var in preds_present:\n",
    "            cm = coef_mats[var]\n",
    "            mean_abs = cm.abs().mean(axis=0)\n",
    "            k = mean_abs.idxmax()\n",
    "            summary.append({\n",
    "                \"variable\": var,\n",
    "                \"overall_best_lag\": int(k),\n",
    "                \"overall_mean_abs_std_coef\": float(mean_abs[k])\n",
    "            })\n",
    "        pd.DataFrame(summary).to_csv(\n",
    "            target_dir / \"overall_best_lag_summary.csv\",\n",
    "            index=False, float_format=\"%.4f\"\n",
    "        )\n",
    "\n",
    "        # ----- Plot annotated heatmaps of standardized coefficients -----\n",
    "        for var in preds_present:\n",
    "            plot_coef_heatmap_with_values(\n",
    "                coef_mats[var],\n",
    "                target_name=target_lc,\n",
    "                var_name=var,\n",
    "                out_file=target_dir / f\"coef_heatmap_{var}_annotated.png\"\n",
    "            )\n",
    "\n",
    "        print(f\"✓ Done target '{target}'. Results saved under: {target_dir}\")\n",
    "\n",
    "    print(\"All requested targets processed.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431b3fa7-d6b6-4ede-98a5-bc5fe9ac6185",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-xgboost_gpu]",
   "language": "python",
   "name": "conda-env-.conda-xgboost_gpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
