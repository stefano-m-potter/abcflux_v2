{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "188e5b77-5ca9-4503-b1e0-bf8ccf6e9289",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/explore/nobackup/people/spotter5/temp_dir/ipykernel_3482424/2606193336.py:84: DtypeWarning: Columns (47) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(IN_CSV)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done. Results saved under: /explore/nobackup/people/spotter5/anna_v/v2/lag_correlation_scan\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\"\"\"\n",
    "NEE vs lagged predictors correlation scan (by calendar month), all sites.\n",
    "Includes lag 0 (same-month) through lag 12.\n",
    "\n",
    "Outputs (under OUT_DIR):\n",
    "  - corr_matrix_<var>.csv                 (target_month x lag_months [0..12])\n",
    "  - best_lag_per_month_all_variables.csv\n",
    "  - overall_best_lag_summary.csv\n",
    "  - corr_heatmap_<var>_annotated.png\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ----------------- Config -----------------\n",
    "IN_CSV  = \"/explore/nobackup/people/spotter5/anna_v/v2/v2_model_training_data_v4.csv\"\n",
    "OUT_DIR = \"/explore/nobackup/people/spotter5/anna_v/v2/lag_correlation_scan\"\n",
    "\n",
    "PREDICTORS = [\"tmean_C\", \"pr\", \"NDVI\", \"snow_cover\", \"snow_depth\"]\n",
    "MAX_LAG    = 12\n",
    "MIN_PAIRS  = 24   # min (x,y) pairs required for a Pearson r\n",
    "\n",
    "# ----------------- Helper functions -----------------\n",
    "def add_lags_for_site(g: pd.DataFrame, cols, max_lag=12) -> pd.DataFrame:\n",
    "    \"\"\"Add 1..max_lag month lags for selected columns within one site.\"\"\"\n",
    "    g = g.sort_values([\"year\", \"month\"]).copy()\n",
    "    g[\"date\"] = pd.to_datetime(dict(year=g[\"year\"], month=g[\"month\"], day=1))\n",
    "    g = g.sort_values(\"date\").reset_index(drop=True)\n",
    "    for col in cols:\n",
    "        if col in g.columns:\n",
    "            # lag 0 is just the same-month column; no need to add explicitly\n",
    "            for L in range(1, max_lag + 1):\n",
    "                g[f\"{col}_lag{L}\"] = g[col].shift(L)\n",
    "    return g\n",
    "\n",
    "def corr_with_min_pairs(x, y, min_pairs=MIN_PAIRS):\n",
    "    v = pd.DataFrame({\"x\": x, \"y\": y}).dropna()\n",
    "    return v[\"x\"].corr(v[\"y\"]) if len(v) >= min_pairs else np.nan\n",
    "\n",
    "def plot_corr_heatmap_with_values(corr_df: pd.DataFrame, var_name: str, out_file: Path):\n",
    "    \"\"\"\n",
    "    Draw a 12×13 heatmap (target months 1..12, lag 0..12) of NEE correlations with lagged `var_name`,\n",
    "    annotate r in each cell.\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "    im = ax.imshow(corr_df.values, cmap=\"bwr\", vmin=-1, vmax=1,\n",
    "                   origin=\"upper\", aspect=\"auto\")\n",
    "\n",
    "    cbar = fig.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
    "    cbar.set_label(f\"Pearson r (NEE vs {var_name} lag)\", fontsize=9)  # smaller legend text\n",
    "\n",
    "    ax.set_xticks(np.arange(len(corr_df.columns)))\n",
    "    ax.set_yticks(np.arange(len(corr_df.index)))\n",
    "    ax.set_xticklabels(corr_df.columns, fontsize=12)   # larger axis tick labels\n",
    "    ax.set_yticklabels(corr_df.index, fontsize=12)\n",
    "    ax.set_xlabel(\"Lag (months, 0 = same month)\", fontsize=12)\n",
    "    ax.set_ylabel(\"Target month\", fontsize=12)\n",
    "    ax.set_title(f\"NEE correlation vs. {var_name} lags\", fontsize=12, pad=16)\n",
    "\n",
    "    # annotate each cell with r (2 decimals)\n",
    "    for i in range(len(corr_df.index)):\n",
    "        for j in range(len(corr_df.columns)):\n",
    "            val = corr_df.iloc[i, j]\n",
    "            if pd.notna(val):\n",
    "                ax.text(j, i, f\"{val:.2f}\", ha=\"center\", va=\"center\",\n",
    "                        color=\"black\", fontsize=9)\n",
    "\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(out_file, dpi=200)\n",
    "    plt.close(fig)\n",
    "\n",
    "# ----------------- Main pipeline -----------------\n",
    "def main():\n",
    "    os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "    # ----- Load & prepare -----\n",
    "    df = pd.read_csv(IN_CSV)\n",
    "\n",
    "    # Normalize NEE column name and compute tmean_C if needed\n",
    "    if \"NEE\" not in df.columns and \"nee\" in df.columns:\n",
    "        df = df.rename(columns={\"nee\": \"NEE\"})\n",
    "    if \"tmean_C\" not in df.columns and {\"tmmn\", \"tmmx\"}.issubset(df.columns):\n",
    "        df[\"tmean_C\"] = df[[\"tmmn\", \"tmmx\"]].mean(axis=1)\n",
    "\n",
    "    # Filter and standardize types\n",
    "    df = df.dropna(subset=[\"site_reference\", \"year\", \"month\"])\n",
    "    df[\"year\"]  = pd.to_numeric(df[\"year\"], errors=\"coerce\").astype(int)\n",
    "    df[\"month\"] = pd.to_numeric(df[\"month\"], errors=\"coerce\").astype(int)\n",
    "    df = df[df[\"year\"] >= 2001].copy()\n",
    "\n",
    "    # Collapse to one row per site-year-month (mean if duplicates)\n",
    "    group_keys = [\"site_reference\", \"year\", \"month\"]\n",
    "    agg_map = {c: \"mean\" for c in set([\"NEE\"] + PREDICTORS) if c in df.columns}\n",
    "    dfm = (\n",
    "        df.groupby(group_keys, as_index=False)\n",
    "          .agg(agg_map)\n",
    "          .sort_values(group_keys)\n",
    "          .reset_index(drop=True)\n",
    "    )\n",
    "    dfm[\"target_month\"] = dfm[\"month\"]\n",
    "\n",
    "    # ----- Add lags within each site -----\n",
    "    df_lagged = (\n",
    "        dfm.groupby(\"site_reference\", group_keys=False)\n",
    "           .apply(lambda g: add_lags_for_site(g, PREDICTORS, MAX_LAG))\n",
    "           .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    # ----- Correlations (per target month × lag 0..12) -----\n",
    "    heatmaps = {\n",
    "        var: pd.DataFrame(index=range(1,13), columns=range(0,MAX_LAG+1), dtype=float)\n",
    "        for var in PREDICTORS\n",
    "    }\n",
    "\n",
    "    for var in PREDICTORS:\n",
    "        for m in range(1, 13):  # target month\n",
    "            sub = df_lagged[df_lagged[\"target_month\"] == m]\n",
    "            for L in range(0, MAX_LAG + 1):\n",
    "                if L == 0:\n",
    "                    series = sub.get(var)  # same-month predictor\n",
    "                else:\n",
    "                    series = sub.get(f\"{var}_lag{L}\")\n",
    "                r = corr_with_min_pairs(sub[\"NEE\"], series, MIN_PAIRS)\n",
    "                heatmaps[var].loc[m, L] = r\n",
    "\n",
    "    # ----- Save correlation matrices & summaries -----\n",
    "    best_rows = []\n",
    "    for var, mat in heatmaps.items():\n",
    "        mat.index.name = \"target_month\"\n",
    "        mat.columns.name = \"lag_months\"\n",
    "        mat.to_csv(Path(OUT_DIR) / f\"corr_matrix_{var}.csv\", float_format=\"%.4f\")\n",
    "\n",
    "        for m in mat.index:\n",
    "            row = mat.loc[m].dropna()\n",
    "            if row.empty:\n",
    "                best_rows.append({\"variable\": var, \"target_month\": m,\n",
    "                                  \"best_lag\": np.nan, \"corr\": np.nan})\n",
    "            else:\n",
    "                k = row.abs().idxmax()\n",
    "                best_rows.append({\"variable\": var, \"target_month\": m,\n",
    "                                  \"best_lag\": int(k), \"corr\": float(row[k])})\n",
    "\n",
    "    best_df = pd.DataFrame(best_rows)\n",
    "    best_df.to_csv(Path(OUT_DIR) / \"best_lag_per_month_all_variables.csv\",\n",
    "                   index=False, float_format=\"%.4f\")\n",
    "\n",
    "    summary = []\n",
    "    for var, mat in heatmaps.items():\n",
    "        mean_abs = mat.abs().mean(axis=0)\n",
    "        k = mean_abs.idxmax()\n",
    "        summary.append({\n",
    "            \"variable\": var,\n",
    "            \"overall_best_lag\": int(k),\n",
    "            \"overall_mean_abs_corr\": float(mean_abs[k])\n",
    "        })\n",
    "    pd.DataFrame(summary).to_csv(\n",
    "        Path(OUT_DIR) / \"overall_best_lag_summary.csv\",\n",
    "        index=False, float_format=\"%.4f\"\n",
    "    )\n",
    "\n",
    "    # ----- Plot annotated heatmaps -----\n",
    "    for var, mat in heatmaps.items():\n",
    "        plot_corr_heatmap_with_values(\n",
    "            mat,\n",
    "            var,\n",
    "            Path(OUT_DIR) / f\"corr_heatmap_{var}_annotated.png\"\n",
    "        )\n",
    "\n",
    "    print(\"Done. Results saved under:\", OUT_DIR)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb22e643-1e56-4501-8e69-def2ff986e81",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/explore/nobackup/people/spotter5/temp_dir/ipykernel_2243625/1083612340.py:104: DtypeWarning: Columns (47) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(IN_CSV)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Done target 'nee'. Results saved under: /explore/nobackup/people/spotter5/anna_v/v2/lag_correlation_scan/nee\n",
      "✓ Done target 'gpp'. Results saved under: /explore/nobackup/people/spotter5/anna_v/v2/lag_correlation_scan/gpp\n",
      "✓ Done target 'reco'. Results saved under: /explore/nobackup/people/spotter5/anna_v/v2/lag_correlation_scan/reco\n",
      "✓ Done target 'ch4_flux_total'. Results saved under: /explore/nobackup/people/spotter5/anna_v/v2/lag_correlation_scan/ch4_flux_total\n",
      "All requested targets processed.\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\"\"\"\n",
    "Lagged-predictor correlation scan (by calendar month), per target variable.\n",
    "Targets: 'nee', 'gpp', 'reco', 'ch4_flux_total'\n",
    "Lags: 0..12 months (0 = same-month)\n",
    "\n",
    "For each TARGET:\n",
    "  OUT_DIR/<target>/\n",
    "    - corr_matrix_<predictor>.csv                 (target_month x lag_months [0..12])\n",
    "    - best_lag_per_month_all_variables.csv\n",
    "    - overall_best_lag_summary.csv\n",
    "    - corr_heatmap_<predictor>_annotated.png\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ----------------- Config -----------------\n",
    "IN_CSV   = \"/explore/nobackup/people/spotter5/anna_v/v2/v2_model_training_data_v4.csv\"\n",
    "OUT_DIR  = \"/explore/nobackup/people/spotter5/anna_v/v2/lag_correlation_scan\"\n",
    "\n",
    "# predictors must exist in the data (case-insensitive); missing ones are skipped gracefully\n",
    "PREDICTORS = [\"tmean_C\", \"pr\", \"NDVI\", \"snow_cover\", \"snow_depth\"]\n",
    "TARGETS    = [\"nee\", \"gpp\", \"reco\", \"ch4_flux_total\"]\n",
    "\n",
    "MAX_LAG    = 12\n",
    "MIN_PAIRS  = 24   # min (x,y) pairs required for a Pearson r\n",
    "\n",
    "# ----------------- Helper functions -----------------\n",
    "def add_lags_for_site(g: pd.DataFrame, cols, max_lag=12) -> pd.DataFrame:\n",
    "    \"\"\"Add 1..max_lag month lags for selected columns within one site.\"\"\"\n",
    "    g = g.sort_values([\"year\", \"month\"]).copy()\n",
    "    g[\"date\"] = pd.to_datetime(dict(year=g[\"year\"], month=g[\"month\"], day=1))\n",
    "    g = g.sort_values(\"date\").reset_index(drop=True)\n",
    "    for col in cols:\n",
    "        if col in g.columns:\n",
    "            for L in range(1, max_lag + 1):\n",
    "                g[f\"{col}_lag{L}\"] = g[col].shift(L)\n",
    "    return g\n",
    "\n",
    "def corr_with_min_pairs(x, y, min_pairs=MIN_PAIRS):\n",
    "    v = pd.DataFrame({\"x\": x, \"y\": y}).dropna()\n",
    "    return v[\"x\"].corr(v[\"y\"]) if len(v) >= min_pairs else np.nan\n",
    "\n",
    "def plot_corr_heatmap_with_values(corr_df: pd.DataFrame, target_name: str, var_name: str, out_file: Path):\n",
    "    \"\"\"\n",
    "    Draw a 12×13 heatmap (target months 1..12, lag 0..12) of correlations:\n",
    "    target_name vs lagged var_name; annotate r in each cell.\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "    im = ax.imshow(corr_df.values, cmap=\"bwr\", vmin=-1, vmax=1,\n",
    "                   origin=\"upper\", aspect=\"auto\")\n",
    "\n",
    "    cbar = fig.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
    "    cbar.set_label(f\"Pearson r ({target_name} vs {var_name} lag)\", fontsize=11)\n",
    "\n",
    "    ax.set_xticks(np.arange(len(corr_df.columns)))\n",
    "    ax.set_yticks(np.arange(len(corr_df.index)))\n",
    "    ax.set_xticklabels(corr_df.columns, fontsize=12)\n",
    "    ax.set_yticklabels(corr_df.index, fontsize=12)\n",
    "    ax.set_xlabel(\"Lag (months, 0 = same month)\", fontsize=12)\n",
    "    ax.set_ylabel(\"Target month\", fontsize=12)\n",
    "    ax.set_title(f\"{target_name} correlation vs. {var_name} lags\", fontsize=12, pad=16)\n",
    "\n",
    "    # annotate each cell with r (2 decimals)\n",
    "    for i in range(len(corr_df.index)):\n",
    "        for j in range(len(corr_df.columns)):\n",
    "            val = corr_df.iloc[i, j]\n",
    "            if pd.notna(val):\n",
    "                ax.text(j, i, f\"{val:.2f}\", ha=\"center\", va=\"center\",\n",
    "                        color=\"black\", fontsize=9)\n",
    "\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(out_file, dpi=200)\n",
    "    plt.close(fig)\n",
    "\n",
    "def normalize_columns_case_insensitive(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Lower-case all column names for uniform access; keep original data.\"\"\"\n",
    "    df = df.copy()\n",
    "    df.columns = [c.lower() for c in df.columns]\n",
    "    return df\n",
    "\n",
    "def ensure_tmean(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Ensure 'tmean_c' exists.\n",
    "    If missing but 'tmmn' and 'tmmx' exist, create it as mean of tmmn/tmmx.\n",
    "    \"\"\"\n",
    "    if \"tmean_c\" not in df.columns:\n",
    "        if {\"tmmn\", \"tmmx\"}.issubset(df.columns):\n",
    "            df[\"tmean_c\"] = df[[\"tmmn\", \"tmmx\"]].mean(axis=1)\n",
    "    return df\n",
    "\n",
    "# ----------------- Main pipeline -----------------\n",
    "def main():\n",
    "    os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "    # ----- Load & prepare (once) -----\n",
    "    df = pd.read_csv(IN_CSV)\n",
    "    df = normalize_columns_case_insensitive(df)\n",
    "    df = ensure_tmean(df)\n",
    "\n",
    "    # Required indexing columns\n",
    "    required_idx = {\"site_reference\", \"year\", \"month\"}\n",
    "    if not required_idx.issubset(df.columns):\n",
    "        missing = sorted(list(required_idx - set(df.columns)))\n",
    "        raise ValueError(f\"Missing required columns: {missing}\")\n",
    "\n",
    "    # Filter and standardize types\n",
    "    df = df.dropna(subset=[\"site_reference\", \"year\", \"month\"])\n",
    "    df[\"year\"]  = pd.to_numeric(df[\"year\"], errors=\"coerce\").astype(\"Int64\")\n",
    "    df[\"month\"] = pd.to_numeric(df[\"month\"], errors=\"coerce\").astype(\"Int64\")\n",
    "    df = df[df[\"year\"] >= 2001].copy()\n",
    "\n",
    "    # Available predictors/targets (lowercase)\n",
    "    available_cols = set(df.columns)\n",
    "\n",
    "    # Compute once: collapse to one row per site-year-month using mean (handles accidental duplicates)\n",
    "    base_group_keys = [\"site_reference\", \"year\", \"month\"]\n",
    "\n",
    "    # We'll rebuild agg_map per target to include that target plus predictors that exist\n",
    "    for target in TARGETS:\n",
    "        target_lc = target.lower()\n",
    "        if target_lc not in available_cols:\n",
    "            print(f\"[WARN] Target '{target}' not found in data. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        # Per-target OUT_DIR\n",
    "        target_dir = Path(OUT_DIR) / target_lc\n",
    "        target_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # Use only predictors that exist\n",
    "        preds_present = []\n",
    "        for p in PREDICTORS:\n",
    "            p_lc = p.lower()\n",
    "            if p_lc in available_cols:\n",
    "                preds_present.append(p_lc)\n",
    "            else:\n",
    "                print(f\"[WARN] Predictor '{p}' not found. Skipping for target '{target}'.\")\n",
    "        if not preds_present:\n",
    "            print(f\"[WARN] No predictors available for target '{target}'. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        # Build aggregation map for groupby\n",
    "        agg_map = {c: \"mean\" for c in [target_lc] + preds_present if c in available_cols}\n",
    "\n",
    "        dfm = (\n",
    "            df.groupby(base_group_keys, as_index=False)\n",
    "              .agg(agg_map)\n",
    "              .sort_values(base_group_keys)\n",
    "              .reset_index(drop=True)\n",
    "        )\n",
    "        dfm[\"target_month\"] = dfm[\"month\"]\n",
    "\n",
    "        # Add lags within each site for the predictors only\n",
    "        df_lagged = (\n",
    "            dfm.groupby(\"site_reference\", group_keys=False)\n",
    "               .apply(lambda g: add_lags_for_site(g, preds_present, MAX_LAG))\n",
    "               .reset_index(drop=True)\n",
    "        )\n",
    "\n",
    "        # Pre-allocate heatmaps (per predictor)\n",
    "        heatmaps = {\n",
    "            var: pd.DataFrame(index=range(1,13), columns=range(0,MAX_LAG+1), dtype=float)\n",
    "            for var in preds_present\n",
    "        }\n",
    "\n",
    "        # Compute correlations: per target month x lag\n",
    "        for var in preds_present:\n",
    "            for m in range(1, 13):  # target month\n",
    "                sub = df_lagged[df_lagged[\"target_month\"] == m]\n",
    "                for L in range(0, MAX_LAG + 1):\n",
    "                    if L == 0:\n",
    "                        series = sub.get(var)  # same-month predictor\n",
    "                    else:\n",
    "                        series = sub.get(f\"{var}_lag{L}\")\n",
    "                    r = corr_with_min_pairs(sub[target_lc], series, MIN_PAIRS)\n",
    "                    heatmaps[var].loc[m, L] = r\n",
    "\n",
    "        # ----- Save correlation matrices & summaries -----\n",
    "        best_rows = []\n",
    "        for var, mat in heatmaps.items():\n",
    "            mat.index.name = \"target_month\"\n",
    "            mat.columns.name = \"lag_months\"\n",
    "            mat.to_csv(target_dir / f\"corr_matrix_{var}.csv\", float_format=\"%.4f\")\n",
    "\n",
    "            for m in mat.index:\n",
    "                row = mat.loc[m].dropna()\n",
    "                if row.empty:\n",
    "                    best_rows.append({\"variable\": var, \"target_month\": m,\n",
    "                                      \"best_lag\": np.nan, \"corr\": np.nan})\n",
    "                else:\n",
    "                    k = row.abs().idxmax()\n",
    "                    best_rows.append({\"variable\": var, \"target_month\": m,\n",
    "                                      \"best_lag\": int(k), \"corr\": float(row[k])})\n",
    "\n",
    "        best_df = pd.DataFrame(best_rows)\n",
    "        best_df.to_csv(target_dir / \"best_lag_per_month_all_variables.csv\",\n",
    "                       index=False, float_format=\"%.4f\")\n",
    "\n",
    "        summary = []\n",
    "        for var, mat in heatmaps.items():\n",
    "            mean_abs = mat.abs().mean(axis=0)\n",
    "            k = mean_abs.idxmax()\n",
    "            summary.append({\n",
    "                \"variable\": var,\n",
    "                \"overall_best_lag\": int(k),\n",
    "                \"overall_mean_abs_corr\": float(mean_abs[k])\n",
    "            })\n",
    "        pd.DataFrame(summary).to_csv(\n",
    "            target_dir / \"overall_best_lag_summary.csv\",\n",
    "            index=False, float_format=\"%.4f\"\n",
    "        )\n",
    "\n",
    "        # ----- Plot annotated heatmaps -----\n",
    "        for var, mat in heatmaps.items():\n",
    "            plot_corr_heatmap_with_values(\n",
    "                mat,\n",
    "                target_name=target_lc,\n",
    "                var_name=var,\n",
    "                out_file=target_dir / f\"corr_heatmap_{var}_annotated.png\"\n",
    "            )\n",
    "\n",
    "        print(f\"✓ Done target '{target}'. Results saved under: {target_dir}\")\n",
    "\n",
    "    print(\"All requested targets processed.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0efa3e3-b587-4185-9a46-3fdcd0984e3c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-xgboost_gpu]",
   "language": "python",
   "name": "conda-env-.conda-xgboost_gpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
