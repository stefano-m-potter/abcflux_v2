{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e237177a-db1c-44fb-8b82-ff1241bfbbf3",
   "metadata": {},
   "source": [
    "10-fold gpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82faade8-1f95-4a25-96e8-2cb3fb71dbd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.model_selection import KFold, GridSearchCV, cross_val_score\n",
    "from sklearn.metrics import mean_absolute_error, r2_score, make_scorer\n",
    "\n",
    "# Suppress warnings\n",
    "os.environ['PYTHONWARNINGS'] = 'ignore::FutureWarning'\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "# 1. Load your dataset\n",
    "file_path = \"/explore/nobackup/people/spotter5/anna_v/v2/v2_model_training_alt_soil_lc_co2.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "df = df[df['flux_method'] == 'EC']\n",
    "\n",
    "# 2. Create tmean_C and date\n",
    "df['tmean_C'] = df[['tmmn', 'tmmx']].mean(axis=1)\n",
    "df['date'] = pd.to_datetime(df[['year', 'month']].assign(day=1))\n",
    "\n",
    "# 3. Define predictors and target\n",
    "# Added 'land_cover' and 'month' to the list of predictors for CatBoost\n",
    "predictor_vars = [\n",
    "    'EVI', 'NDVI', 'sur_refl_b01', 'sur_refl_b02', 'sur_refl_b03', \n",
    "    'sur_refl_b07', 'NDWI', 'pdsi', 'srad', 'tmean_C', 'vap', 'vs',\n",
    "    'bdod_0_100cm', 'cec_0_100cm', 'cfvo_0_100cm', 'clay_0_100cm',\n",
    "    'nitrogen_0_100cm', 'ocd_0_100cm', 'phh2o_0_100cm', 'sand_0_100cm',\n",
    "    'silt_0_100cm', 'soc_0_100cm', 'co2_cont', 'ALT',\n",
    "    'land_cover', 'month' # Categorical features\n",
    "]\n",
    "target_var = 'gpp'\n",
    "categorical_features = ['land_cover', 'month']\n",
    "\n",
    "# 4. Drop rows only if the target variable is missing\n",
    "# CatBoost can also handle missing values (NaN) in predictor variables.\n",
    "df_model = df.dropna(subset=[target_var]).copy()\n",
    "\n",
    "# Convert categorical features to 'category' dtype for CatBoost\n",
    "for col in categorical_features:\n",
    "    df_model[col] = df_model[col].astype('category')\n",
    "\n",
    "# 5. Define features (X) and target (y)\n",
    "X = df_model[predictor_vars]\n",
    "y = df_model[target_var]\n",
    "\n",
    "# 6. Define parameter grid for CatBoost\n",
    "# Note: CatBoost uses 'depth' instead of 'max_depth', 'iterations' for 'n_estimators'\n",
    "param_grid = {\n",
    "    'depth': [4, 6, 8, 10, 12, 14, 16],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'l2_leaf_reg': [0.1, 0.5, 1, 3, 5],\n",
    "    'iterations': [700, 1000, 1500]\n",
    "}\n",
    "\n",
    "# 7. Model and 10-fold CV\n",
    "# Pass the list of categorical features directly to the model\n",
    "cat_model = CatBoostRegressor(\n",
    "    random_state=42,\n",
    "    cat_features=categorical_features,\n",
    "    verbose=0, # Suppress verbose output during training\n",
    "    allow_writing_files=False # Suppress creation of catboost_info dir\n",
    ")\n",
    "cv = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# 8. Grid search (optimize RMSE)\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=cat_model,\n",
    "    param_grid=param_grid,\n",
    "    cv=cv,\n",
    "    scoring='neg_root_mean_squared_error',\n",
    "    n_jobs=-1\n",
    ")\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "# 9. Metrics\n",
    "best_rmse = -grid_search.best_score_\n",
    "best_params = grid_search.best_params_\n",
    "print(f\"✅ Best RMSE: {best_rmse:.3f}\")\n",
    "print(\"✅ Best Parameters:\", best_params)\n",
    "\n",
    "# 10. R² using the best estimator from grid search\n",
    "r2_scores = cross_val_score(\n",
    "    grid_search.best_estimator_,\n",
    "    X,\n",
    "    y,\n",
    "    scoring='r2',\n",
    "    cv=cv,\n",
    "    n_jobs=-1\n",
    ")\n",
    "print(f\"✅ 10-Fold CV R²: {np.mean(r2_scores):.3f} ± {np.std(r2_scores):.3f}\")\n",
    "\n",
    "# 11. MAE using the best estimator from grid search\n",
    "mae_scorer = make_scorer(mean_absolute_error, greater_is_better=False)\n",
    "mae_scores = cross_val_score(\n",
    "    grid_search.best_estimator_,\n",
    "    X,\n",
    "    y,\n",
    "    scoring=mae_scorer,\n",
    "    cv=cv,\n",
    "    n_jobs=-1\n",
    ")\n",
    "print(f\"✅ 10-Fold CV MAE: {-np.mean(mae_scores):.3f} ± {np.std(mae_scores):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf7c83a-372f-410a-b6af-4cb8f4b83536",
   "metadata": {},
   "outputs": [],
   "source": [
    "'t'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4227ab6b-d92f-4ed8-9fa2-65d023d9e955",
   "metadata": {},
   "source": [
    "LOOSO gpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad19d56-61fb-46b9-963d-c508e454c63c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/explore/nobackup/people/spotter5/temp_dir/ipykernel_2730655/3720502333.py:57: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[col] = X[col].astype('category')\n",
      "/explore/nobackup/people/spotter5/temp_dir/ipykernel_2730655/3720502333.py:57: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[col] = X[col].astype('category')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing site: Fyodorovskoye_RU-Fyo_tower...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "\n",
    "# Suppress warnings\n",
    "os.environ['PYTHONWARNINGS'] = 'ignore::FutureWarning'\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "# 1. Load your dataset\n",
    "file_path = \"/explore/nobackup/people/spotter5/anna_v/v2/v2_model_training_alt_soil_lc_co2.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "df = df[df['flux_method'] == 'EC']\n",
    "\n",
    "# 2. Create tmean_C and date\n",
    "df['tmean_C'] = df[['tmmn', 'tmmx']].mean(axis=1)\n",
    "df['date'] = pd.to_datetime(df[['year', 'month']].assign(day=1))\n",
    "\n",
    "# 3. Define predictors and target\n",
    "# Added 'land_cover' and 'month' to the list of predictors for CatBoost\n",
    "feature_cols = [\n",
    "    'EVI', 'NDVI', 'sur_refl_b01', 'sur_refl_b02', 'sur_refl_b03', \n",
    "    'sur_refl_b07', 'NDWI', 'pdsi', 'srad', 'tmean_C', 'vap', 'vs',\n",
    "    'bdod_0_100cm', 'cec_0_100cm', 'cfvo_0_100cm', 'clay_0_100cm',\n",
    "    'nitrogen_0_100cm', 'ocd_0_100cm', 'phh2o_0_100cm', 'sand_0_100cm',\n",
    "    'silt_0_100cm', 'soc_0_100cm', 'co2_cont', 'ALT',\n",
    "    'land_cover', 'month' # Categorical features are now included directly\n",
    "]\n",
    "target_col = 'gpp'\n",
    "categorical_features = ['land_cover', 'month']\n",
    "\n",
    "# Drop rows only if the target variable or site_reference is missing.\n",
    "# CatBoost will handle missing values in the numerical predictor variables.\n",
    "df = df.dropna(subset=['site_reference', target_col])\n",
    "\n",
    "# Define output path for CSVs and create it\n",
    "out_path = os.path.join(\"/explore/nobackup/people/spotter5/anna_v/v2/loocv\", target_col)\n",
    "os.makedirs(out_path, exist_ok=True)\n",
    "\n",
    "# Define a separate output path for figures and create it\n",
    "figures_path = os.path.join(out_path, \"figures\")\n",
    "os.makedirs(figures_path, exist_ok=True)\n",
    "\n",
    "\n",
    "# Prepare features (X) and target (y)\n",
    "# No one-hot encoding is needed for CatBoost\n",
    "X = df[feature_cols]\n",
    "y = df[target_col]\n",
    "sites = df[\"site_reference\"].unique()\n",
    "\n",
    "# Convert categorical features to 'category' dtype for CatBoost\n",
    "for col in categorical_features:\n",
    "    X[col] = X[col].astype('category')\n",
    "\n",
    "results = []\n",
    "all_preds_df_list = []\n",
    "\n",
    "# Leave-One-Site-Out CV\n",
    "for test_site in sites:\n",
    "    print(f\"Processing site: {test_site}...\")\n",
    "    train_idx = df[\"site_reference\"] != test_site\n",
    "    test_idx = df[\"site_reference\"] == test_site\n",
    "\n",
    "    if test_idx.sum() < 1:\n",
    "        continue\n",
    "\n",
    "    X_train, y_train = X.loc[train_idx], y.loc[train_idx]\n",
    "    X_test, y_test = X.loc[test_idx], y.loc[test_idx]\n",
    "    dates_test = df.loc[test_idx, \"date\"]\n",
    "\n",
    "    # Initialize and train the CatBoost model\n",
    "    model = CatBoostRegressor(\n",
    "        iterations=1200, #best is 700 and 3, score of 0.74\n",
    "        learning_rate=0.01,\n",
    "        depth=12,\n",
    "        subsample=0.7,\n",
    "        random_state=42,\n",
    "        l2_leaf_reg=0.1,\n",
    "        cat_features=categorical_features,\n",
    "        verbose=0, # Suppress verbose output\n",
    "        allow_writing_files=False\n",
    "    )\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    site_df = pd.DataFrame({\n",
    "        \"Site\": test_site,\n",
    "        \"Date\": dates_test.values,\n",
    "        \"Observed\": y_test.values,\n",
    "        \"Predicted\": y_pred\n",
    "    })\n",
    "    all_preds_df_list.append(site_df)\n",
    "\n",
    "    # Metrics\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "    results.append({\n",
    "        \"Site\": test_site,\n",
    "        \"RMSE\": rmse,\n",
    "        \"MAE\": mae,\n",
    "        \"R2\": r2\n",
    "    })\n",
    "\n",
    "# Combine results\n",
    "results_df = pd.DataFrame(results)\n",
    "all_preds_df = pd.concat(all_preds_df_list, ignore_index=True)\n",
    "\n",
    "# Save to disk with '_cat' suffix\n",
    "results_csv_path = os.path.join(out_path, f'catboost_results_{target_col}_cat.csv')\n",
    "predictions_csv_path = os.path.join(out_path, f'catboost_predictions_{target_col}_cat.csv')\n",
    "results_df.to_csv(results_csv_path, index=False)\n",
    "all_preds_df.to_csv(predictions_csv_path, index=False)\n",
    "print(f\"\\nResults saved to: {results_csv_path}\")\n",
    "print(f\"Predictions saved to: {predictions_csv_path}\")\n",
    "\n",
    "\n",
    "# Pooled metrics\n",
    "rmse_all = np.sqrt(mean_squared_error(all_preds_df[\"Observed\"], all_preds_df[\"Predicted\"]))\n",
    "r2_all = r2_score(all_preds_df[\"Observed\"], all_preds_df[\"Predicted\"])\n",
    "mae_all = mean_absolute_error(all_preds_df[\"Observed\"], all_preds_df[\"Predicted\"])\n",
    "\n",
    "print(\"\\n--- Site-Specific Results ---\")\n",
    "print(results_df)\n",
    "print(\"\\n--- Pooled Metrics ---\")\n",
    "print(f\"Pooled RMSE: {rmse_all:.4f}\")\n",
    "print(f\"Pooled MAE:  {mae_all:.4f}\")\n",
    "print(f\"Pooled R²:   {r2_all:.4f}\")\n",
    "\n",
    "# Median metrics across sites\n",
    "median_rmse = results_df[\"RMSE\"].median()\n",
    "median_mae = results_df[\"MAE\"].median()\n",
    "median_r2 = results_df[\"R2\"].median()\n",
    "\n",
    "print(\"\\n--- Median Metrics Across Sites ---\")\n",
    "print(f\"Median RMSE: {median_rmse:.4f}\")\n",
    "print(f\"Median MAE:  {median_mae:.4f}\")\n",
    "print(f\"Median R²:   {median_r2:.4f}\")\n",
    "\n",
    "# --- Plotting ---\n",
    "# Loop through each site and save a separate plot\n",
    "unique_sites = all_preds_df[\"Site\"].unique()\n",
    "if not unique_sites.any():\n",
    "    print(\"\\nNo sites to plot.\")\n",
    "else:\n",
    "    print(\"\\nGenerating and saving individual site plots...\")\n",
    "    for site in unique_sites:\n",
    "        fig, ax = plt.subplots(figsize=(12, 7))\n",
    "        \n",
    "        site_df = all_preds_df[all_preds_df[\"Site\"] == site].sort_values(\"Date\")\n",
    "        site_metrics = results_df[results_df[\"Site\"] == site].iloc[0]\n",
    "        rmse_val = round(site_metrics[\"RMSE\"], 2)\n",
    "        r2_val = round(site_metrics[\"R2\"], 2)\n",
    "        mae_val = round(site_metrics[\"MAE\"], 2)\n",
    "\n",
    "        ax.plot(site_df[\"Date\"], site_df[\"Observed\"], label=\"Observed\", marker=\"o\", linestyle='-', markersize=4)\n",
    "        ax.plot(site_df[\"Date\"], site_df[\"Predicted\"], label=\"Predicted\", marker=\"x\", linestyle='--', markersize=4)\n",
    "        ax.set_title(f\"Observed vs. Predicted {target_col} for Site: {site}\")\n",
    "        ax.set_xlabel(\"Date\")\n",
    "        ax.set_ylabel(target_col)\n",
    "        ax.legend()\n",
    "        ax.grid(True)\n",
    "        fig.autofmt_xdate() # Auto-formats the x-axis labels for dates\n",
    "\n",
    "        # Add metrics text to the plot\n",
    "        textstr = f\"RMSE: {rmse_val}\\nMAE: {mae_val}\\nR²: {r2_val}\"\n",
    "        ax.text(\n",
    "            0.97, 0.03, textstr,\n",
    "            transform=ax.transAxes,\n",
    "            fontsize=10,\n",
    "            verticalalignment='bottom',\n",
    "            horizontalalignment='right',\n",
    "            bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"white\", alpha=0.7)\n",
    "        )\n",
    "        \n",
    "        # Define the output path for the plot\n",
    "        plot_filename = f'catboost_{target_col}_{site}_timeseries_cat.png'\n",
    "        plot_path = os.path.join(figures_path, plot_filename)\n",
    "        \n",
    "        # Save the figure\n",
    "        plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
    "        \n",
    "        # Close the plot to free up memory\n",
    "        plt.close(fig)\n",
    "        \n",
    "    print(f\"All site plots saved to: {figures_path}\")\n",
    "\n",
    "out_path = '/explore/nobackup/people/spotter5/anna_v/v2/models'\n",
    "os.makedirs(out_path, exist_ok = True)\n",
    "model_filename = f'{target_col}.json'\n",
    "model.save_model(os.path.join(out_path, model_filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c814d203-0d1e-4652-8523-03589843ee66",
   "metadata": {},
   "source": [
    "10-fold nee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3dfe6241-84e6-4fc1-90b5-441d2d2e955c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/spotter5/.conda/envs/xgboost_gpu/lib/python3.9/site-packages/joblib/externals/loky/process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 75\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;66;03m# 8. Grid search (optimize RMSE)\u001b[39;00m\n\u001b[1;32m     68\u001b[0m grid_search \u001b[38;5;241m=\u001b[39m GridSearchCV(\n\u001b[1;32m     69\u001b[0m     estimator\u001b[38;5;241m=\u001b[39mcat_model,\n\u001b[1;32m     70\u001b[0m     param_grid\u001b[38;5;241m=\u001b[39mparam_grid,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     73\u001b[0m     n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     74\u001b[0m )\n\u001b[0;32m---> 75\u001b[0m \u001b[43mgrid_search\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;66;03m# 9. Metrics\u001b[39;00m\n\u001b[1;32m     78\u001b[0m best_rmse \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mgrid_search\u001b[38;5;241m.\u001b[39mbest_score_\n",
      "File \u001b[0;32m~/.conda/envs/xgboost_gpu/lib/python3.9/site-packages/sklearn/base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1471\u001b[0m     )\n\u001b[1;32m   1472\u001b[0m ):\n\u001b[0;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/xgboost_gpu/lib/python3.9/site-packages/sklearn/model_selection/_search.py:1018\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m   1012\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[1;32m   1013\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[1;32m   1014\u001b[0m     )\n\u001b[1;32m   1016\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[0;32m-> 1018\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1020\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[1;32m   1021\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[1;32m   1022\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/.conda/envs/xgboost_gpu/lib/python3.9/site-packages/sklearn/model_selection/_search.py:1572\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1570\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[1;32m   1571\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1572\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/xgboost_gpu/lib/python3.9/site-packages/sklearn/model_selection/_search.py:964\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    956\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    957\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m    958\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    959\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    960\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[1;32m    961\u001b[0m         )\n\u001b[1;32m    962\u001b[0m     )\n\u001b[0;32m--> 964\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    965\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    966\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    967\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    968\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    969\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    970\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    971\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    972\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    973\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    974\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    975\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    976\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    977\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    978\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrouted_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplitter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    979\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    980\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    982\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    983\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    984\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    985\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    986\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    987\u001b[0m     )\n",
      "File \u001b[0;32m~/.conda/envs/xgboost_gpu/lib/python3.9/site-packages/sklearn/utils/parallel.py:74\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     69\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     70\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     71\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     73\u001b[0m )\n\u001b[0;32m---> 74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/xgboost_gpu/lib/python3.9/site-packages/joblib/parallel.py:2007\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   2001\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[1;32m   2002\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[1;32m   2003\u001b[0m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[1;32m   2004\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[1;32m   2005\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 2007\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/xgboost_gpu/lib/python3.9/site-packages/joblib/parallel.py:1650\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1647\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[1;32m   1649\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1650\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[1;32m   1652\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[1;32m   1653\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[1;32m   1654\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[1;32m   1655\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[1;32m   1656\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/xgboost_gpu/lib/python3.9/site-packages/joblib/parallel.py:1762\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[1;32m   1760\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(\n\u001b[1;32m   1761\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING)):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1763\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[1;32m   1766\u001b[0m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[1;32m   1767\u001b[0m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.model_selection import KFold, GridSearchCV, cross_val_score\n",
    "from sklearn.metrics import mean_absolute_error, r2_score, make_scorer\n",
    "\n",
    "# Suppress warnings\n",
    "os.environ['PYTHONWARNINGS'] = 'ignore::FutureWarning'\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "# 1. Load your dataset\n",
    "file_path = \"/explore/nobackup/people/spotter5/anna_v/v2/v2_model_training_alt_soil_lc_co2.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "df = df[df['flux_method'] == 'EC']\n",
    "\n",
    "# 2. Create tmean_C and date\n",
    "df['tmean_C'] = df[['tmmn', 'tmmx']].mean(axis=1)\n",
    "df['date'] = pd.to_datetime(df[['year', 'month']].assign(day=1))\n",
    "\n",
    "# 3. Define predictors and target\n",
    "# Added 'land_cover' and 'month' to the list of predictors for CatBoost\n",
    "predictor_vars = [\n",
    "    'EVI', 'NDVI', 'sur_refl_b01', 'sur_refl_b02', 'sur_refl_b03', \n",
    "    'sur_refl_b07', 'NDWI', 'pdsi', 'srad', 'tmean_C', 'vap', 'vs',\n",
    "    'bdod_0_100cm', 'cec_0_100cm', 'cfvo_0_100cm', 'clay_0_100cm',\n",
    "    'nitrogen_0_100cm', 'ocd_0_100cm', 'phh2o_0_100cm', 'sand_0_100cm',\n",
    "    'silt_0_100cm', 'soc_0_100cm', 'co2_cont', 'ALT',\n",
    "    'land_cover', 'month' # Categorical features\n",
    "]\n",
    "target_var = 'nee'\n",
    "categorical_features = ['land_cover', 'month']\n",
    "\n",
    "# 4. Drop rows only if the target variable is missing\n",
    "# CatBoost can also handle missing values (NaN) in predictor variables.\n",
    "df_model = df.dropna(subset=[target_var]).copy()\n",
    "\n",
    "# Convert categorical features to 'category' dtype for CatBoost\n",
    "for col in categorical_features:\n",
    "    df_model[col] = df_model[col].astype('category')\n",
    "\n",
    "# 5. Define features (X) and target (y)\n",
    "X = df_model[predictor_vars]\n",
    "y = df_model[target_var]\n",
    "\n",
    "# 6. Define parameter grid for CatBoost\n",
    "# Note: CatBoost uses 'depth' instead of 'max_depth', 'iterations' for 'n_estimators'\n",
    "param_grid = {\n",
    "    'depth': [4, 6, 8, 10, 12, 14, 16],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'l2_leaf_reg': [0.1, 0.5, 1, 3, 5],\n",
    "    'iterations': [1000, 1500]\n",
    "}\n",
    "\n",
    "# 7. Model and 10-fold CV\n",
    "# Pass the list of categorical features directly to the model\n",
    "cat_model = CatBoostRegressor(\n",
    "    random_state=42,\n",
    "    cat_features=categorical_features,\n",
    "    verbose=0, # Suppress verbose output during training\n",
    "    allow_writing_files=False # Suppress creation of catboost_info dir\n",
    ")\n",
    "cv = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# 8. Grid search (optimize RMSE)\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=cat_model,\n",
    "    param_grid=param_grid,\n",
    "    cv=cv,\n",
    "    scoring='neg_root_mean_squared_error',\n",
    "    n_jobs=-1\n",
    ")\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "# 9. Metrics\n",
    "best_rmse = -grid_search.best_score_\n",
    "best_params = grid_search.best_params_\n",
    "print(f\"✅ Best RMSE: {best_rmse:.3f}\")\n",
    "print(\"✅ Best Parameters:\", best_params)\n",
    "\n",
    "# 10. R² using the best estimator from grid search\n",
    "r2_scores = cross_val_score(\n",
    "    grid_search.best_estimator_,\n",
    "    X,\n",
    "    y,\n",
    "    scoring='r2',\n",
    "    cv=cv,\n",
    "    n_jobs=-1\n",
    ")\n",
    "print(f\"✅ 10-Fold CV R²: {np.mean(r2_scores):.3f} ± {np.std(r2_scores):.3f}\")\n",
    "\n",
    "# 11. MAE using the best estimator from grid search\n",
    "mae_scorer = make_scorer(mean_absolute_error, greater_is_better=False)\n",
    "mae_scores = cross_val_score(\n",
    "    grid_search.best_estimator_,\n",
    "    X,\n",
    "    y,\n",
    "    scoring=mae_scorer,\n",
    "    cv=cv,\n",
    "    n_jobs=-1\n",
    ")\n",
    "print(f\"✅ 10-Fold CV MAE: {-np.mean(mae_scores):.3f} ± {np.std(mae_scores):.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c23f0de3-8d7e-4433-a30d-cecf765984b7",
   "metadata": {},
   "source": [
    "LOOSO NEE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5554b0df-b499-4082-9c61-4639b42d884c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/explore/nobackup/people/spotter5/temp_dir/ipykernel_2730655/4044039451.py:57: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[col] = X[col].astype('category')\n",
      "/explore/nobackup/people/spotter5/temp_dir/ipykernel_2730655/4044039451.py:57: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[col] = X[col].astype('category')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing site: Fyodorovskoye_RU-Fyo_tower...\n",
      "Processing site: Saskatchewan - Western Boreal, Mature Aspen_CA-Oas_tower...\n",
      "Processing site: Saskatchewan - Western Boreal, Mature Jack Pine_CA-Ojp_tower...\n",
      "Processing site: Flakaliden_SE-Fla_tower...\n",
      "Processing site: Hyytiala_FI-Hyy_tower...\n",
      "Processing site: Manitoba - Northern Old Black Spruce (former BOREAS Northern Study Area)_CA-Man_tower...\n",
      "Processing site: Saskatchewan - Western Boreal, Mature Black Spruce_CA-Obs_tower...\n",
      "Processing site: Kaamanen_FI-Kaa_tower...\n",
      "Processing site: Nelegel_RU-Nel_tower...\n",
      "Processing site: Neleger Burnt Forest_RU-NeB_tower...\n",
      "Processing site: Neleger larch forest_RU-NeF_tower...\n",
      "Processing site: Zackenberg Heath_GL-ZaH_tower...\n",
      "Processing site: Central Marsh_US-Cms_tower...\n",
      "Processing site: Yakutsk Spasskaya Pad larch_RU-SkP_tower...\n",
      "Processing site: Zotino; Central Siberia_RU-Zfw 2_tower...\n",
      "Processing site: Degero_SE-Deg_tower...\n",
      "Processing site: Sodankyla_FI-Sod_tower...\n",
      "Processing site: UCI-1964 burn site_CA-NS3_tower...\n",
      "Processing site: Neleger Cutover_RU-NeC_tower...\n",
      "Processing site: Saskatchewan - Western Boreal, forest burned in 1998_CA-SF3_tower...\n",
      "Processing site: Saskatchewan - Western Boreal, forest burned in 1989_CA-SF2_tower...\n",
      "Processing site: UCI-1998 burn site_CA-NS7_tower...\n",
      "Processing site: UCI-1930 burn site_CA-NS2_tower...\n",
      "Processing site: UCI-1981 burn site_CA-NS5_tower...\n",
      "Processing site: UCI-1989 burn site_CA-NS6_tower...\n",
      "Processing site: Delta Junction deciduous broadleaf forest_US-Bn1_tower...\n",
      "Processing site: Samoylov Island_RU-Sam_tower...\n",
      "Processing site: Zotino_RU-Zot_tower...\n",
      "Processing site: UCI-1850 burn site_CA-NS1_tower...\n",
      "Processing site: Hakasia 5yr_RU-Ha2_tower...\n",
      "Processing site: Hakasia Steppe_RU-Ha1_tower...\n",
      "Processing site: UCI-1964 burn site wet_CA-NS4_tower...\n",
      "Processing site: Norunda_SE-Nor_tower...\n",
      "Processing site: Delta Junction  evergreen conifer forest_US-Bn2_tower...\n",
      "Processing site: Kherlenbayan Ulaan_MN-Kbu_tower...\n",
      "Processing site: Southern Khentei Taiga_MN-Skt_tower...\n",
      "Processing site: University of Alaska, Fairbanks_US-Uaf_tower...\n",
      "Processing site: Kytalyk, Russia_RU-Cok_tower...\n",
      "Processing site: Ontario - Groundhog River, Boreal Mixedwood Forest_CA-Gro_tower...\n",
      "Processing site: Saskatchewan - Western Boreal, forest burned in 1977_CA-SF1_tower...\n",
      "Processing site: Alberta - Western Peatland - LaBiche River,Black Spruce,Larch Fen_CA-WP1_tower...\n",
      "Processing site: Quebec - Eastern Boreal, Mature Black Spruce_CA-Qfo_tower...\n",
      "Processing site: Saskatchewan - Western Boreal, Jack Pine harvested in 1975_CA-SJ3_tower...\n",
      "Processing site: Skyttorp 2_SE-Sk2_tower...\n",
      "Processing site: HJP02 Jack Pine_CA-HJP02_tower...\n",
      "Processing site: HJP94 Jack Pine_CA-HJP94_tower...\n",
      "Processing site: Saskatchewan - Western Boreal, Jack Pine harvested in 2002_CA-SJ2_tower...\n",
      "Processing site: Hakasia 10 yr_RU-Ha3_tower...\n",
      "Processing site: Alberta - Western Peatland - Poor Fen (Sphagnum moss)_CA-WP2_tower...\n",
      "Processing site: Alberta - Western Peatland - Rich Fen  (Carex)_CA-WP3_tower...\n",
      "Processing site: HJP75 Jack Pine_CA-HJP75_tower...\n",
      "Processing site: Ivotuk_US-Ivo_tower...\n",
      "Processing site: Yakutsk Spasskaya Pad Pine_RU-Sk2_tower...\n",
      "Processing site: Atqasuk_US-Atq_tower...\n",
      "Processing site: Daring Lake_CA-DL1_tower...\n",
      "Processing site: Tura_RU-Tur_tower...\n",
      "Processing site: Kalevansuo_FI-Kns_tower...\n",
      "Processing site: Skyttorp 1_SE-Sk1_tower...\n",
      "Processing site: Siikaneva_FI-Sii_tower...\n",
      "Processing site: Barrow-BES_US-Bes_tower...\n",
      "Processing site: Faejemyr_SE-Faj_tower...\n",
      "Processing site: Knottasen_SE-Kno_tower...\n",
      "Processing site: Salmisuo_FI-Salm_tower...\n",
      "Processing site: Lompolojankka_FI-Lom_tower...\n",
      "Processing site: Churchill Fen Site 1_CA-CF1_tower...\n",
      "Processing site: Imnavait Creek Watershed Tussock Tundra_US-ICt_tower...\n",
      "Processing site: Imnavait Creek Watershed Heath Tundra_US-ICh_tower...\n",
      "Processing site: Imnavait Creek Watershed Wet Sedge Tundra_US-ICs_tower...\n",
      "Processing site: Quebec - 1975 Harvested Black Spruce_CA-Qc2_tower...\n",
      "Processing site: Eight Mile Lake_US-EML_tower...\n",
      "Processing site: Lac Le Caron peatland, an ombrotrophic bog_CA-LLC_tower...\n",
      "Processing site: Anaktuvuk River Moderate Burn_US-An2_tower...\n",
      "Processing site: Anaktuvuk River Severe Burn_US-An1_tower...\n",
      "Processing site: Anaktuvuk River Unburned_US-An3_tower...\n",
      "Processing site: Cape Bounty_CA-CB_tower...\n",
      "Processing site: Nuuk Fen_GL-NuF_tower...\n",
      "Processing site: Rylekaerene_tower...\n",
      "Processing site: Seida_RU-Vrk_tower...\n",
      "Processing site: Zackenberg Fen_GL-ZaF_tower...\n",
      "Processing site: Andoya_NO-And_tower...\n",
      "Processing site: Bayelva, Spitsbergen_SJ-Blv_tower...\n",
      "Processing site: Iqaluit_CA-Iqa_tower...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/spotter5/.conda/envs/xgboost_gpu/lib/python3.9/site-packages/sklearn/metrics/_regression.py:1211: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.\n",
      "  warnings.warn(msg, UndefinedMetricWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing site: Pond Inlet_CA-Pin_tower...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/spotter5/.conda/envs/xgboost_gpu/lib/python3.9/site-packages/sklearn/metrics/_regression.py:1211: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.\n",
      "  warnings.warn(msg, UndefinedMetricWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing site: Poker Flat Research Range: Succession from fire scar to deciduous forest_US-Rpf_tower...\n",
      "Processing site: Lake Hazen, Ellesmere Island_CA-LHazen1-semidesert_tower...\n",
      "Processing site: Samoylov Island_RU-Sam (open)_tower...\n",
      "Processing site: Lettosuo_FI-Let_tower...\n",
      "Processing site: Udleg practice forest_MN-Udg_tower...\n",
      "Processing site: Daring Lake_CA-DL3_tower...\n",
      "Processing site: Elgeeii forest station_RU-Ege_tower...\n",
      "Processing site: Bonanza Creek Black Spruce_US-BZS_tower...\n",
      "Processing site: Daring Lake_CA-DL4_tower...\n",
      "Processing site: Tiksi_RU-Tks_tower...\n",
      "Processing site: Samoylov Island_RU-Sam (closed)_tower...\n",
      "Processing site: Attawapiskat River Fen_CA-ARF_tower...\n",
      "Processing site: Bonanza Creek Thermokarst Bog_US-BZB_tower...\n",
      "Processing site: Poker Flat Research Range Black Spruce Forest_US-Prr_tower...\n",
      "Processing site: Attawapiskat River Bog_CA-ARB_tower...\n",
      "Processing site: Bonanza Creek Rich Fen_US-BZF_tower...\n",
      "Processing site: Cascaden Ridge Fire Scar_US-Fcr_tower...\n",
      "Processing site: Siikaneva2_FI-Si2_tower...\n",
      "Processing site: Cherskii ecotone_RU-Eusk_cher1_tower...\n",
      "Processing site: Kenttarova_FI-Ken_tower...\n",
      "Processing site: Sammaltunturi fell_FI-SamFell_tower...\n",
      "Processing site: Varrio_FI-Var_tower...\n",
      "Processing site: ARM-NSA-Barrow_US-A10_tower...\n",
      "Processing site: Adventdalen_SJ-Adv_tower...\n",
      "Processing site: Lake Hazen, Ellesmere Island_CA-LHazen2-meadow wetland_tower...\n",
      "Processing site: Stordalen Fen_SE-St1_tower...\n",
      "Processing site: Tervalamminsuo_Dry_tower...\n",
      "Processing site: Tervalamminsuo_Wet_tower...\n",
      "Processing site: NGEE Arctic Barrow_US-NGB_tower...\n",
      "Processing site: Cherskii disturbed forest_RU-Eusk_cher2_tower...\n",
      "Processing site: Disko_GL-Dsk_tower...\n",
      "Processing site: Havikpak Creek_CA-HPC_tower...\n",
      "Processing site: Scotty Creek Landscape_CA-SCC_tower...\n",
      "Processing site: ZOTTO Bog_RU-Zo1_tower...\n",
      "Processing site: ZOTTO Forest_RU-Zo2_tower...\n",
      "Processing site: Trail Valley Creek_CA-TVC_tower...\n",
      "Processing site: Cherskii reference_RU-Ch2_tower...\n",
      "Processing site: Flux Observations of Carbon from an Airborne Laboratory (FOCAL) Campaign Site 1_US-Fo1_tower...\n",
      "Processing site: Barrow-CMDL_US-Brw_tower...\n",
      "Processing site: Svartberget_SE-Svb_tower...\n",
      "Processing site: Scotty Creek Bog_CA-SCB_tower...\n",
      "Processing site: Stordalen Palsa Bog_SE-Sto_tower...\n",
      "Processing site: Barrow-BEO_US-Beo_tower...\n",
      "Processing site: Council, Alaska_US-KOC_tower...\n",
      "Processing site: Rosinedal-3_SE-Ros_tower...\n",
      "Processing site: ARM-NSA-Oliktok_US-A03_tower...\n",
      "Processing site: Hyltemossa_SE-Htm_tower...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "\n",
    "# Suppress warnings\n",
    "os.environ['PYTHONWARNINGS'] = 'ignore::FutureWarning'\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "# 1. Load your dataset\n",
    "file_path = \"/explore/nobackup/people/spotter5/anna_v/v2/v2_model_training_alt_soil_lc_co2.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "df = df[df['flux_method'] == 'EC']\n",
    "\n",
    "# 2. Create tmean_C and date\n",
    "df['tmean_C'] = df[['tmmn', 'tmmx']].mean(axis=1)\n",
    "df['date'] = pd.to_datetime(df[['year', 'month']].assign(day=1))\n",
    "\n",
    "# 3. Define predictors and target\n",
    "# Added 'land_cover' and 'month' to the list of predictors for CatBoost\n",
    "feature_cols = [\n",
    "    'EVI', 'NDVI', 'sur_refl_b01', 'sur_refl_b02', 'sur_refl_b03', \n",
    "    'sur_refl_b07', 'NDWI', 'pdsi', 'srad', 'tmean_C', 'vap', 'vs',\n",
    "    'bdod_0_100cm', 'cec_0_100cm', 'cfvo_0_100cm', 'clay_0_100cm',\n",
    "    'nitrogen_0_100cm', 'ocd_0_100cm', 'phh2o_0_100cm', 'sand_0_100cm',\n",
    "    'silt_0_100cm', 'soc_0_100cm', 'co2_cont', 'ALT',\n",
    "    'land_cover', 'month' # Categorical features are now included directly\n",
    "]\n",
    "target_col = 'nee'\n",
    "categorical_features = ['land_cover', 'month']\n",
    "\n",
    "# Drop rows only if the target variable or site_reference is missing.\n",
    "# CatBoost will handle missing values in the numerical predictor variables.\n",
    "df = df.dropna(subset=['site_reference', target_col])\n",
    "\n",
    "# Define output path for CSVs and create it\n",
    "out_path = os.path.join(\"/explore/nobackup/people/spotter5/anna_v/v2/loocv\", target_col)\n",
    "os.makedirs(out_path, exist_ok=True)\n",
    "\n",
    "# Define a separate output path for figures and create it\n",
    "figures_path = os.path.join(out_path, \"figures\")\n",
    "os.makedirs(figures_path, exist_ok=True)\n",
    "\n",
    "\n",
    "# Prepare features (X) and target (y)\n",
    "# No one-hot encoding is needed for CatBoost\n",
    "X = df[feature_cols]\n",
    "y = df[target_col]\n",
    "sites = df[\"site_reference\"].unique()\n",
    "\n",
    "# Convert categorical features to 'category' dtype for CatBoost\n",
    "for col in categorical_features:\n",
    "    X[col] = X[col].astype('category')\n",
    "\n",
    "results = []\n",
    "all_preds_df_list = []\n",
    "\n",
    "# Leave-One-Site-Out CV\n",
    "for test_site in sites:\n",
    "    print(f\"Processing site: {test_site}...\")\n",
    "    train_idx = df[\"site_reference\"] != test_site\n",
    "    test_idx = df[\"site_reference\"] == test_site\n",
    "\n",
    "    if test_idx.sum() < 1:\n",
    "        continue\n",
    "\n",
    "    X_train, y_train = X.loc[train_idx], y.loc[train_idx]\n",
    "    X_test, y_test = X.loc[test_idx], y.loc[test_idx]\n",
    "    dates_test = df.loc[test_idx, \"date\"]\n",
    "\n",
    "    # Initialize and train the CatBoost model\n",
    "    model = CatBoostRegressor(\n",
    "        iterations=1200, #1200, 8 is best\n",
    "        learning_rate=0.01,\n",
    "        depth=12,\n",
    "        # subsample=0.7,\n",
    "        l2_leaf_reg=0.1,\n",
    "        random_state=42,\n",
    "        cat_features=categorical_features,\n",
    "        verbose=0, # Suppress verbose output\n",
    "        allow_writing_files=False\n",
    "    )\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    site_df = pd.DataFrame({\n",
    "        \"Site\": test_site,\n",
    "        \"Date\": dates_test.values,\n",
    "        \"Observed\": y_test.values,\n",
    "        \"Predicted\": y_pred\n",
    "    })\n",
    "    all_preds_df_list.append(site_df)\n",
    "\n",
    "    # Metrics\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "    results.append({\n",
    "        \"Site\": test_site,\n",
    "        \"RMSE\": rmse,\n",
    "        \"MAE\": mae,\n",
    "        \"R2\": r2\n",
    "    })\n",
    "\n",
    "# Combine results\n",
    "results_df = pd.DataFrame(results)\n",
    "all_preds_df = pd.concat(all_preds_df_list, ignore_index=True)\n",
    "\n",
    "# Save to disk with '_cat' suffix\n",
    "results_csv_path = os.path.join(out_path, f'catboost_results_{target_col}_cat.csv')\n",
    "predictions_csv_path = os.path.join(out_path, f'catboost_predictions_{target_col}_cat.csv')\n",
    "results_df.to_csv(results_csv_path, index=False)\n",
    "all_preds_df.to_csv(predictions_csv_path, index=False)\n",
    "print(f\"\\nResults saved to: {results_csv_path}\")\n",
    "print(f\"Predictions saved to: {predictions_csv_path}\")\n",
    "\n",
    "\n",
    "# Pooled metrics\n",
    "rmse_all = np.sqrt(mean_squared_error(all_preds_df[\"Observed\"], all_preds_df[\"Predicted\"]))\n",
    "r2_all = r2_score(all_preds_df[\"Observed\"], all_preds_df[\"Predicted\"])\n",
    "mae_all = mean_absolute_error(all_preds_df[\"Observed\"], all_preds_df[\"Predicted\"])\n",
    "\n",
    "print(\"\\n--- Site-Specific Results ---\")\n",
    "print(results_df)\n",
    "print(\"\\n--- Pooled Metrics ---\")\n",
    "print(f\"Pooled RMSE: {rmse_all:.4f}\")\n",
    "print(f\"Pooled MAE:  {mae_all:.4f}\")\n",
    "print(f\"Pooled R²:   {r2_all:.4f}\")\n",
    "\n",
    "# Median metrics across sites\n",
    "median_rmse = results_df[\"RMSE\"].median()\n",
    "median_mae = results_df[\"MAE\"].median()\n",
    "median_r2 = results_df[\"R2\"].median()\n",
    "\n",
    "print(\"\\n--- Median Metrics Across Sites ---\")\n",
    "print(f\"Median RMSE: {median_rmse:.4f}\")\n",
    "print(f\"Median MAE:  {median_mae:.4f}\")\n",
    "print(f\"Median R²:   {median_r2:.4f}\")\n",
    "\n",
    "# --- Plotting ---\n",
    "# Loop through each site and save a separate plot\n",
    "unique_sites = all_preds_df[\"Site\"].unique()\n",
    "if not unique_sites.any():\n",
    "    print(\"\\nNo sites to plot.\")\n",
    "else:\n",
    "    print(\"\\nGenerating and saving individual site plots...\")\n",
    "    for site in unique_sites:\n",
    "        fig, ax = plt.subplots(figsize=(12, 7))\n",
    "        \n",
    "        site_df = all_preds_df[all_preds_df[\"Site\"] == site].sort_values(\"Date\")\n",
    "        site_metrics = results_df[results_df[\"Site\"] == site].iloc[0]\n",
    "        rmse_val = round(site_metrics[\"RMSE\"], 2)\n",
    "        r2_val = round(site_metrics[\"R2\"], 2)\n",
    "        mae_val = round(site_metrics[\"MAE\"], 2)\n",
    "\n",
    "        ax.plot(site_df[\"Date\"], site_df[\"Observed\"], label=\"Observed\", marker=\"o\", linestyle='-', markersize=4)\n",
    "        ax.plot(site_df[\"Date\"], site_df[\"Predicted\"], label=\"Predicted\", marker=\"x\", linestyle='--', markersize=4)\n",
    "        ax.set_title(f\"Observed vs. Predicted {target_col} for Site: {site}\")\n",
    "        ax.set_xlabel(\"Date\")\n",
    "        ax.set_ylabel(target_col)\n",
    "        ax.legend()\n",
    "        ax.grid(True)\n",
    "        fig.autofmt_xdate() # Auto-formats the x-axis labels for dates\n",
    "\n",
    "        # Add metrics text to the plot\n",
    "        textstr = f\"RMSE: {rmse_val}\\nMAE: {mae_val}\\nR²: {r2_val}\"\n",
    "        ax.text(\n",
    "            0.97, 0.03, textstr,\n",
    "            transform=ax.transAxes,\n",
    "            fontsize=10,\n",
    "            verticalalignment='bottom',\n",
    "            horizontalalignment='right',\n",
    "            bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"white\", alpha=0.7)\n",
    "        )\n",
    "        \n",
    "        # Define the output path for the plot\n",
    "        plot_filename = f'catboost_{target_col}_{site}_timeseries_cat.png'\n",
    "        plot_path = os.path.join(figures_path, plot_filename)\n",
    "        \n",
    "        # Save the figure\n",
    "        plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
    "        \n",
    "        # Close the plot to free up memory\n",
    "        plt.close(fig)\n",
    "        \n",
    "    print(f\"All site plots saved to: {figures_path}\")\n",
    "\n",
    "out_path = '/explore/nobackup/people/spotter5/anna_v/v2/models'\n",
    "os.makedirs(out_path, exist_ok = True)\n",
    "model_filename = f'{target_col}.json'\n",
    "model.save_model(os.path.join(out_path, model_filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bddf421e-db19-428c-b969-4d395db9a7d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "39ebe4ee-3586-4dd0-8b98-9eb81fdcc3c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Site-Specific Results ---\n",
      "                                                  Site       RMSE        MAE  \\\n",
      "0                           Fyodorovskoye_RU-Fyo_tower  29.373125  20.528958   \n",
      "1    Saskatchewan - Western Boreal, Mature Aspen_CA...  53.808814  40.098685   \n",
      "2    Saskatchewan - Western Boreal, Mature Jack Pin...  22.654623  16.510666   \n",
      "3                              Flakaliden_SE-Fla_tower  35.516745  21.258172   \n",
      "4                                Hyytiala_FI-Hyy_tower  36.225151  26.603169   \n",
      "..                                                 ...        ...        ...   \n",
      "180             Zotino; Central Siberia_RU-Zfw 1_tower   2.255942   2.164188   \n",
      "181                           Gunnarsholt_IS-Gun_tower  73.072091  57.988888   \n",
      "182         Happy Valley Wet Sedge Tundra_US-HVs_tower  13.284002  11.954152   \n",
      "183                          Happy Valley_US-HVa_tower  14.009065  12.037370   \n",
      "184                             Sag River_US-Sag_tower   6.224993   6.083372   \n",
      "\n",
      "           R2  \n",
      "0    0.205471  \n",
      "1    0.303037  \n",
      "2    0.217843  \n",
      "3    0.441125  \n",
      "4    0.387159  \n",
      "..        ...  \n",
      "180  0.464738  \n",
      "181 -1.607740  \n",
      "182  0.689262  \n",
      "183  0.597347  \n",
      "184  0.748501  \n",
      "\n",
      "[185 rows x 4 columns]\n",
      "\n",
      "--- Pooled Metrics ---\n",
      "Pooled RMSE: 22.2289\n",
      "Pooled MAE:  14.1064\n",
      "Pooled R²:   0.4918\n",
      "\n",
      "--- Median Metrics Across Sites ---\n",
      "Median RMSE: 14.7969\n",
      "Median MAE:  11.8695\n",
      "Median R²:   0.4707\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Site-Specific Results ---\")\n",
    "print(results_df)\n",
    "print(\"\\n--- Pooled Metrics ---\")\n",
    "print(f\"Pooled RMSE: {rmse_all:.4f}\")\n",
    "print(f\"Pooled MAE:  {mae_all:.4f}\")\n",
    "print(f\"Pooled R²:   {r2_all:.4f}\")\n",
    "\n",
    "# Median metrics across sites\n",
    "median_rmse = results_df[\"RMSE\"].median()\n",
    "median_mae = results_df[\"MAE\"].median()\n",
    "median_r2 = results_df[\"R2\"].median()\n",
    "\n",
    "print(\"\\n--- Median Metrics Across Sites ---\")\n",
    "print(f\"Median RMSE: {median_rmse:.4f}\")\n",
    "print(f\"Median MAE:  {median_mae:.4f}\")\n",
    "print(f\"Median R²:   {median_r2:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be96d0de-a4e5-47c8-b897-d27df7c8c0b3",
   "metadata": {},
   "source": [
    "10 fold reco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7f6424-a25f-4ba6-b363-bae59afea129",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.model_selection import KFold, GridSearchCV, cross_val_score\n",
    "from sklearn.metrics import mean_absolute_error, r2_score, make_scorer\n",
    "\n",
    "# Suppress warnings\n",
    "os.environ['PYTHONWARNINGS'] = 'ignore::FutureWarning'\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "# 1. Load your dataset\n",
    "file_path = \"/explore/nobackup/people/spotter5/anna_v/v2/v2_model_training_alt_soil_lc_co2.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "df = df[df['flux_method'] == 'EC']\n",
    "\n",
    "# 2. Create tmean_C and date\n",
    "df['tmean_C'] = df[['tmmn', 'tmmx']].mean(axis=1)\n",
    "df['date'] = pd.to_datetime(df[['year', 'month']].assign(day=1))\n",
    "\n",
    "# 3. Define predictors and target\n",
    "# Added 'land_cover' and 'month' to the list of predictors for CatBoost\n",
    "predictor_vars = [\n",
    "    'EVI', 'NDVI', 'sur_refl_b01', 'sur_refl_b02', 'sur_refl_b03', \n",
    "    'sur_refl_b07', 'NDWI', 'pdsi', 'srad', 'tmean_C', 'vap', 'vs',\n",
    "    'bdod_0_100cm', 'cec_0_100cm', 'cfvo_0_100cm', 'clay_0_100cm',\n",
    "    'nitrogen_0_100cm', 'ocd_0_100cm', 'phh2o_0_100cm', 'sand_0_100cm',\n",
    "    'silt_0_100cm', 'soc_0_100cm', 'co2_cont', 'ALT',\n",
    "    'land_cover', 'month' # Categorical features\n",
    "]\n",
    "target_var = 'reco'\n",
    "categorical_features = ['land_cover', 'month']\n",
    "\n",
    "# 4. Drop rows only if the target variable is missing\n",
    "# CatBoost can also handle missing values (NaN) in predictor variables.\n",
    "df_model = df.dropna(subset=[target_var]).copy()\n",
    "\n",
    "# Convert categorical features to 'category' dtype for CatBoost\n",
    "for col in categorical_features:\n",
    "    df_model[col] = df_model[col].astype('category')\n",
    "\n",
    "# 5. Define features (X) and target (y)\n",
    "X = df_model[predictor_vars]\n",
    "y = df_model[target_var]\n",
    "\n",
    "# 6. Define parameter grid for CatBoost\n",
    "# Note: CatBoost uses 'depth' instead of 'max_depth', 'iterations' for 'n_estimators'\n",
    "param_grid = {\n",
    "    'depth': [4, 6, 8],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'l2_leaf_reg': [1, 3, 5],\n",
    "    'iterations': [500, 700]\n",
    "}\n",
    "\n",
    "# 7. Model and 10-fold CV\n",
    "# Pass the list of categorical features directly to the model\n",
    "cat_model = CatBoostRegressor(\n",
    "    random_state=42,\n",
    "    cat_features=categorical_features,\n",
    "    verbose=0, # Suppress verbose output during training\n",
    "    allow_writing_files=False # Suppress creation of catboost_info dir\n",
    ")\n",
    "cv = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# 8. Grid search (optimize RMSE)\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=cat_model,\n",
    "    param_grid=param_grid,\n",
    "    cv=cv,\n",
    "    scoring='neg_root_mean_squared_error',\n",
    "    n_jobs=-1\n",
    ")\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "# 9. Metrics\n",
    "best_rmse = -grid_search.best_score_\n",
    "best_params = grid_search.best_params_\n",
    "print(f\"✅ Best RMSE: {best_rmse:.3f}\")\n",
    "print(\"✅ Best Parameters:\", best_params)\n",
    "\n",
    "# 10. R² using the best estimator from grid search\n",
    "r2_scores = cross_val_score(\n",
    "    grid_search.best_estimator_,\n",
    "    X,\n",
    "    y,\n",
    "    scoring='r2',\n",
    "    cv=cv,\n",
    "    n_jobs=-1\n",
    ")\n",
    "print(f\"✅ 10-Fold CV R²: {np.mean(r2_scores):.3f} ± {np.std(r2_scores):.3f}\")\n",
    "\n",
    "# 11. MAE using the best estimator from grid search\n",
    "mae_scorer = make_scorer(mean_absolute_error, greater_is_better=False)\n",
    "mae_scores = cross_val_score(\n",
    "    grid_search.best_estimator_,\n",
    "    X,\n",
    "    y,\n",
    "    scoring=mae_scorer,\n",
    "    cv=cv,\n",
    "    n_jobs=-1\n",
    ")\n",
    "print(f\"✅ 10-Fold CV MAE: {-np.mean(mae_scores):.3f} ± {np.std(mae_scores):.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308e5685-fbcc-410c-a4bf-a712f7cabacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOOSO RECO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4ca456b2-b006-473d-a547-f78119bdc854",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/explore/nobackup/people/spotter5/temp_dir/ipykernel_2730655/287600856.py:57: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[col] = X[col].astype('category')\n",
      "/explore/nobackup/people/spotter5/temp_dir/ipykernel_2730655/287600856.py:57: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[col] = X[col].astype('category')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing site: Fyodorovskoye_RU-Fyo_tower...\n",
      "Processing site: Saskatchewan - Western Boreal, Mature Aspen_CA-Oas_tower...\n",
      "Processing site: Saskatchewan - Western Boreal, Mature Jack Pine_CA-Ojp_tower...\n",
      "Processing site: Hyytiala_FI-Hyy_tower...\n",
      "Processing site: Manitoba - Northern Old Black Spruce (former BOREAS Northern Study Area)_CA-Man_tower...\n",
      "Processing site: Saskatchewan - Western Boreal, Mature Black Spruce_CA-Obs_tower...\n",
      "Processing site: Kaamanen_FI-Kaa_tower...\n",
      "Processing site: Neleger Burnt Forest_RU-NeB_tower...\n",
      "Processing site: Neleger larch forest_RU-NeF_tower...\n",
      "Processing site: Nelegel_RU-Nel_tower...\n",
      "Processing site: Zackenberg Heath_GL-ZaH_tower...\n",
      "Processing site: Central Marsh_US-Cms_tower...\n",
      "Processing site: Zotino; Central Siberia_RU-Zfw 2_tower...\n",
      "Processing site: Degero_SE-Deg_tower...\n",
      "Processing site: Flakaliden_SE-Fla_tower...\n",
      "Processing site: Neleger Cutover_RU-NeC_tower...\n",
      "Processing site: Sodankyla_FI-Sod_tower...\n",
      "Processing site: UCI-1964 burn site_CA-NS3_tower...\n",
      "Processing site: Saskatchewan - Western Boreal, forest burned in 1989_CA-SF2_tower...\n",
      "Processing site: UCI-1930 burn site_CA-NS2_tower...\n",
      "Processing site: UCI-1981 burn site_CA-NS5_tower...\n",
      "Processing site: UCI-1989 burn site_CA-NS6_tower...\n",
      "Processing site: Samoylov Island_RU-Sam_tower...\n",
      "Processing site: UCI-1998 burn site_CA-NS7_tower...\n",
      "Processing site: Zotino_RU-Zot_tower...\n",
      "Processing site: UCI-1850 burn site_CA-NS1_tower...\n",
      "Processing site: Hakasia Steppe_RU-Ha1_tower...\n",
      "Processing site: Saskatchewan - Western Boreal, forest burned in 1998_CA-SF3_tower...\n",
      "Processing site: UCI-1964 burn site wet_CA-NS4_tower...\n",
      "Processing site: Norunda_SE-Nor_tower...\n",
      "Processing site: Kherlenbayan Ulaan_MN-Kbu_tower...\n",
      "Processing site: Southern Khentei Taiga_MN-Skt_tower...\n",
      "Processing site: University of Alaska, Fairbanks_US-Uaf_tower...\n",
      "Processing site: Atqasuk_US-Atq_tower...\n",
      "Processing site: Kytalyk, Russia_RU-Cok_tower...\n",
      "Processing site: Ontario - Groundhog River, Boreal Mixedwood Forest_CA-Gro_tower...\n",
      "Processing site: Saskatchewan - Western Boreal, forest burned in 1977_CA-SF1_tower...\n",
      "Processing site: Alberta - Western Peatland - LaBiche River,Black Spruce,Larch Fen_CA-WP1_tower...\n",
      "Processing site: Quebec - Eastern Boreal, Mature Black Spruce_CA-Qfo_tower...\n",
      "Processing site: Saskatchewan - Western Boreal, Jack Pine harvested in 1975_CA-SJ3_tower...\n",
      "Processing site: Skyttorp 2_SE-Sk2_tower...\n",
      "Processing site: HJP02 Jack Pine_CA-HJP02_tower...\n",
      "Processing site: HJP94 Jack Pine_CA-HJP94_tower...\n",
      "Processing site: Saskatchewan - Western Boreal, Jack Pine harvested in 2002_CA-SJ2_tower...\n",
      "Processing site: Yakutsk Spasskaya Pad Pine_RU-Sk2_tower...\n",
      "Processing site: Yakutsk Spasskaya Pad larch_RU-SkP_tower...\n",
      "Processing site: Alberta - Western Peatland - Poor Fen (Sphagnum moss)_CA-WP2_tower...\n",
      "Processing site: Alberta - Western Peatland - Rich Fen  (Carex)_CA-WP3_tower...\n",
      "Processing site: HJP75 Jack Pine_CA-HJP75_tower...\n",
      "Processing site: Ivotuk_US-Ivo_tower...\n",
      "Processing site: Tura_RU-Tur_tower...\n",
      "Processing site: Daring Lake_CA-DL1_tower...\n",
      "Processing site: Kalevansuo_FI-Kns_tower...\n",
      "Processing site: Skyttorp 1_SE-Sk1_tower...\n",
      "Processing site: Salmisuo_FI-Salm_tower...\n",
      "Processing site: Lompolojankka_FI-Lom_tower...\n",
      "Processing site: Churchill Fen Site 1_CA-CF1_tower...\n",
      "Processing site: Imnavait Creek Watershed Tussock Tundra_US-ICt_tower...\n",
      "Processing site: Imnavait Creek Watershed Heath Tundra_US-ICh_tower...\n",
      "Processing site: Imnavait Creek Watershed Wet Sedge Tundra_US-ICs_tower...\n",
      "Processing site: Quebec - 1975 Harvested Black Spruce_CA-Qc2_tower...\n",
      "Processing site: Eight Mile Lake_US-EML_tower...\n",
      "Processing site: Anaktuvuk River Moderate Burn_US-An2_tower...\n",
      "Processing site: Anaktuvuk River Severe Burn_US-An1_tower...\n",
      "Processing site: Anaktuvuk River Unburned_US-An3_tower...\n",
      "Processing site: Lac Le Caron peatland, an ombrotrophic bog_CA-LLC_tower...\n",
      "Processing site: Nuuk Fen_GL-NuF_tower...\n",
      "Processing site: Rylekaerene_tower...\n",
      "Processing site: Seida_RU-Vrk_tower...\n",
      "Processing site: Zackenberg Fen_GL-ZaF_tower...\n",
      "Processing site: Andoya_NO-And_tower...\n",
      "Processing site: Bayelva, Spitsbergen_SJ-Blv_tower...\n",
      "Processing site: Iqaluit_CA-Iqa_tower...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/spotter5/.conda/envs/xgboost_gpu/lib/python3.9/site-packages/sklearn/metrics/_regression.py:1211: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.\n",
      "  warnings.warn(msg, UndefinedMetricWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing site: Pond Inlet_CA-Pin_tower...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/spotter5/.conda/envs/xgboost_gpu/lib/python3.9/site-packages/sklearn/metrics/_regression.py:1211: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.\n",
      "  warnings.warn(msg, UndefinedMetricWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing site: Poker Flat Research Range: Succession from fire scar to deciduous forest_US-Rpf_tower...\n",
      "Processing site: Lake Hazen, Ellesmere Island_CA-LHazen1-semidesert_tower...\n",
      "Processing site: Samoylov Island_RU-Sam (open)_tower...\n",
      "Processing site: Lettosuo_FI-Let_tower...\n",
      "Processing site: Elgeeii forest station_RU-Ege_tower...\n",
      "Processing site: Siikaneva_FI-Sii_tower...\n",
      "Processing site: Udleg practice forest_MN-Udg_tower...\n",
      "Processing site: Daring Lake_CA-DL3_tower...\n",
      "Processing site: Bonanza Creek Black Spruce_US-BZS_tower...\n",
      "Processing site: Daring Lake_CA-DL4_tower...\n",
      "Processing site: Tiksi_RU-Tks_tower...\n",
      "Processing site: Samoylov Island_RU-Sam (closed)_tower...\n",
      "Processing site: Attawapiskat River Fen_CA-ARF_tower...\n",
      "Processing site: Bonanza Creek Thermokarst Bog_US-BZB_tower...\n",
      "Processing site: Poker Flat Research Range Black Spruce Forest_US-Prr_tower...\n",
      "Processing site: Attawapiskat River Bog_CA-ARB_tower...\n",
      "Processing site: Bonanza Creek Rich Fen_US-BZF_tower...\n",
      "Processing site: Cascaden Ridge Fire Scar_US-Fcr_tower...\n",
      "Processing site: Siikaneva2_FI-Si2_tower...\n",
      "Processing site: Varrio_FI-Var_tower...\n",
      "Processing site: Adventdalen_SJ-Adv_tower...\n",
      "Processing site: Lake Hazen, Ellesmere Island_CA-LHazen2-meadow wetland_tower...\n",
      "Processing site: Tervalamminsuo_Dry_tower...\n",
      "Processing site: Tervalamminsuo_Wet_tower...\n",
      "Processing site: Barrow-BEO_US-Beo_tower...\n",
      "Processing site: Havikpak Creek_CA-HPC_tower...\n",
      "Processing site: Scotty Creek Landscape_CA-SCC_tower...\n",
      "Processing site: Barrow-BES_US-Bes_tower...\n",
      "Processing site: Trail Valley Creek_CA-TVC_tower...\n",
      "Processing site: Cherskii reference_RU-Ch2_tower...\n",
      "Processing site: NGEE Arctic Barrow_US-NGB_tower...\n",
      "Processing site: Svartberget_SE-Svb_tower...\n",
      "Processing site: Scotty Creek Bog_CA-SCB_tower...\n",
      "Processing site: Stordalen Palsa Bog_SE-Sto_tower...\n",
      "Processing site: Council, Alaska_US-KOC_tower...\n",
      "Processing site: Rosinedal-3_SE-Ros_tower...\n",
      "Processing site: Hustai grassland_MN-Hst_tower...\n",
      "Processing site: Nalaikh grassland_MN-Nkh_tower...\n",
      "Processing site: Bibai bog_JP-Bby_tower...\n",
      "Processing site: Fyodorovskoye2_RU-Fy2_tower...\n",
      "Processing site: Wolf_creek_Buckbrush_CA-WCBB_tower...\n",
      "Processing site: Wolf_creek_SparseShrub_CA-WCPLT_tower...\n",
      "Processing site: Wolf_creek_forest_CA-WCF_tower...\n",
      "Processing site: Smith Creek_CA-SMC_tower...\n",
      "Processing site: NGEE Arctic Council_US-NGC_tower...\n",
      "Processing site: Bonanza Creek Old Thermokarst Bog_US-BZo_tower...\n",
      "Processing site: Kenttarova_FI-Ken_tower...\n",
      "Processing site: NEON Healy (HEAL)_US-xHE_tower...\n",
      "Processing site: Bouleau peatland_CA-BOU_tower...\n",
      "Processing site: Bernard spruce-moss valley_tower...\n",
      "Processing site: NEON Delta Junction (DEJU)_US-xDJ_tower...\n",
      "Processing site: NEON Caribou Creek - Poker Flats Watershed (BONA)_US-xBN_tower...\n",
      "Processing site: NEON Barrow Environmental Observatory (BARR)_US-xBA_tower...\n",
      "Processing site: YKD (unburned)_US-YK2_tower...\n",
      "Processing site: Ljusdal_HY_HY_tower...\n",
      "Processing site: YKD (burned)_US-YK1_tower...\n",
      "Processing site: Ranskalankorpi_FI-Ran forestry treatment_tower...\n",
      "Processing site: Ranskalankorpi_FI-Ran_tower...\n",
      "Processing site: Stortjarn_SE-Srj_tower...\n",
      "Processing site: Wolf_creek_upper_forest_CA-WCUF_tower...\n",
      "Processing site: Halmyran_SE-Hmr_tower...\n",
      "Processing site: Halsingfors mire_SE-HfM_tower...\n",
      "Processing site: Tombstone_slavin_CA-TWOSL_tower...\n",
      "Processing site: Disko_GL-Dsk_tower...\n",
      "Processing site: Ljusdal_SLM_SLM_tower...\n",
      "Processing site: Lily Lake Fen_US-KPL_tower...\n",
      "Processing site: Abisko Stordalen birch forest_tower...\n",
      "Processing site: North Star Yedoma_US-NSY_tower...\n",
      "Processing site: Mukhrino field station, Khanty-Mansiysk, Russia_RU-Muh_tower...\n",
      "Processing site: Plotnikovo field station, Tomsk, Rusia_RU-Plt_tower...\n",
      "Processing site: Cambridge Bay - Mesic_CB-mesic_tower...\n",
      "Processing site: Igarka_RU-Iga_tower...\n",
      "Processing site: Iqaluit Tundra_CA-IQ1_tower...\n",
      "Processing site: Churchill Fen 3_CA-CF3_tower...\n",
      "Processing site: Pitsalu_tower...\n",
      "Processing site: Mittimatalik (Pond Inlet) Tundra_CA-Mtk_tower...\n",
      "Processing site: Happy Valley Wet Sedge Tundra_US-HVs_tower...\n",
      "Processing site: Happy Valley_US-HVa_tower...\n",
      "Processing site: Sag River_US-Sag_tower...\n",
      "\n",
      "Results saved to: /explore/nobackup/people/spotter5/anna_v/v2/loocv/reco/catboost_results_reco_cat.csv\n",
      "Predictions saved to: /explore/nobackup/people/spotter5/anna_v/v2/loocv/reco/catboost_predictions_reco_cat.csv\n",
      "\n",
      "--- Site-Specific Results ---\n",
      "                                                  Site       RMSE        MAE  \\\n",
      "0                           Fyodorovskoye_RU-Fyo_tower  69.524058  46.880004   \n",
      "1    Saskatchewan - Western Boreal, Mature Aspen_CA...  36.347934  23.360059   \n",
      "2    Saskatchewan - Western Boreal, Mature Jack Pin...  19.709691  14.245813   \n",
      "3                                Hyytiala_FI-Hyy_tower  24.643850  19.131102   \n",
      "4    Manitoba - Northern Old Black Spruce (former B...  39.921932  25.983450   \n",
      "..                                                 ...        ...        ...   \n",
      "148                                      Pitsalu_tower  42.983467  34.427687   \n",
      "149      Mittimatalik (Pond Inlet) Tundra_CA-Mtk_tower   6.862663   6.482311   \n",
      "150         Happy Valley Wet Sedge Tundra_US-HVs_tower  42.650290  42.025482   \n",
      "151                          Happy Valley_US-HVa_tower  46.156818  46.026550   \n",
      "152                             Sag River_US-Sag_tower  39.931313  38.972953   \n",
      "\n",
      "            R2  \n",
      "0     0.603114  \n",
      "1     0.765873  \n",
      "2     0.851730  \n",
      "3     0.838268  \n",
      "4     0.533505  \n",
      "..         ...  \n",
      "148  -1.120058  \n",
      "149  -1.378782  \n",
      "150 -30.076940  \n",
      "151 -99.305120  \n",
      "152 -47.058376  \n",
      "\n",
      "[153 rows x 4 columns]\n",
      "\n",
      "--- Pooled Metrics ---\n",
      "Pooled RMSE: 31.7239\n",
      "Pooled MAE:  19.7693\n",
      "Pooled R²:   0.6977\n",
      "\n",
      "--- Median Metrics Across Sites ---\n",
      "Median RMSE: 21.7661\n",
      "Median MAE:  15.9475\n",
      "Median R²:   0.5217\n",
      "\n",
      "Generating and saving individual site plots...\n",
      "All site plots saved to: /explore/nobackup/people/spotter5/anna_v/v2/loocv/reco/figures\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "\n",
    "# Suppress warnings\n",
    "os.environ['PYTHONWARNINGS'] = 'ignore::FutureWarning'\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "# 1. Load your dataset\n",
    "file_path = \"/explore/nobackup/people/spotter5/anna_v/v2/v2_model_training_alt_soil_lc_co2.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "df = df[df['flux_method'] == 'EC']\n",
    "\n",
    "# 2. Create tmean_C and date\n",
    "df['tmean_C'] = df[['tmmn', 'tmmx']].mean(axis=1)\n",
    "df['date'] = pd.to_datetime(df[['year', 'month']].assign(day=1))\n",
    "\n",
    "# 3. Define predictors and target\n",
    "# Added 'land_cover' and 'month' to the list of predictors for CatBoost\n",
    "feature_cols = [\n",
    "    'EVI', 'NDVI', 'sur_refl_b01', 'sur_refl_b02', 'sur_refl_b03', \n",
    "    'sur_refl_b07', 'NDWI', 'pdsi', 'srad', 'tmean_C', 'vap', 'vs',\n",
    "    'bdod_0_100cm', 'cec_0_100cm', 'cfvo_0_100cm', 'clay_0_100cm',\n",
    "    'nitrogen_0_100cm', 'ocd_0_100cm', 'phh2o_0_100cm', 'sand_0_100cm',\n",
    "    'silt_0_100cm', 'soc_0_100cm', 'co2_cont', 'ALT',\n",
    "    'land_cover', 'month' # Categorical features are now included directly\n",
    "]\n",
    "target_col = 'reco'\n",
    "categorical_features = ['land_cover', 'month']\n",
    "\n",
    "# Drop rows only if the target variable or site_reference is missing.\n",
    "# CatBoost will handle missing values in the numerical predictor variables.\n",
    "df = df.dropna(subset=['site_reference', target_col])\n",
    "\n",
    "# Define output path for CSVs and create it\n",
    "out_path = os.path.join(\"/explore/nobackup/people/spotter5/anna_v/v2/loocv\", target_col)\n",
    "os.makedirs(out_path, exist_ok=True)\n",
    "\n",
    "# Define a separate output path for figures and create it\n",
    "figures_path = os.path.join(out_path, \"figures\")\n",
    "os.makedirs(figures_path, exist_ok=True)\n",
    "\n",
    "\n",
    "# Prepare features (X) and target (y)\n",
    "# No one-hot encoding is needed for CatBoost\n",
    "X = df[feature_cols]\n",
    "y = df[target_col]\n",
    "sites = df[\"site_reference\"].unique()\n",
    "\n",
    "# Convert categorical features to 'category' dtype for CatBoost\n",
    "for col in categorical_features:\n",
    "    X[col] = X[col].astype('category')\n",
    "\n",
    "results = []\n",
    "all_preds_df_list = []\n",
    "\n",
    "# Leave-One-Site-Out CV\n",
    "for test_site in sites:\n",
    "    print(f\"Processing site: {test_site}...\")\n",
    "    train_idx = df[\"site_reference\"] != test_site\n",
    "    test_idx = df[\"site_reference\"] == test_site\n",
    "\n",
    "    if test_idx.sum() < 1:\n",
    "        continue\n",
    "\n",
    "    X_train, y_train = X.loc[train_idx], y.loc[train_idx]\n",
    "    X_test, y_test = X.loc[test_idx], y.loc[test_idx]\n",
    "    dates_test = df.loc[test_idx, \"date\"]\n",
    "\n",
    "    # Initialize and train the CatBoost model\n",
    "    model = CatBoostRegressor(\n",
    "        iterations=700,\n",
    "        learning_rate=0.01,\n",
    "        depth=3,\n",
    "        subsample=0.7,\n",
    "        random_state=42,\n",
    "        cat_features=categorical_features,\n",
    "        verbose=0, # Suppress verbose output\n",
    "        allow_writing_files=False\n",
    "    )\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    site_df = pd.DataFrame({\n",
    "        \"Site\": test_site,\n",
    "        \"Date\": dates_test.values,\n",
    "        \"Observed\": y_test.values,\n",
    "        \"Predicted\": y_pred\n",
    "    })\n",
    "    all_preds_df_list.append(site_df)\n",
    "\n",
    "    # Metrics\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "    results.append({\n",
    "        \"Site\": test_site,\n",
    "        \"RMSE\": rmse,\n",
    "        \"MAE\": mae,\n",
    "        \"R2\": r2\n",
    "    })\n",
    "\n",
    "# Combine results\n",
    "results_df = pd.DataFrame(results)\n",
    "all_preds_df = pd.concat(all_preds_df_list, ignore_index=True)\n",
    "\n",
    "# Save to disk with '_cat' suffix\n",
    "results_csv_path = os.path.join(out_path, f'catboost_results_{target_col}_cat.csv')\n",
    "predictions_csv_path = os.path.join(out_path, f'catboost_predictions_{target_col}_cat.csv')\n",
    "results_df.to_csv(results_csv_path, index=False)\n",
    "all_preds_df.to_csv(predictions_csv_path, index=False)\n",
    "print(f\"\\nResults saved to: {results_csv_path}\")\n",
    "print(f\"Predictions saved to: {predictions_csv_path}\")\n",
    "\n",
    "\n",
    "# Pooled metrics\n",
    "rmse_all = np.sqrt(mean_squared_error(all_preds_df[\"Observed\"], all_preds_df[\"Predicted\"]))\n",
    "r2_all = r2_score(all_preds_df[\"Observed\"], all_preds_df[\"Predicted\"])\n",
    "mae_all = mean_absolute_error(all_preds_df[\"Observed\"], all_preds_df[\"Predicted\"])\n",
    "\n",
    "print(\"\\n--- Site-Specific Results ---\")\n",
    "print(results_df)\n",
    "print(\"\\n--- Pooled Metrics ---\")\n",
    "print(f\"Pooled RMSE: {rmse_all:.4f}\")\n",
    "print(f\"Pooled MAE:  {mae_all:.4f}\")\n",
    "print(f\"Pooled R²:   {r2_all:.4f}\")\n",
    "\n",
    "# Median metrics across sites\n",
    "median_rmse = results_df[\"RMSE\"].median()\n",
    "median_mae = results_df[\"MAE\"].median()\n",
    "median_r2 = results_df[\"R2\"].median()\n",
    "\n",
    "print(\"\\n--- Median Metrics Across Sites ---\")\n",
    "print(f\"Median RMSE: {median_rmse:.4f}\")\n",
    "print(f\"Median MAE:  {median_mae:.4f}\")\n",
    "print(f\"Median R²:   {median_r2:.4f}\")\n",
    "\n",
    "# --- Plotting ---\n",
    "# Loop through each site and save a separate plot\n",
    "unique_sites = all_preds_df[\"Site\"].unique()\n",
    "if not unique_sites.any():\n",
    "    print(\"\\nNo sites to plot.\")\n",
    "else:\n",
    "    print(\"\\nGenerating and saving individual site plots...\")\n",
    "    for site in unique_sites:\n",
    "        fig, ax = plt.subplots(figsize=(12, 7))\n",
    "        \n",
    "        site_df = all_preds_df[all_preds_df[\"Site\"] == site].sort_values(\"Date\")\n",
    "        site_metrics = results_df[results_df[\"Site\"] == site].iloc[0]\n",
    "        rmse_val = round(site_metrics[\"RMSE\"], 2)\n",
    "        r2_val = round(site_metrics[\"R2\"], 2)\n",
    "        mae_val = round(site_metrics[\"MAE\"], 2)\n",
    "\n",
    "        ax.plot(site_df[\"Date\"], site_df[\"Observed\"], label=\"Observed\", marker=\"o\", linestyle='-', markersize=4)\n",
    "        ax.plot(site_df[\"Date\"], site_df[\"Predicted\"], label=\"Predicted\", marker=\"x\", linestyle='--', markersize=4)\n",
    "        ax.set_title(f\"Observed vs. Predicted {target_col} for Site: {site}\")\n",
    "        ax.set_xlabel(\"Date\")\n",
    "        ax.set_ylabel(target_col)\n",
    "        ax.legend()\n",
    "        ax.grid(True)\n",
    "        fig.autofmt_xdate() # Auto-formats the x-axis labels for dates\n",
    "\n",
    "        # Add metrics text to the plot\n",
    "        textstr = f\"RMSE: {rmse_val}\\nMAE: {mae_val}\\nR²: {r2_val}\"\n",
    "        ax.text(\n",
    "            0.97, 0.03, textstr,\n",
    "            transform=ax.transAxes,\n",
    "            fontsize=10,\n",
    "            verticalalignment='bottom',\n",
    "            horizontalalignment='right',\n",
    "            bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"white\", alpha=0.7)\n",
    "        )\n",
    "        \n",
    "        # Define the output path for the plot\n",
    "        plot_filename = f'catboost_{target_col}_{site}_timeseries_cat.png'\n",
    "        plot_path = os.path.join(figures_path, plot_filename)\n",
    "        \n",
    "        # Save the figure\n",
    "        plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
    "        \n",
    "        # Close the plot to free up memory\n",
    "        plt.close(fig)\n",
    "        \n",
    "    print(f\"All site plots saved to: {figures_path}\")\n",
    "\n",
    "\n",
    "out_path = '/explore/nobackup/people/spotter5/anna_v/v2/models'\n",
    "os.makedirs(out_path, exist_ok = True)\n",
    "model_filename = f'{target_col}.json'\n",
    "model.save_model(os.path.join(out_path, model_filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a595f41-ed1e-4ec6-9cfd-a2207e69b0a1",
   "metadata": {},
   "source": [
    "10-fold CH4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d807f620-44dd-4ef0-bc94-85251282753e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.model_selection import KFold, GridSearchCV, cross_val_score\n",
    "from sklearn.metrics import mean_absolute_error, r2_score, make_scorer\n",
    "\n",
    "# Suppress warnings\n",
    "os.environ['PYTHONWARNINGS'] = 'ignore::FutureWarning'\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "# 1. Load your dataset\n",
    "file_path = \"/explore/nobackup/people/spotter5/anna_v/v2/v2_model_training_alt_soil_lc_co2.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "df = df[df['flux_method'] == 'EC']\n",
    "\n",
    "# 2. Create tmean_C and date\n",
    "df['tmean_C'] = df[['tmmn', 'tmmx']].mean(axis=1)\n",
    "df['date'] = pd.to_datetime(df[['year', 'month']].assign(day=1))\n",
    "\n",
    "# 3. Define predictors and target\n",
    "# Added 'land_cover' and 'month' to the list of predictors for CatBoost\n",
    "predictor_vars = [\n",
    "    'EVI', 'NDVI', 'sur_refl_b01', 'sur_refl_b02', 'sur_refl_b03', \n",
    "    'sur_refl_b07', 'NDWI', 'pdsi', 'srad', 'tmean_C', 'vap', 'vs',\n",
    "    'bdod_0_100cm', 'cec_0_100cm', 'cfvo_0_100cm', 'clay_0_100cm',\n",
    "    'nitrogen_0_100cm', 'ocd_0_100cm', 'phh2o_0_100cm', 'sand_0_100cm',\n",
    "    'silt_0_100cm', 'soc_0_100cm', 'co2_cont', 'ALT',\n",
    "    'land_cover', 'month' # Categorical features\n",
    "]\n",
    "target_var = 'ch4_flux_total'\n",
    "categorical_features = ['land_cover', 'month']\n",
    "\n",
    "# 4. Drop rows only if the target variable is missing\n",
    "# CatBoost can also handle missing values (NaN) in predictor variables.\n",
    "df_model = df.dropna(subset=[target_var]).copy()\n",
    "\n",
    "# Convert categorical features to 'category' dtype for CatBoost\n",
    "for col in categorical_features:\n",
    "    df_model[col] = df_model[col].astype('category')\n",
    "\n",
    "# 5. Define features (X) and target (y)\n",
    "X = df_model[predictor_vars]\n",
    "y = df_model[target_var]\n",
    "\n",
    "# 6. Define parameter grid for CatBoost\n",
    "# Note: CatBoost uses 'depth' instead of 'max_depth', 'iterations' for 'n_estimators'\n",
    "param_grid = {\n",
    "    'depth': [4, 6, 8],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'l2_leaf_reg': [1, 3, 5],\n",
    "    'iterations': [500, 700]\n",
    "}\n",
    "\n",
    "# 7. Model and 10-fold CV\n",
    "# Pass the list of categorical features directly to the model\n",
    "cat_model = CatBoostRegressor(\n",
    "    random_state=42,\n",
    "    cat_features=categorical_features,\n",
    "    verbose=0, # Suppress verbose output during training\n",
    "    allow_writing_files=False # Suppress creation of catboost_info dir\n",
    ")\n",
    "cv = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# 8. Grid search (optimize RMSE)\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=cat_model,\n",
    "    param_grid=param_grid,\n",
    "    cv=cv,\n",
    "    scoring='neg_root_mean_squared_error',\n",
    "    n_jobs=-1\n",
    ")\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "# 9. Metrics\n",
    "best_rmse = -grid_search.best_score_\n",
    "best_params = grid_search.best_params_\n",
    "print(f\"✅ Best RMSE: {best_rmse:.3f}\")\n",
    "print(\"✅ Best Parameters:\", best_params)\n",
    "\n",
    "# 10. R² using the best estimator from grid search\n",
    "r2_scores = cross_val_score(\n",
    "    grid_search.best_estimator_,\n",
    "    X,\n",
    "    y,\n",
    "    scoring='r2',\n",
    "    cv=cv,\n",
    "    n_jobs=-1\n",
    ")\n",
    "print(f\"✅ 10-Fold CV R²: {np.mean(r2_scores):.3f} ± {np.std(r2_scores):.3f}\")\n",
    "\n",
    "# 11. MAE using the best estimator from grid search\n",
    "mae_scorer = make_scorer(mean_absolute_error, greater_is_better=False)\n",
    "mae_scores = cross_val_score(\n",
    "    grid_search.best_estimator_,\n",
    "    X,\n",
    "    y,\n",
    "    scoring=mae_scorer,\n",
    "    cv=cv,\n",
    "    n_jobs=-1\n",
    ")\n",
    "print(f\"✅ 10-Fold CV MAE: {-np.mean(mae_scores):.3f} ± {np.std(mae_scores):.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd7a688-60c4-4814-b6e3-3152ac02f8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOSO CH4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd6c411-274b-45c6-a0ff-d8ef2b746396",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "\n",
    "# Suppress warnings\n",
    "os.environ['PYTHONWARNINGS'] = 'ignore::FutureWarning'\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "# 1. Load your dataset\n",
    "file_path = \"/explore/nobackup/people/spotter5/anna_v/v2/v2_model_training_alt_soil_lc_co2.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "df = df[df['flux_method'] == 'EC']\n",
    "\n",
    "# 2. Create tmean_C and date\n",
    "df['tmean_C'] = df[['tmmn', 'tmmx']].mean(axis=1)\n",
    "df['date'] = pd.to_datetime(df[['year', 'month']].assign(day=1))\n",
    "\n",
    "# 3. Define predictors and target\n",
    "# Added 'land_cover' and 'month' to the list of predictors for CatBoost\n",
    "feature_cols = [\n",
    "    'EVI', 'NDVI', 'sur_refl_b01', 'sur_refl_b02', 'sur_refl_b03', \n",
    "    'sur_refl_b07', 'NDWI', 'pdsi', 'srad', 'tmean_C', 'vap', 'vs',\n",
    "    'bdod_0_100cm', 'cec_0_100cm', 'cfvo_0_100cm', 'clay_0_100cm',\n",
    "    'nitrogen_0_100cm', 'ocd_0_100cm', 'phh2o_0_100cm', 'sand_0_100cm',\n",
    "    'silt_0_100cm', 'soc_0_100cm', 'co2_cont', 'ALT',\n",
    "    'land_cover', 'month' # Categorical features are now included directly\n",
    "]\n",
    "target_col = 'ch4_flux_total'\n",
    "categorical_features = ['land_cover', 'month']\n",
    "\n",
    "# Drop rows only if the target variable or site_reference is missing.\n",
    "# CatBoost will handle missing values in the numerical predictor variables.\n",
    "df = df.dropna(subset=['site_reference', target_col])\n",
    "\n",
    "# Define output path for CSVs and create it\n",
    "out_path = os.path.join(\"/explore/nobackup/people/spotter5/anna_v/v2/loocv\", target_col)\n",
    "os.makedirs(out_path, exist_ok=True)\n",
    "\n",
    "# Define a separate output path for figures and create it\n",
    "figures_path = os.path.join(out_path, \"figures\")\n",
    "os.makedirs(figures_path, exist_ok=True)\n",
    "\n",
    "\n",
    "# Prepare features (X) and target (y)\n",
    "# No one-hot encoding is needed for CatBoost\n",
    "X = df[feature_cols]\n",
    "y = df[target_col]\n",
    "sites = df[\"site_reference\"].unique()\n",
    "\n",
    "# Convert categorical features to 'category' dtype for CatBoost\n",
    "for col in categorical_features:\n",
    "    X[col] = X[col].astype('category')\n",
    "\n",
    "results = []\n",
    "all_preds_df_list = []\n",
    "\n",
    "# Leave-One-Site-Out CV\n",
    "for test_site in sites:\n",
    "    print(f\"Processing site: {test_site}...\")\n",
    "    train_idx = df[\"site_reference\"] != test_site\n",
    "    test_idx = df[\"site_reference\"] == test_site\n",
    "\n",
    "    if test_idx.sum() < 1:\n",
    "        continue\n",
    "\n",
    "    X_train, y_train = X.loc[train_idx], y.loc[train_idx]\n",
    "    X_test, y_test = X.loc[test_idx], y.loc[test_idx]\n",
    "    dates_test = df.loc[test_idx, \"date\"]\n",
    "\n",
    "    # Initialize and train the CatBoost model\n",
    "    model = CatBoostRegressor(\n",
    "        iterations=700,\n",
    "        learning_rate=0.01,\n",
    "        depth=3,\n",
    "        subsample=0.7,\n",
    "        random_state=42,\n",
    "        cat_features=categorical_features,\n",
    "        verbose=0, # Suppress verbose output\n",
    "        allow_writing_files=False\n",
    "    )\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    site_df = pd.DataFrame({\n",
    "        \"Site\": test_site,\n",
    "        \"Date\": dates_test.values,\n",
    "        \"Observed\": y_test.values,\n",
    "        \"Predicted\": y_pred\n",
    "    })\n",
    "    all_preds_df_list.append(site_df)\n",
    "\n",
    "    # Metrics\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "    results.append({\n",
    "        \"Site\": test_site,\n",
    "        \"RMSE\": rmse,\n",
    "        \"MAE\": mae,\n",
    "        \"R2\": r2\n",
    "    })\n",
    "\n",
    "# Combine results\n",
    "results_df = pd.DataFrame(results)\n",
    "all_preds_df = pd.concat(all_preds_df_list, ignore_index=True)\n",
    "\n",
    "# Save to disk with '_cat' suffix\n",
    "results_csv_path = os.path.join(out_path, f'catboost_results_{target_col}_cat.csv')\n",
    "predictions_csv_path = os.path.join(out_path, f'catboost_predictions_{target_col}_cat.csv')\n",
    "results_df.to_csv(results_csv_path, index=False)\n",
    "all_preds_df.to_csv(predictions_csv_path, index=False)\n",
    "print(f\"\\nResults saved to: {results_csv_path}\")\n",
    "print(f\"Predictions saved to: {predictions_csv_path}\")\n",
    "\n",
    "\n",
    "# Pooled metrics\n",
    "rmse_all = np.sqrt(mean_squared_error(all_preds_df[\"Observed\"], all_preds_df[\"Predicted\"]))\n",
    "r2_all = r2_score(all_preds_df[\"Observed\"], all_preds_df[\"Predicted\"])\n",
    "mae_all = mean_absolute_error(all_preds_df[\"Observed\"], all_preds_df[\"Predicted\"])\n",
    "\n",
    "print(\"\\n--- Site-Specific Results ---\")\n",
    "print(results_df)\n",
    "print(\"\\n--- Pooled Metrics ---\")\n",
    "print(f\"Pooled RMSE: {rmse_all:.4f}\")\n",
    "print(f\"Pooled MAE:  {mae_all:.4f}\")\n",
    "print(f\"Pooled R²:   {r2_all:.4f}\")\n",
    "\n",
    "# Median metrics across sites\n",
    "median_rmse = results_df[\"RMSE\"].median()\n",
    "median_mae = results_df[\"MAE\"].median()\n",
    "median_r2 = results_df[\"R2\"].median()\n",
    "\n",
    "print(\"\\n--- Median Metrics Across Sites ---\")\n",
    "print(f\"Median RMSE: {median_rmse:.4f}\")\n",
    "print(f\"Median MAE:  {median_mae:.4f}\")\n",
    "print(f\"Median R²:   {median_r2:.4f}\")\n",
    "\n",
    "# --- Plotting ---\n",
    "# Loop through each site and save a separate plot\n",
    "unique_sites = all_preds_df[\"Site\"].unique()\n",
    "if not unique_sites.any():\n",
    "    print(\"\\nNo sites to plot.\")\n",
    "else:\n",
    "    print(\"\\nGenerating and saving individual site plots...\")\n",
    "    for site in unique_sites:\n",
    "        fig, ax = plt.subplots(figsize=(12, 7))\n",
    "        \n",
    "        site_df = all_preds_df[all_preds_df[\"Site\"] == site].sort_values(\"Date\")\n",
    "        site_metrics = results_df[results_df[\"Site\"] == site].iloc[0]\n",
    "        rmse_val = round(site_metrics[\"RMSE\"], 2)\n",
    "        r2_val = round(site_metrics[\"R2\"], 2)\n",
    "        mae_val = round(site_metrics[\"MAE\"], 2)\n",
    "\n",
    "        ax.plot(site_df[\"Date\"], site_df[\"Observed\"], label=\"Observed\", marker=\"o\", linestyle='-', markersize=4)\n",
    "        ax.plot(site_df[\"Date\"], site_df[\"Predicted\"], label=\"Predicted\", marker=\"x\", linestyle='--', markersize=4)\n",
    "        ax.set_title(f\"Observed vs. Predicted {target_col} for Site: {site}\")\n",
    "        ax.set_xlabel(\"Date\")\n",
    "        ax.set_ylabel(target_col)\n",
    "        ax.legend()\n",
    "        ax.grid(True)\n",
    "        fig.autofmt_xdate() # Auto-formats the x-axis labels for dates\n",
    "\n",
    "        # Add metrics text to the plot\n",
    "        textstr = f\"RMSE: {rmse_val}\\nMAE: {mae_val}\\nR²: {r2_val}\"\n",
    "        ax.text(\n",
    "            0.97, 0.03, textstr,\n",
    "            transform=ax.transAxes,\n",
    "            fontsize=10,\n",
    "            verticalalignment='bottom',\n",
    "            horizontalalignment='right',\n",
    "            bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"white\", alpha=0.7)\n",
    "        )\n",
    "        \n",
    "        # Define the output path for the plot\n",
    "        plot_filename = f'catboost_{target_col}_{site}_timeseries_cat.png'\n",
    "        plot_path = os.path.join(figures_path, plot_filename)\n",
    "        \n",
    "        # Save the figure\n",
    "        plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
    "        \n",
    "        # Close the plot to free up memory\n",
    "        plt.close(fig)\n",
    "        \n",
    "    print(f\"All site plots saved to: {figures_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4fb0478-16b9-45a2-9f72-06d4f7041ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "'t'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0a3462-e2e4-48c6-9dbe-3abd4247d566",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-xgboost_gpu]",
   "language": "python",
   "name": "conda-env-.conda-xgboost_gpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
